
========================================================
 Indeed, knowledge can aim itself at a target, travel vast distances  having scarcely any effect, and then utterly transform the destination. In our story, too, if we wanted the transporter malfunction to have  a significant physical effect at astronomical distances, it would have  to be via knowledge. All those torrents of photons streaming out of  the starship and carrying, intentionally or unintentionally, information  about a wedding will have a noticeable effect on the distant planet  only if someone there cares about the possibility of such information  enough to set up scientific instruments that could detect it. Now, as I have explained, our imaginary laws of physics which say  that a voltage surge happens ‘in one universe but not the other’ cannot  be deterministic unless the universes are fungible. So, what happens  when the transporter is used again, after the universes are no longer  fungible? Imagine a second starship, of the same type as the first and  far away. What happens if the second starship runs its transporter  immediately after the first one did? One logically possible answer would be that  nothing  happens – in  other words, the laws of physics would say that, once the two universes  are different, all transporters just work normally and never produce    a voltage surge again. However, that would also provide a way of  communicating faster than light, albeit unreliably and only once. You  set up a voltmeter in the transporter room and run the transporter. If  the voltage surges, you know that the other starship, however far away,  276 the beginning of infinity has not yet run its transporter (because, if it had, that would have put  a permanent end to such surges everywhere). The laws governing the  real multiverse do not allow information to flow in that way. If we  want our fictional laws of physics to be universal from the inhabitants’  point of view, the second transporter must do exactly what the first  one did. It must cause a voltage surge in one universe and not in the  other. But in that case something must determine  which  universe the second  surge will happen in. ‘In one universe but not the other’ is no longer  a deterministic specification. Also, a surge must not happen if the  transporter is run  only  in the other universe. That would constitute  inter-universe communication. It must depend on both instances of the  transporter being run simultaneously. Even that could allow some  inter-universe communication, as follows. In the universe where a surge  has once happened, run the transporter at a prearranged time and  observe the voltmeter. If no surge happens, then the transporter in the  other universe is switched off. So we are at an impasse. It is remarkable  how much subtlety there can be in the apparently straightforward,  binary distinction between ‘same’ and different’ – or between ‘affected’  and ‘unaffected’. In the real quantum theory, too, the prohibitions on  inter-universe communication and faster-than-light communication  are closely connected. There is a way – I think it is the only way – to meet simultaneously  the requirements that our fictional laws of physics be universal and  deterministic, and forbid faster-than-light and inter-universe communi- cation:  more universes . Imagine an uncountably infinite number of  them, initially all fungible. The transporter causes previously fungible  ones to become different, as before; but now the relevant law of physics  says, ‘The voltage surges in  half the universes  in which the transporter  is used.’ So, if the two starships both run their transporters, then, after  the two spheres of differentiation have overlapped, there will be  universes of four different kinds: those in which a surge happened only  in the first starship, only in the second, in neither, and in both. In other  words, in the overlap region there are four different histories, each  taking place in one quarter of the universes. Our fictional theory has not provided enough structure in its multi- verse to give a meaning to ‘half the universes’, but the real quantum  277 The Multiverse theory does. As I explained in Chapter 8, the method that a theory  provides for giving a meaning to proportions and averages for infinite  sets is called a  measure . A familiar example is that classical physics  assigns  lengths  to infinite sets of points arranged in a line .  Let us suppose  that our theory provides a measure for universes. Now we are allowed storylines such as the following. In the universes  in which the couple married, they spend their honeymoon on a human- colonized planet that the starship is visiting. As they are teleporting  back up, the voltage surge in  half  those universes causes someone’s  electronic notepad to play a voice message suggesting that one of the  newlyweds has already been unfaithful. This sets off a chain of events  that ends in divorce. So now our original collection of fungible universes  contains three
========================================================
 be able to harness physical phenomena of which  we are unaware. Also, that teleportation to or from another location  would be mistaken for ‘destruction’ (without trace) and ‘creation’ (out  of thin air) in your experiment and that therefore this cannot be ruled  out as a possible cause of the anomalies. When headlines appear of the form ‘Teleportation Possibly  Observed in City Museum, Say Scientists’ and ‘Scientists Prove Alien  323 A Physicist’s History of Bad Philosophy Abduction is Real,’ protest mildly that you have claimed no such  thing, that your results are not conclusive, merely suggestive, and  that more studies are needed to determine the mechanism of this  perplexing phenomenon. You have made no false claim. Data can become ‘inconsistent with  conventional physics’ by the mundane means of containing errors, just  as genes can ‘cause happiness’ by countless mundane means such as  affecting your appearance. The fact that your paper does not point this  out does not make it false. Moreover, as I said, the crucial step consists  of a definition, and definitions, provided only that they are consistent,  cannot be false. You have  defined  an observation of more people entering  than leaving as a ‘destruction’ of people. Although, in everyday language,  that phrase has a connotation of people disappearing in puffs of smoke,  that is not what it means in this study. For all you know, they  could  be  disappearing in puffs of smoke, or in invisible spaceships: that would  be consistent with your data. But your paper takes no position on that.  It is entirely about the outcomes of your observations.  So you had better not name your research paper ‘Errors Made When  Counting People Incompetently’. Aside from being a public-relations  blunder, that title might even be considered unscientific, according to  explanationless science. For it would be taking a position on the  ‘interpretation’ of the observed data, about which it provides no  evidence.  In my view this is a scientific experiment in form only. The substance  of scientific theories is explanation, and explanation  of   errors  constitutes  most of the content of the design of any non-trivial scientific experiment. As the above example illustrates, a generic feature of experimentation  is that the bigger the errors you make, either in the numbers or in your  naming and interpretation of the measured quantities, the more exciting  the results are,  if true . So, without powerful techniques of error-detection  and -correction – which depend on explanatory theories – this gives  rise to an instability where false results drown out the true. In the ‘hard  sciences’ – which usually do good science – false results due to all sorts  of errors are nevertheless common. But they are corrected when    their explanations are criticized and tested. That cannot happen in  explanationless science. Consequently, as soon as scientists allow themselves to stop demand- 324 the beginning of infinity ing good explanations and consider only whether a prediction is  accurate or inaccurate, they are liable to make fools of themselves. This  is the means by which a succession of eminent physicists over the  decades have been fooled by conjurers into believing that various  conjuring tricks have been done by ‘paranormal’ means. Bad philosophy cannot easily be countered by good philosophy –  argument and explanation – because it holds itself immune. But it can  be countered by  progress . People want to understand the world, no  matter how loudly they may deny that. And progress makes bad  philosophy harder to believe. That is not a matter of refutation by logic  or experience, but of explanation. If Mach were alive today I expect  he would have accepted the existence of atoms once he saw them  through a microscope, behaving according to atomic theory. As a matter  of logic, it would still be open to him to say, ‘I’m not seeing atoms, I’m  only seeing a video monitor. And I’m only seeing that theory’s predict- ions  about me , not about atoms, come true.’ But the fact that that is a  general-purpose bad explanation would be borne in upon him. It would  also be open to him to say, ‘Very well, atoms do exist, but electrons do  not.’ But he might well tire of that game if a better one seems to be  available – that is to say, if rapid progress is made. And then he would  soon realize that it is not a game. Bad philosophy is philosophy that denies the possibility, desirability  or existence of progress. And progress is the only effective way of  opposing bad philosophy. If progress cannot continue indefinitely, bad  philosophy will inevitably come again into the ascendancy – for it will  be true. terminology Bad philosophy   Philosophy that actively prevents the growth of  knowledge. Interpretation   The explanatory part of a scientific theory, supposedly  distinct from its predictive or instrumental part. Copenhagen interpretation   Niels Bohr’s combination of instrument-
========================================================
 hostile that they barely permit astrophysicists  at all. So, if we imagine all the values consistent with the emergence of  astrophysicists arrayed on a line, then the anthropic explanation leads  us to expect the measured value to fall at some typical point, not too  close to the middle or to either end. 100 the beginning of infinity However – and here we are reaching Sciama’s main conclusion – that  prediction changes radically if there are  several  constants to explain.  For although any one constant is unlikely to be near the edge of its  range, the more constants there are, the more likely it is that at least  one of them will be. This can be illustrated pictorially as follows, with  our bull’s-eye replaced by a line segment, a square, a cube . . . and we  can imagine this sequence continuing for as many dimensions as there  are fine-tuned constants in nature. Arbitrarily define ‘near the edge’ as  meaning ‘within 10 per cent of the whole range from it’. Then in the  case of one constant, as shown in the diagram, 20 per cent of its possible  values are near one of the two edges of the range, and 80 per cent are  ‘away from the edge’. But with two constants a pair of values has to  satisfy two constraints in order to be ‘away from the edge’. Only 64  per cent of them do so. Hence 36 per cent are near the edge. With three  constants, nearly half the possible choices are near the edge. With 100  constants, over 99.9999999 per cent of them are. Whatever anthropic reasoning predicts about the values of multiple    constants, it predicts will only just happen. So, the more constants are involved, the closer to having no astro- physicists a typical universe- with -astrophysicists is. It is not known  how many constants are involved, but it seems to be several, in which  101 Creation case the overwhelming majority of universes in the anthropically  selected region would be close to its edge. Hence, Sciama concluded,  the anthropic explanation predicts that the universe is  only just  capable  of producing astrophysicists – almost the opposite prediction from the  one that it makes in the case of one constant. On the face of it, this might in turn seem to explain another great  unsolved scientific mystery, known as ‘Fermi’s problem’, named after  the physicist Enrico Fermi, who is said to have asked, ‘ Where are they? ’  Where are the extraterrestrial civilizations? Given the Principle of  Mediocrity, or even just what we know of the galaxy and the universe,  there is no reason to believe that the phenomenon of astrophysicists  is unique to our planet. Similar conditions presumably exist in many  other solar systems, so why would some of them not produce similar  outcomes? Moreover, given the timescales on which stars and galaxies  develop, it is overwhelmingly unlikely that any given extraterrestrial  civilization is currently at a similar state of technological development  to ours: it is likely to be millions of years younger (i.e. non-existent)  or older. The older civilizations have had plenty of time to explore the  galaxy – or at least to send robot space probes or signals. Fermi’s  problem is that we do not see any such civilizations, probes or signals.  Many candidate explanations have been proposed, and none of them,  so far, are very good. The anthropic explanation of fine-tuning, in the  light of Sciama’s argument, might seem to solve the problem neatly:    if the constants of physics in our universe are only just capable of  producing astrophysicists, then it is not surprising that this event has  happened only once, since its happening twice independently in the  same universe would be vanishingly unlikely. Unfortunately, that turns out to be a bad explanation too, because  focusing on fundamental  constants  is parochial: there is no relevant  difference between (1) ‘the same’ laws of physics with different constants  and (2) different laws of physics. And there are infinitely many logically  possible laws of physics. If they were all instantiated in real universes  – as has been suggested by some cosmologists, such as Max Tegmark –    it would be statistically certain that our universe is exactly on the edge  of the astrophysicist-producing class of universes.  We know that that cannot be so from an argument due to Feynman  (which he applied to a slightly different problem). Consider the class  102 the beginning of infinity of all possible universes that contain astrophysicists, and consider what  else  most of them contain. In particular, consider a sphere just large  enough to contain your own brain. If you are interested in explaining  fine-tuning, your brain in its current state counts as an ‘astrophysicist’  for these purposes. In the class of all universes that contain astro- physicists, there are many that contain a sphere whose interior is  perfectly identical to the interior of your sphere, including every detail  of your brain. But in the vast majority of those universes there is
========================================================
 intentions, feelings and other states of mind that characterize an ‘I’. To  object to being ‘controlled’ by Roman numerals when we find them  helpful is like protesting at being controlled by one’s own intentions.  By that argument, it is slavery to escape from slavery. But in fact when  I obey the program that constitutes me (or when I obey the laws of  physics), ‘obey’ means something different from what a slave does. The  two meanings explain events at different levels of emergence. Contrary to what is sometimes said, there were also fairly efficient  ways of multiplying and dividing Roman numerals. So a ship with     crates, each containing jars in a   -by-   grid, could be known to hold    jars altogether without anyone having performed the lengthy  count that was implicit in that numeral. And one could tell at a    glance that    was less than   . Thus, manipulating numbers  independently of tallying or counting opened up applications such as  calculating prices, wages, taxes, interest rates and so on. It was also a  conceptual advance that opened the door to future progress. However,  in regard to these more sophisticated applications, the system was not  universal. Since there was no higher-valued symbol than    (one  thousand), the numerals from two thousand onwards all began with  a string of   ’s, which therefore became nothing more than tally marks  for thousands. The more of them there were in a numeral, the more  one would have to fall back on tallying (examining many instances of  the symbol one by one) in order to do arithmetic.  Just as one could upgrade the vocabulary of an ancient writing  system by adding pictograms, so one could add symbols to a system  of numerals to increase its range. And this was done. But the resulting  system would still always have a highest-valued symbol, and hence  131 The Jump to Universality would not be universal for doing arithmetic without tallying. The only way to emancipate arithmetic from tallying is with rules  of universal reach. As with alphabets, a small set of basic rules and  symbols is sufficient. The universal system in general use today has ten  symbols, the digits 0 to 9, and its universality is due to a rule that the  value of a digit depends on its position in the number. For instance,  the digit 2 means two when written by itself, but means two hundred  in the numeral 204. Such ‘positional’ systems require ‘placeholders’,  such as the digit 0 in 204, whose only function is to place the 2 into  the position where it means two hundred. This system originated in India, but it is not known when. It might  have been as late as the ninth century, since before that only a few  ambiguous documents seem to show it in use. At any rate, its tremen- dous potential in science, mathematics, engineering and trade was not  widely realized. At approximately that time it was embraced by Arab  scholars, yet was not generally used in the Arab world until a thousand  years later. This curious lack of enthusiasm for universality was repeated  in medieval Europe: a few scholars adopted Indian numerals from    the Arabs in the tenth century (resulting in the misnomer ‘Arabic  numerals’), but again these numerals did not come into everyday use  for centuries.  As early as 1900 bce the ancient Babylonians had invented what  was in effect a universal system of numerals, but they too may not have  cared about its universality – nor even been aware of it. It was a  positional system, but very cumbersome compared with the Indian  one. It had 59 ‘digits’, each of which was itself written as a numeral in  a Roman-numeral-like system. So using it for arithmetic with numbers  occurring in everyday life was actually more complicated than using  Roman numerals. It also had no symbol for zero, so it used spaces as  placeholders. It had no way of representing trailing zeros, and no  equivalent of the decimal point (as if, in our system, the numbers 200,  20, 2, 0.2 and so on were all written as 2, and were distinguished only  by context). All this suggests that universality was not the system’s  main design objective, and that it was not greatly valued when it was  achieved.  Perhaps an insight into this recurring oddity is provided by a re    - markable episode in the third century  bce  involving the ancient Greek  132 the beginning of infinity scientist and mathematician Archimedes. His research in astronomy  and pure mathematics led him to a need to do arithmetic with some  rather large numbers, so he had to invent his own system of numerals.  His starting point was a Greek system with which he was familiar,  similar to the Roman one but with a highest-valued symbol  Μ  for  10,000 (one myriad). The range of the system had already been ex    - tended with the rule that digits written above an  Μ  would be multiplied  by a myriad. For instance, the symbol for twenty was  κ  and the symbol  for four was  δ , so they could write twenty-four myriad (240,000)
========================================================
 first is an attribute  of deep ideas, the second an attribute of deep silliness. By confusing  them, one ascribes to the best art and philosophy the qualities of the  worst. Since, in that view, readers, viewers and critics can attribute any  meaning they choose to the second kind of ambiguity, bad philosophy  declares the same to be true of all knowledge: all meanings are equal,  and none of them is objectively true. One then has a choice between  complete nihilism or regarding  all  ‘ambiguity’ as a good thing in those  fields. Horgan chooses the latter option: he classifies art and philosophy  as ‘ironic’ fields, irony being the presence of multiple conflicting mean- ings in a statement.  However, unlike the postmodernists, Horgan thinks that science and  mathematics are the shining exceptions to all that. They alone are  capable of non-ironic knowledge. But there is also, he concludes, such  a thing as  ironic science  – the kind of science that cannot ‘resolve  questions’ because, essentially, it is just philosophy or art. Ironic science  can  continue indefinitely, but that is precisely because it never resolves  anything; it never discovers objective truth. Its only value is in the    eye of the beholder. So the future, according to Horgan, belongs    to ironic knowledge. Objective knowledge has already reached its  ultimate bounds. 449 The Beginning Horgan surveys some of the open questions of fundamental science,  and judges them all either ‘ironic’ or non-fundamental, in support of  his thesis. But that conclusion was made inevitable by his premises  alone. For consider the prospect of  any  future discovery that would  constitute fundamental progress. We cannot know what it is, but bad  philosophy can already split it, on principle, into a new rule of thumb  and a new ‘interpretation’ (or explanation). The new rule of thumb  cannot possibly be fundamental: it will just be another equation.    Only a trained expert could tell the difference between it and the old  equation. The new ‘interpretation’ will by definition be pure philosophy,  and hence must be ‘ironic’. By this method, any potential progress can  be pre-emptively reinterpreted as non-progress.  Horgan rightly points out that his prophecy cannot be proved false  by placing it in the context of previous failed prophecies. The fact that  Michelson was wrong about the achievements of the nineteenth century,  and Lagrange about those of the seventeenth, does not imply that  Horgan was wrong about those of the twentieth. However, it so  happens that our current scientific  knowledge  includes a historically  unusual number of deep, fundamental problems. Never before in the  history of human thought has it been so obvious that our knowledge  is tiny and our ignorance vast. And so, unusually, Horgan’s pessimism  contradicts existing knowledge as well as being a prophetic fallacy.  For example, the problem-situation of fundamental physics today has  a radically different structure from that of 1894. Although physicists  then were aware of some phenomena and theoretical issues which we  now recognize as harbingers of the revolutionary explanations to come,  their importance was unclear at the time. It was hard to distinguish  those harbingers from anomalies that would eventually be cleared up  with existing explanations plus the tweaking of the ‘sixth place of  decimals’ or minor terms in a formula. But today there is no such  excuse for denying that some of our problems are fundamental. Our  best theories are telling us of profound mismatches between themselves  and the reality that they are supposed to explain. One of the most blatant examples of that is that physics currently  has  two  fundamental ‘systems of the world’ – quantum theory and the  general theory of relativity – and they are radically inconsistent. There  are many ways of characterizing this inconsistency – known as the  450 the beginning of infinity problem of quantum gravity – corresponding to the many proposals  for solving it that have been tried without success. One aspect is the  ancient tension between the discrete and the continuous. The resolution  that I described in Chapter 11, in terms of continuous clouds of fungible  instances of a particle with diverse discrete attributes, works only if  the spacetime in which this happens is itself continuous. But if spacetime  is affected by the gravitation of the cloud, then it would acquire discrete  attributes. In cosmology, there has been revolutionary progress even in the few  years since  The End of Science  was written – and also since I wrote  The Fabric of Reality  soon afterwards. At the time, all viable cosmo- logical theories had the expansion of the universe gradually slowing  down, due to gravity, ever since the initial explosion at the Big Bang  and for ever in the future. Cosmologists were trying to determine  whether, despite slowing down, its expansion rate was sufficient to  make the universe expand for ever
========================================================
 of these axioms are  intuitively not just desirable to make a system fair, but essential for it  to be rational. Yet they are inconsistent. It seems to follow that a group of people jointly making decisions  is necessarily irrational in one way or another. It may be a dictatorship,  or under some sort of arbitrary rule; or, if it meets all three represen- tativeness conditions, then it must sometimes change its ‘mind’ in a  direction opposite to that in which criticism and persuasion have been  effective. So it will make perverse choices, no matter how wise and  benevolent the people who interpret and enforce its preferences may  be – unless, possibly, one of them is a dictator (see below). So there is  no such thing as ‘the will of the people’. There is no way to regard  338 the beginning of infinity ‘society’ as a decision-maker with self-consistent preferences. This is  hardly the conclusion that social-choice theory was supposed to report  back to the world.   As with the apportionment problem, there were attempts to fix the  implications of Arrow’s theorem with ‘why don’t they just . . . ?’ ideas.  For instance, why not take into account  how intense  people’s preferences  are? For, if slightly over half the electorate barely prefers X to Y, but  the rest consider it a matter of life and death that Y should be done,  then most intuitive conceptions of representative government would  designate Y as ‘the will of the people’. But intensities of preferences,  and especially the differences in intensities among different people, or  between the same person at different times, are notoriously difficult to  define, let alone measure – like happiness. And, in any case, including  such things makes no difference: there are still no-go theorems. As with the apportionment problem, it seems that whenever one  patches up a decision-making system in one way, it becomes paradox- ical in another. A further serious problem that has been identified in  many decision-making institutions is that they create incentives for  participants to lie about their preferences. For instance, if there are two  options of which you mildly prefer one, you have an incentive to  register your preference as ‘strong’ instead. Perhaps you are prevented  from doing that by a sense of civic responsibility. But a decision-making  system moderated by civic responsibility has the defect that it gives  disproportionate weight to the opinions of people who  lack  civic  responsibility and are willing to lie. On the other hand, a society in  which everyone knows everyone sufficiently well to make such lying  difficult cannot have an effectively secret ballot, and the system will  then give disproportionate weight to the faction most able to intimidate  waverers. One perennially controversial social-choice problem is that of de    - vising an electoral system. Such a system is mathematically similar to  an apportionment scheme, but, instead of allocating seats to states on  the basis of population, it allocates them to candidates (or parties) on  the basis of votes. However, it is more paradoxical than apportionment  and has more serious consequences, because in the case of elections  the element of  persuasion  is central to the whole exercise: an election  is supposed to determine what the voters have become persuaded of.  339 Choices (In contrast, apportionment is not about states trying to persuade  people to migrate from other states.) Consequently an electoral system  can contribute to, or can inhibit, traditions of criticism in the society  concerned.  For example, an electoral system in which seats are allocated wholly  or partly in proportion to the number of votes received by each party  is called a ‘proportional-representation’ system. We know from Balinski  and Young that, if an electoral system is too proportional, it will be  subject to the analogue of the population paradox and other paradoxes.  And indeed the political scientist Peter Kurrild-Klitgaard, in a study  of the most recent eight general elections in Denmark (under its  proportional-representation system), showed that every one of them  manifested paradoxes. These included the ‘More-Preferred-Less-Seats  paradox’, in which a majority of voters prefer party X to party Y, but  party Y receives more seats than party X.  But that is really the least of the irrational attributes of proportional  representation. A more important one – which is shared by even the  mildest of proportional systems – is that they assign  dis proportionate  power in the legislature to the  third-largest party , and often to even  smaller parties. It works like this. It is rare (in any system) for a    single party to receive an overall majority of votes. Hence, if votes are  reflected proportionately in the legislature, no legislation can be passed  unless some of the parties cooperate to pass it, and no government can  be formed unless some of them form a coalition. Sometimes the two  largest
========================================================
 only superficial? Was Beethoven fooling  himself when he thought that the sheets in his waste-paper basket  contained  mistakes : that they were  worse  than the sheets he would  eventually publish? Was he merely meeting the arbitrary standards of  his culture, like the twentieth-century women who carefully adjusted  their hemlines each year to conform to the latest fashions? Or is there  a real meaning to saying that the music of Beethoven and Mozart was  as far above that of their Stone Age ancestors banging mammoth bones  together as Ramanujan’s mathematics was above tally marks?  Is it an illusion that the  criteria  that Beethoven and Mozart were  trying to meet were better too? Or is there no such thing as better? Is  there only ‘I know what I like,’ or what tradition or authority designates  as good? Or what our genes predispose us to like? The psychologist  Shigeru Watanabe has found that sparrows prefer harmonious to  discordant music. Is that all that human artistic appreciation is? All these theories assume – with little or no argument – that for each  logically possible aesthetic standard there could exist, say, a culture in  which people would enjoy and be deeply moved by art that met that  standard. Or that a genetic predisposition could exist with the same  properties. But is it not much more plausible that only very exceptional  aesthetic standards could possibly end up as the norm of any culture,  or be the objective towards which some great artist, creating a new  artistic style, spent a lifetime working? Quite generally, cultural  relativism (about art or morality) has a very hard time explaining what  people are doing when they think they are improving a tradition.  Then there is the equivalent of instrumentalism: is art no more than  a means to non-artistic ends? For instance, artistic creations can deliver  information – a painting can depict something, and a piece of music  can represent an emotion. But their beauty is not primarily in that  content. It is in the form. For instance, here is a boring picture: 357 Why are Flowers Beautiful?  and here is another picture with much the same content: yet with greater aesthetic value. One can see that someone thought  about the second picture. In its composition, framing, cropping, light- ing, focus – it has the  appearance of design  by the photographer. But  design for what? Unlike Paley’s watch, it does not seem to have a  function – it only seems to be more beautiful than the first picture. But  what does that mean? One possible instrumental purpose of beauty is  attraction.  A beautiful  object can be attractive to people who appreciate the beauty. Attractive- ness (to a given audience) can be functional, and is a down-to-earth,  scientifically measurable quantity. Art can be literally attractive in the  sense of causing people to move towards it. Visitors to an art gallery  can see a painting and be reluctant to leave, and then later be caused,  by the painting, to return to it. People may travel great distances to  hear a musical performance – and so on. If you see a work of art that  358 the beginning of infinity you appreciate, that means that you want to dwell on it, to give it your  attention, in order to appreciate more in it. If you are an artist, and  halfway through creating a work of art you see something in it that  you want to bring out, then you are being attracted by a beauty that  you have not yet experienced. You are being attracted by the  idea  of a  piece of art before you have created it.  Not all attractiveness has anything to do with aesthetics. You lose  your balance and fall off a log because we are all attracted to the planet  Earth. That may seem merely a play on the word ‘attraction’: our  attraction to the Earth is due not to aesthetic appreciation but to a law  of physics, which affects artists no more than it does aardvarks. A red  traffic light may induce us to stop and stare at it so long as it remains  red. But that is not artistic appreciation either, even though it is  attraction. It is mechanical. But, when analysed in sufficient detail,  everything  is mechanical. The  laws of physics are sovereign. So, can one draw the conclusion that  beauty cannot have an objective meaning other than ‘that which we  are attracted to by processes in our brains and hence by the laws of  physics’? One cannot, because by that argument the physical world  would not exist objectively either, since the laws of physics also deter- mine what a scientist or mathematician wants to call true. Yet one  cannot  explain  what a mathematician does – or what Hofstadter’s  dominoes do – without referring to the objective truths of mathematics.  New art is unpredictable, like new scientific discoveries. Is that    the unpredictability of randomness, or the deeper unknowability of  knowledge-creation? In other words, is art truly creative, like science  and mathematics? That question is usually asked the other way round,  because
========================================================
 information: one cannot send a message to  the other universe; nor can one change anything in one’s own universe  sooner than light could reach that thing. Nor can one bring new  information – even random information – into the world: everything  that happens is determined by laws of physics from what has gone  before. However, one can, of course, bring new  knowledge  into the  world. Knowledge consists of explanations, and none of those conditions  prevents the creation of new explanations. All this is true of the real  world too. We can temporarily think of the two universes as being literally  parallel. Suppress the third dimension of space and think of a universe  264 the beginning of infinity as being two-dimensional, like an infinitely flat television. Then place  a second such television parallel to it, showing exactly the same pictures  (symbolizing the objects in the two universes). Now forget the material  of which the televisions are made. Only the pictures exist. This is to  stress that a universe is not a receptacle containing physical objects: it  is  those objects. In real physics, even space is a physical object, capable  of warping and affecting matter and being affected by it. So now we have two perfectly parallel, identical universes, each  including an instance of our starship, its crew and its transporter, and  of the whole of space. Because of the symmetry between them, it is  now misleading to call one of them ‘the ordinary universe’ and the  other ‘the phantom zone’. So I shall just call them ‘universes’. The two  of them together (which comprise the whole of physical reality in the  story so far) are the  multiverse . Similarly, it is misleading to speak of  the ‘original’ object and its ‘ doppelgänger ’: they are simply the two  instances  of the object. If our science-fiction speculation were to stop there, the two universes  would have to remain identical for ever. There is nothing logically  impossible about that. Yet it would make our story fatally flawed both  as fiction and as scientific speculation – and for the same reason: it is  a story of two universes, but only one  history . That is to say, there is  only one script about what is really there in both universes. Considered  as fiction, therefore, it is really a single-universe story in a pointless  disguise. Considered as scientific speculation, it describes a world that  would not be explicable to its inhabitants. For how could they ever  argue that their history takes place in two universes and not three or  thirty? Why not two today and thirty tomorrow? Moreover, since their  world has only one history, all their good explanations about nature  would be about that history. That single history would be what they  meant by their ‘world’ or ‘universe’. Nothing of the underlying two-ness  of their reality would be accessible to them, nor would it make any  more sense to them as an explanation than would three-ness or thirty- ness – yet they would be factually mistaken.  A remark about explanation: Although the story so far would be a  bad explanation from the inhabitants’ point of view, it is not necessarily  bad from ours. Imagining inexplicable worlds can help us to understand  the nature of explicability. I have already imagined some inexplicable  265 The Multiverse worlds for that very reason in previous chapters, and I shall imagine  more in this chapter. But, in the end, I want to tell of an explicable  world, and it will be ours. A remark about terminology: The  world  is the whole of physical  reality. In classical (pre-quantum) physics, the world was thought to  consist of one  universe  – something like a whole three-dimensional  space for the whole of time, and all its contents. According to quantum  physics, as I shall explain, the world is a much larger and more com  - plicated object, a  multiverse , which includes many such universes  (among other things). And a  history  is a sequence of events happening  to objects and possibly their identical counterparts. So, in my story so  far, the world is a multiverse that consists of two universes but has  only a single history.  So our two universes must not stay identical. Something like a  transporter malfunction will have to make them different. Yet, as I  said, that may seem to have been ruled out by those restrictions on  information flow. The laws of physics in the fictional multiverse are  deterministic and symmetrical. So what can the transporter possibly  do that would make the two universes differ? It may seem that whatever  one instance of it does to one universe, its  doppelgänger  must be doing  to the other, so the universes can only remain the same. Surprisingly, that is not so. It  is  consistent for two identical entities  to become different under deterministic and symmetrical laws. But, for  that to happen, they must initially be more than just exact images of  each other: they must be  fungible  (the  g  is pronounced as in ‘plunger’),  by
========================================================
 instance, conjurers, politicians and examination candidates are some- times suspected of receiving information through concealed earpieces  and then repeating it mechanically while pretending that it originated  in their brains. Also, when someone is consenting to a medical pro  - cedure, the physician has to make sure that they are not merely uttering  words without knowing what they mean. To test that, one can repeat  a question in a different way, or ask a different question involving  similar words. Then one can check whether the replies change ac    - cordingly. That sort of thing happens naturally in any free-ranging  conversation. A Turing test is similar, but with a different emphasis. When testing  a human, we want to know whether it  is  an unimpaired human (and  not a front for any other human). When testing an AI, we are hoping  156 the beginning of infinity to find a hard-to-vary explanation to the effect that its utterances  cannot  come from any human but only from the AI. In both cases,  interrogating a human as a control for the experiment is pointless. Without a good explanation of how an entity’s utterances were  created, observing them tells us nothing about that. In the Turing test,  at the simplest level, we need to be convinced that the utterances are  not being directly composed by a human masquerading as the AI, as in  the Hofstadter hoax. But the possibility of a hoax is the least of it. For  instance, I guessed above that  Elbot  had recited a stock joke in response  to mistakenly recognizing the keyword ‘spouse’. But the joke would  have quite a different significance if we knew that it was  not  a stock  joke – because no such joke had ever been encoded into the program.  How could we know that? Only from a good explanation. For  instance, we might know it because we ourselves wrote the program.  Another way would be for the author of the program to explain to us  how it works – how it creates knowledge, including jokes. If the  explanation was good, we should know that the program was an AI.  In fact, if we had  only  such an explanation but had not yet seen any  output from the program – and even if it had not been written yet – we  should still conclude that it was a genuine AI program. So there would  be no need for a Turing test. That is why I said that if lack of computer  power were the only thing preventing the achievement of AI, there  would be no need to wait.  Explaining how an AI program works in detail might well be in    - tractably complicated. In practice the author’s explanation would  always be at some emergent, abstract level. But that would not prevent  it from being a good explanation. It would not have to account for the  specific computational steps that composed a joke, just as the theory  of evolution does not have to account for why every specific mutation  succeeded or failed in the history of a given adaptation. It would just  explain how it  could  happen, and why we should expect it to happen,  given how the program works. If that were a good explanation, it would  convince us that the joke – the knowledge in the joke – originated in  the program and not in the programmer. Thus the very same utterance  by the program – the joke – can be either evidence that it is  not  think- ing or evidence that it  is  thinking depending on the best available  explanation of how the program works. 157 Artificial Creativity The nature of humour is not very well understood, so we do not  know whether general-purpose thinking is required to compose jokes.  So it is conceivable that, despite the wide range of subject matter about  which one can joke, there are hidden connections that reduce all joke  making to a single narrow function. In that case there could one day  be general-purpose joke-making programs that are not people, just as  today there are chess-playing programs that are not people. It sounds  implausible, but, since we have no good explanation ruling it out, we  could not rely on joke-making as our only way of judging an AI. What  we could do, though, is have a conversation ranging over a diverse  range of topics, and pay attention to whether the program’s utterances  were or were not adapted, in their meanings, to the various purposes  that came up. If the program really is thinking, then in the course    of such a conversation it will  explain itself  – in one of countless,  unpredictable ways – just as you or I would. There is a deeper issue too. AI abilities must have some sort of  universality: special-purpose thinking would not count as thinking in  the sense Turing intended. My guess is that every AI is a person: a  general-purpose explainer. It is conceivable that there are other levels  of universality between AI and ‘universal explainer/constructor’, and  perhaps separate levels for those associated attributes like conscious- ness. But those attributes all seem to have arrived in one jump to  universality in humans, and, although we have
========================================================
 brain. Also, Darwin’s theory of evolution had not yet  been published, and supernatural accounts of the nature of human  beings were still prevalent. Today there is less mitigation for the minority  of scientists and philosophers who still believe that AI is unattainable.  For instance, the philosopher John Searle has placed the AI project in  the following historical perspective: for centuries, some people have  tried to explain the mind in mechanical terms, using similes and  metaphors based on the most complex machines of the day. First the  brain was supposed to be like an immensely complicated set of gears  and levers. Then it was hydraulic pipes, then steam engines, then  telephone ex      changes – and, now that computers are our most impressive  technology, brains are said to be computers. But this is still no more  than a metaphor, says Searle, and there is no more reason to expect the  brain to be a computer than a steam engine. But there is. A steam engine is not a universal simulator. But a  computer is, so expecting it to be able to do whatever neurons can is  not a metaphor: it is a known and proven property of the laws of  physics as best we know them. (And, as it happens, hydraulic pipes  could also be made into a universal classical computer, and so could  gears and levers, as Babbage showed.) Ironically, Lady Lovelace’s objection has almost the same logic as  Douglas Hofstadter’s argument for reductionism (Chapter 5) – yet  Hofstadter is one of today’s foremost  proponents  of the possibility of  AI. That is because both of them share the mistaken premise that  low-level computational steps cannot possibly add up to a higher-level  ‘I’ that affects anything. The difference between them is that they chose  opposite horns of the dilemma that that poses: Lovelace chose the false  conclusion that AI is impossible, while Hofstadter chose the false  conclusion that no such ‘I’ can exist.  Because of Babbage’s failure either to build a universal computer or  139 The Jump to Universality to persuade others to do so, an entire century would pass before the  first one was built. During that time, what happened was more like the  ancient history of universality: although calculating machines similar  to the Difference Engine were being built by others even before Babbage  had given up, the Analytical Engine was almost entirely ignored even  by mathematicians.  In 1936 Turing developed the definitive theory of universal classical  computers. His motivation was not to build such a computer, but only  to use the theory abstractly to study the nature of mathematical proof.  And when the first universal computers were built, a few years later,  it was, again, not out of any special intention to implement universality.  They were built in Britain and the United States during the Second  World War for specific wartime applications. The British computers,  named Colossus (in which Turing was involved), were used for code- breaking; the American one, ENIAC, was designed to solve the equations  needed for aiming large guns. The technology used in both was elec- tronic vacuum tubes, which acted like relays but about a hundred times  as fast. At the same time, in Germany, the engineer Konrad Zuse was  building a programmable calculator out of relays – just as Babbage  should have done. All three of these devices had the technological  features necessary to be a universal computer, but none of them was  quite configured for this. In the event, the Colossus machines never did  anything but code-breaking, and most were dismantled after the war.  Zuse’s machine was destroyed by Allied bombing. But ENIAC  was  allowed to jump to universality: after the war it was put to diverse uses  for which it had never been designed, such as weather forecasting and  the hydrogen-bomb project.  The history of electronic technology since the Second World War  has been dominated by miniaturization, with ever more microscopic  switches being implemented in each new device. These improve-  ments led to a jump to universality in about 1970, when several  companies independently produced a microprocessor, a universal  classical com  puter on a single silicon chip. From then on, designers  of  any  information-processing device could start with a microprocessor  and then customize it – program it – to perform the specific tasks  needed for that device. Today, your washing machine is almost certainly  controlled by a computer that could be programmed to do astrophysics  140 the beginning of infinity or word processing instead, if it were given suitable input–output  devices and enough memory to hold the necessary data.  It is a remarkable fact that, in that sense (that is to say, ignoring  issues of speed, memory capacity and input–output devices), the    human ‘computers’ of old, the steam-powered Analytical Engine with  its literal bells and whistles, the room-sized vacuum-tube computers  of the Second World War, and present-day
========================================================
 often  do this  by imparting useful functionality to their organism, and in those cases  *This terminology differs slightly from that of Dawkins. Anything that is copied,    for whatever reason, he calls a replicator. What I call a replicator he calls an ‘active    replicator’. 94 the beginning of infinity their knowledge incidentally includes knowledge about that functionality.  Functionality, in turn, is achieved by encoding, into genes, regularities  in the environment and sometimes even rule-of-thumb approximations  to laws of nature, in which case the genes are incidentally encoding that  knowledge too. But the core of the explanation for the presence of a  gene is always that it got itself replicated more than its rival genes. Non-explanatory human knowledge can also evolve in an analogous  way: rules of thumb are not passed on perfectly to the next generation  of users, and the ones that survive in the long run are not necessarily  the ones that optimize the ostensible function. For instance, a rule that  is expressed in an elegant rhyme may be remembered, and repeated,  better than one that is more accurate but expressed in ungainly prose.  Also, no human knowledge is entirely non-explanatory. There is always  at least a background of assumptions about reality against which the  meaning of a rule of thumb is understood, and that background can  make some false rules of thumb seem plausible. Explanatory theories evolve through a more complicated mechanism.  Accidental errors in transmission and memory still play a role, but a  much smaller one. That is because good explanations are hard to vary  even without being tested, and hence random errors in the transmission  of a good explanation are easier for the receiver to detect and correct.  The most important source of variation in explanatory theories is  creativity. For instance, when people are trying to understand an idea  that they hear from others, they typically understand it to mean what  makes most sense to them, or what they are most expecting to hear,  or what they fear to hear, and so on. Those meanings are conjectured  by the listener or reader, and may differ from what the speaker or  writer intended. In addition, people often try to improve explanations  even when they have received them accurately: they make creative  amendments, spurred by their own criticism. If they then pass the  explanation on to others, they usually try to pass on what they consider  to be the improved version. Unlike genes, many memes take different physical forms every time  they are replicated. People rarely express ideas in exactly the same  words in which they heard them. They also translate from one language  to another, and between spoken and written language, and so on. Yet  we rightly call what is transmitted the  same  idea – the same meme –  95 Creation throughout. Thus, in the case of most memes, the real replicator is  abstract: it is the knowledge itself. This is in principle true of genes as  well: biotechnology routinely transcribes genes into the memories of  computers, where they are stored in a different physical form. Those  records could be translated back into DNA strands and implanted in  different animals. The only reason this is not yet a common practice  is that it is easier to copy the original gene. But one day the genes of a  rare species could survive its extinction by causing themselves to be  stored on a computer and then implanted into a cell of a different  species. I say ‘causing themselves to be stored’ because the biotech- nologists would not be recording information indiscriminately, but  only information that met a criterion such as ‘gene of an endangered  species’. The ability to interest biotechnologists in this way would then  be part of the reach of the knowledge in those genes.  So, both human knowledge and biological adaptations are abstract  replicators: forms of information which, once they are embodied in a  suitable physical system, tend to remain so while most variants of them  do not. The fact that the principles of neo-Darwinist theory are, from a  certain perspective, self-evident has itself been used as a criticism of  the theory. For instance, if the theory  must  be true, how can it be  testable? One reply, often attributed to Haldane, is that the whole  theory would be refuted by the discovery of a single fossilized rabbit  in a stratum of Cambrian rock. However, that is misleading. The import  of such an observation would depend on what explanations were  available under the given circumstances. For instance, misidentifications  of fossils, and of strata, have sometimes been made and would have  to be ruled out by good explanations before one could call the discovery  ‘a fossilized rabbit in Cambrian rock’.  Even given such explanations, what would have been ruled out by  the rabbit would be not the theory of evolution itself, but only the  prevailing theory of the history of life and geological processes
========================================================
 that seems strange, imagine, for  the sake of argument, that a historian were to discover that Einstein  wrote his papers only as a joke, or at gunpoint, and was actually a  lifelong believer in Kepler’s laws. This would be a bizarre and important  discovery about the  history  of physics, and all the textbooks about  that would have to be rewritten. But our knowledge of physics itself  would be unaffected, and physics textbooks would not need any change  at all. The second half of the answer is that the reason why the scientists  257 A Dream of Socrates are trying to learn the theory, and also why they have such disregard  for faithfulness to the original, is that they want to know how the  world is. Crucially, this is the same objective that the originator of the  theory had. If it is a good theory – if it is a superb theory, as the  fundamental theories of physics nowadays are – then it is exceedingly  hard to vary while still remaining a viable explanation. So the learners,  through criticism of their initial guesses and with the help of their  books, teachers and colleagues, seeking a viable explanation, will arrive  at the same theory as the originator.  That  is how the theory manages  to be passed faithfully from generation to generation, despite no one  caring about its faithfulness one way or the other.  Slowly, and with many setbacks, the same is becoming true in  non-scientific fields. The way to converge with each other is to converge  upon the truth. 258 11 The Multiverse The idea of a ‘doppelgänger’ (a ‘double’ of a person) is a frequent theme  of science fiction. For instance, the classic television series  Star Trek   featured several types of doppelgänger story involving malfunctions  of the ‘transporter’, the starship’s teleportation device, normally used  for short-range space travel. Since teleporting something is conceptually  similar to making a copy of it at a different location, one can imagine  various ways in which the process could go wrong and somehow end  up with two instances of each passenger – the original and the copy. Stories vary in how similar the  doppelgänger s are to their originals.  To share literally all their attributes, they would have to be at exactly  the same location as well as looking alike. But what would that mean?  Trying to make atoms coincide leads to some problematic physics – for  instance, two coinciding nuclei are liable to combine to form atoms of  heavier chemical elements. And if two identical human bodies were to  coincide even approximately, they would explode simply because water  at double its normal density exerts a pressure of hundreds of thousands  of atmospheres. In fiction one could imagine different laws of physics  to avoid those problems; but, even then, if the  doppelgänger s continued  to coincide with their originals throughout the story, it would not really  be about  doppelgänger s. Sooner or later they have to be different.  Sometimes they are the good and evil ‘sides’ of the same person;  sometimes they start with identical minds but become increasingly  different through having different experiences.  Sometimes a  doppelgänger  is not  copied  from an original, but exists  from the outset in a ‘parallel universe’. In some stories there is a ‘rift’  between universes through which one can communicate or even travel  to meet one’s  doppelgänger . In others, the universes remain mutually  259 The Multiverse imperceptible, in which case the interest of the story (or, rather, two  stories) is in how events are affected by the differences between them.  For instance, the movie  Sliding Doors  interleaves two variants of a  love story, following the fortunes of two instances of the same couple  in two universes which initially differ only in one small detail. In a  related genre, known as ‘alternative history’, one of the two stories  need not be told explicitly because it is a part of our own history and  is assumed to be known to the audience. For example, the novel  Fatherland,  by Robert Harris, is about a universe in which Germany  won the Second World War; Robert Silverberg’s  Roma Eterna  is about  one in which the Roman Empire did not fall. In another class of stories, the transporter’s malfunction accidentally  exiles the passengers to a ‘phantom zone’ where they are imperceptible  to everyone in the ordinary world, but can see and hear them (and each  other). So they have the distressing experience of yelling and gesticulating  in vain to their shipmates, who are oblivious and walk right through  them.  In some stories it is only  copies  of the travellers that are sent to a  phantom zone, unbeknown to the originals. Such a story may end with  the exiles discovering that they can, after all, have some effect on the  ordinary world. They use that effect to signal their existence, and are  rescued through a reversal of the process that exiled them. Depending  on the fictional science that has been supposed, they then may
========================================================
 on  Earth. Suppose, for instance, that there was a prehistoric continent,  isolated from the others, on which evolution happened several times  as fast as elsewhere, and that, by convergent evolution, a rabbit-like  creature evolved there during the Cambrian era; and suppose that the  continents were later connected by a catastrophe that obliterated most  96 the beginning of infinity of the life forms on that continent and submerged their fossils. The  rabbit-like creature was a rare survivor which became extinct soon  afterwards. Given the supposed evidence, that is still an infinitely better  explanation than, for instance, creationism or Lamarckism, neither of  which gives  any  account of the origin of the apparent knowledge in  the rabbit. So what  would  refute the Darwinian theory of evolution? Evidence  which, in the light of the best available explanation, implies that  knowledge came into existence in a different way. For instance, if    an organism was observed to undergo only (or mainly) favourable  mutations, as predicted by Lamarckism or spontaneous generation,  then Darwinism’s ‘random variation’ postulate would be refuted. If  organisms were observed to be born with new, complex adaptations  – for anything – of which there were no precursors in their parents,  then the gradual-change prediction would be refuted and so would  Darwinism’s mechanism of knowledge-creation. If an organism was  born with a complex adaptation that has survival value today, yet was  not favoured by selection pressure in its ancestry (say, an ability to  detect and use internet weather forecasts to decide when to hibernate),  then Darwinism would again be refuted. A fundamentally new ex    - planation would be needed. Facing more or less the same unsolved  problem that Paley and Darwin faced, we should have to set about  finding an explanation that worked. Fine-tuning The physicist Brandon Carter calculated in 1974 that if the strength  of the interaction between charged particles were a few per cent smaller,  no planets would ever have formed and the only condensed objects in  the universe would be stars; and if it were a few per cent greater, then  no stars would ever explode, and so no elements other than hydrogen  and helium would exist outside them. In either case there would be no  complex chemistry and hence presumably no life. Another example: if the initial expansion rate of the universe at the  Big Bang had been slightly higher, no stars would have formed and  there would be nothing in the universe but hydrogen – at an extremely  low and ever-decreasing density. If it had been slightly lower, the  97 Creation universe would have recollapsed soon after the Big Bang. Similar results  have been since obtained for other constants of physics that are not  determined by any known theory. For most, if not all of them, it seems  that if they had been slightly different, there would have been no  possibility for life to exist. This is a remarkable fact which has even been cited as evidence that  those constants were intentionally fine-tuned, i.e. designed, by a super- natural being. This is a new version of creationism, and of the design  argument, now based on the appearance of design  in the laws of physics .  (Ironically, given the history of this controversy, the new argument is  that the laws of physics must have been designed to create a biosphere  by Darwinian evolution .) It even persuaded the philosopher Antony  Flew – formerly an enthusiastic advocate of atheism – of the existence  of a supernatural designer. But it should not have. As I shall explain in  a moment, it is not even clear that this fine-tuning constitutes an  appearance of design in Paley’s sense; but, even if it does, that does    not alter the fact that invoking the supernatural makes for a bad  explanation. And, in any case, arguing for supernatural explanations  on the grounds that a current scientific explanation is flawed or lacking  is just a mistake. As we carved in stone in Chapter 3, problems are  inevitable – there are always unsolved problems. But they get solved.  Science continues to make progress even, or especially, after making  great discoveries, because the discoveries themselves reveal further  problems. Therefore the existence of an unsolved problem in physics  is no more evidence for a supernatural explanation than the existence  of an unsolved crime is evidence that a ghost committed it.  A simple objection to the idea that fine-tuning requires an explanation  at all is that we have no good explanation implying that planets are  essential to the formation of life, or that chemistry is. The physicist  Robert Forward wrote a superb science-fiction story,  Dragon’s Egg ,  based on the premise that information could be stored and processed  – and life and intelligence could evolve – through the interactions  between neutrons on the surface of a neutron star (a star that has  collapsed gravitationally to a diameter of only a
========================================================
 policy is bad, and to remove them without violence if they are. Just as  the institutions of science are structured so as to avoid entrenching  theories, but instead to expose them to criticism and testing, so political  institutions should not make it hard to oppose rulers and policies,  non-violently, and should embody traditions of peaceful, critical dis-  cussion of them and of the institutions themselves and everything else.  Thus, systems of government are to be judged not for their prophetic  ability to choose and install good leaders and policies, but for their  ability to remove bad ones that are already there.  That entire stance is fallibilism in action. It  assumes  that rulers and  policies are always going to be flawed – that problems are inevitable.  But it also assumes that improving upon them is possible: problems  are soluble. The ideal towards which this is working is not that nothing  212 the beginning of infinity unexpected will go wrong, but that when it does it will be an opportun- ity for further progress. Why would anyone want to make the leaders and policies that they  themselves favour more vulnerable to removal? Indeed, let me first  ask:  why would anyone want to replace bad leaders and policies at  all?  That question may seem absurd, but perhaps it is absurd only from  the perspective of a civilization that takes progress for granted. If we  did not expect progress, why should we expect the new leader or policy,  chosen by whatever method, to be any better than the old? On the  contrary, we should then expect any changes on average to do as much  harm as good. And then the precautionary principle advises, ‘Better  the devil you know than the devil you don’t.’ There is a closed loop of  ideas here: on the assumption that knowledge is not going to grow,  the precautionary principle is true; and on the assumption that the  precautionary principle is true, we cannot afford to allow knowledge  to grow. Unless a society is expecting its own future choices to be better  than its present ones, it will strive to make its present policies and  institutions as immutable as possible. Therefore Popper’s criterion can  be met only by societies that expect their knowledge to grow – and to  grow unpredictably. And, further, they are expecting that if it did grow,  that would help .  This expectation is what I call optimism, and I can state it, in its  most general form, thus: The Principle of Optimism All evils are caused by insufficient knowledge. Optimism is, in the first instance, a way of explaining failure, not  prophesying success. It says that there is no fundamental barrier, no  law of nature or supernatural decree, preventing progress. Whenever  we try to improve things and fail, it is not because the spiteful (or  unfathomably benevolent) gods are thwarting us or punishing us for  trying, or because we have reached a limit on the capacity of reason  to make improvements, or because it is best that we fail, but always  because we did not know enough, in time. But optimism is also a stance  towards the future, because nearly all failures, and nearly all successes,  are yet to come. 213 Optimism Optimism follows from the explicability of the physical world, as I  explained in Chapter 3. If something is permitted by the laws of physics,  then the only thing that can prevent it from being technologically  possible is not knowing how. Optimism also assumes that none of the  prohibitions  imposed by the laws of physics are necessarily  evils . So,  for instance, the lack of the impossible knowledge of prophecy is not  an insuperable obstacle to progress. Nor are insoluble mathematical  problems, as I explained in Chapter 8.  That means that in the long run there are no insuperable evils, and  in the short run the only insuperable evils are parochial ones. There  can be no such thing as a disease for which it is impossible to discover  a cure, other than certain types of brain damage – those that have  dissipated the knowledge that constitutes the patient’s personality. For  a sick person is a physical object, and the task of transforming this  object into the same person in good health is one that no law of physics  rules out. Hence there is a way of achieving such a transformation –  that is to say, a cure. It is only a matter of knowing how. If we do not,  for the moment, know how to eliminate a particular evil, or we know  in theory but do not yet have enough time or resources (i.e. wealth),  then, even so, it is universally true that  either  the laws of physics forbid  eliminating it in a given time with the available resources  or  there is a  way of eliminating it in the time and with those resources. The same must hold, equally trivially, for the evil of death – that is  to say, the deaths of human beings from disease or old age. This problem  has a tremendous resonance in every culture – in its literature, its values,  its objectives great and small. It also has an almost
========================================================
 what to want , what to  strive for. The human mind seeks explanations; and now that we  know how to find them, we are not going to stop voluntarily. Here  is another misconception in the Garden of Eden myth: that the  supposed unproblematic state would be a  good  state to be in. Some  theologians have denied this, and I agree with them: an unproblematic  state is a state without creative thought. Its other name is death. All those kinds of problem (survival-related, progress-related, moral,  and sheer-curiosity-driven problems) are connected. We can, for in    - stance, expect that our ability to cope with existential threats will  64 the beginning of infinity continue to depend on knowledge that was originally created for its  own sake. And we can expect disagreements about goals and values  always to exist, because, among other reasons, moral explanations  depend partly on facts about the physical world. For instance, the  moral stances in the Principle of Mediocrity and the Spaceship Earth  idea depend on the physical world not being explicable in the sense  that I have argued it must be. Nor will we ever run out of problems. The deeper an explanation  is, the more new problems it creates. That must be so, if only because  there can be no such thing as an ultimate explanation: just as ‘the gods  did it’ is always a bad explanation, so any other purported foundation  of all explanations must be bad too. It must be easily variable because  it cannot answer the question: why that foundation and not another?  Nothing can be explained only in terms of itself. That holds for  philosophy just as it does for science, and in particular it holds for  moral  philosophy: no utopia is possible, but only because our values  and our objectives can continue to improve indefinitely.  Thus fallibilism alone rather understates the error-prone nature of  knowledge-creation. Knowledge-creation is not only  subject  to error:  errors are common, and significant, and always will be, and correcting  them will always reveal further and better problems. And so the maxim  that I suggested should be carved in stone, namely ‘The Earth’s  biosphere is  incapable  of supporting human life’ is actually a special  case of a much more general truth, namely that, for people,  problems  are inevitable . So let us carve  that  in stone: It is inevitable that we face problems, but no particular problem is  inevitable. We survive, and thrive, by solving each problem as it comes  PROBLEMS ARE INEVITABLE PROBLEMS ARE INEVITABLE 65 The Spark up. And, since the human ability to transform nature is limited only  by the laws of physics, none of the endless stream of problems will  ever constitute an impassable barrier. So a complementary and equally  important truth about people and the physical world is that  problems  are soluble.  By ‘soluble’ I mean that the right knowledge would solve  them. It is not, of course, that we can possess knowledge just by wishing  for it; but it is in principle accessible to us. So let us carve that in    stone too: That  progress  is both possible and desirable is perhaps the  quintessential idea of the Enlightenment. It motivates all traditions of  criticism, as well as the principle of seeking good explanations. But it  can be interpreted in two almost opposite ways, both of which,  confusingly, are known as ‘perfectibility’. One is that humans, or human  societies, are capable of attaining a state of supposed perfection – such  as the Buddhist or Hindu ‘nirvana’, or various political utopias. The  other is that every attainable state can be indefinitely improved.  Fallibilism rules out that first position in favour of the second. Neither  the human condition in particular nor our explanatory knowledge in  general will ever be perfect, nor even approximately perfect. We shall  always be at the  beginning  of infinity. These two interpretations of human progress and perfectibility have  historically inspired two broad branches of the Enlightenment which,  though they share attributes such as their rejection of authority, are so  different in important respects in that it is most unfortunate that they  share the same name. The utopian ‘Enlightenment’ is sometimes called  the Continental (European) Enlightenment to distinguish it from the  more fallibilist British Enlightenment, which began a little earlier and  PROBLEMS ARE SOLUBLE PROBLEMS ARE SOLUBLE PROBLEMS ARE SOLUBLE 66 the beginning of infinity took a very different course. (See, for instance, the historian Roy  Porter’s book  Enlightenment .) In my terminology, the Continental  Enlightenment understood that problems are soluble but not that they  are inevitable, while the British Enlightenment understood both equally.  Note that this is a classification of ideas, not of nations or even  individual thinkers: not all Enlightenment thinkers belong wholly to  one branch or the other; nor were all thinkers of the respective  Enlighten  ments born in the
========================================================
 know in considerable detail what is happening  there.  Somehow that jet happens in such a way that billions of years later,  on the other side of the universe, a chemical scum can know and predict  what the jet will do, and can understand why. That means that one  physical system – say, an astrophysicist’s brain – contains an accurate  working model of the other, the jet. Not just a superficial image (though  it contains that as well), but an explanatory theory that embodies the  same mathematical relationships and causal structure. That is scientific  knowledge. Furthermore, the faithfulness with which the one structure  resembles the other is steadily increasing. That constitutes the creation  of knowledge. Here we have physical objects very unlike each other,  and whose behaviour is dominated by  different laws of physics,  embodying the same mathematical and  causal structures – and doing  so ever more accurately over time. Of all the physical processes that  can occur in nature, only the creation of knowledge exhibits that  underlying unity. In Arecibo, Puerto Rico, there is a giant radio telescope, one of whose  many uses is in the Search For Extraterrestrial Intelligence ( SETI). In  an office in a building near the telescope there is a small domestic  refrigerator. Inside that refrigerator is a bottle of champagne, sealed  by a cork. Consider that cork. It is going to be removed from the bottle if and when  SETI  succeeds  in its mission to detect radio signals transmitted by an extraterrestrial  intelligence. Hence, if you were to keep a careful watch on the cork,  and one day saw it popping from the bottle, you could infer that an  extraterrestrial intelligence exists. The configuration of the cork is    what experimentalists call a ‘proxy’: a physical variable which can    be measured as a way of measuring another variable. (All scientific  measure      ments involve chains of proxies.) Thus we can also regard the  entire Arecibo observatory, including its staff and that bottle and its  cork, as a scientific instrument to detect distant people.  The behaviour of that humble cork is therefore extraordinarily  difficult to explain or predict. To predict it, you have to know whether  73 The Spark there really are people sending radio signals from various solar systems.  To explain it, you have to explain how you know about those people  and their attributes. Nothing less than that specific knowledge, which  depends among other things on subtle properties of the chemistry on  the planets of distant stars, can explain or predict with any accuracy  whether, and when, that cork will pop.  The  SETI  instrument is also remarkably finely tuned to its purpose.  Completely insensitive to the presence of several tonnes of people a few  metres away, and even to the tens of millions of tonnes of people on  the same planet, it detects only people on planets orbiting other stars,  and only if they are radio engineers. No other type of phenomenon on  Earth, or in the universe, is sensitive to what people are doing at  locations hundreds of light years away, let alone with that enormous  degree of discrimination.  This is made possible in part by the corresponding fact that few  types of matter are as prominent, at those distances, as that type of  scum. Specifically, the only phenomena that our best current instru- ments can detect at stellar distances are (1) extraordinarily luminous  ones such as stars (or, to be precise, only their surfaces); (2) a few  objects that obscure our view of those luminous objects; and (3)    the effects of certain types of knowledge. We can detect devices    such as lasers and radio transmitters that have been designed for the  purpose of communication; and we can detect components of planetary  atmospheres that could not be present in the absence of life. Thus those  types of knowledge are among the most prominent phenomena in    the universe. Note also that the  SETI  instrument is exquisitely adapted to detecting  something that has never yet been detected. Biological evolution could  never produce such an adaptation. Only scientific knowledge can. This  illustrates why non-explanatory knowledge cannot be universal. Like  all science, the  SETI  project can conjecture the existence of something,  calculate what some of its observable attributes would be, and then  construct an instrument to detect it. Non-explanatory systems cannot  cross the conceptual gap that an explanatory conjecture crosses, to  engage with unexperienced evidence or non-existent phenomena. Nor  is that true only of fundamental science:  if  such-and-such a load were  put on the proposed bridge it would collapse, says the engineer, and  74 the beginning of infinity such statements can be true and immensely valuable even if the bridge  is never even built, let alone subjected to such a load. Similar champagne bottles are stored in other laboratories. The  popping of each such cork signals a discovery about
========================================================
 put those terms in  quotation marks because those events are not in the least accidental.  They have all happened inevitably, according to deterministic laws of  physics. All of them were caused by the transporter.  Here is another situation where, if we are not careful, common sense  makes false assumptions about the physical world, and can make  descriptions of situations sound paradoxical even though the situations  themselves are quite straightforward. Dawkins gives an example in his  book  Unweaving the Rainbow ,   analysing the claim that a television  psychic was making accurate predictions: There are about 100,000 five-minute periods in a year. The probability  that any given watch, say mine, will stop in a designated five-minute  period is about 1 in 100,000. Low odds, but there are 10 million people  watching the [television psychic’s] show. If only half of them are wearing  watches, we could expect about 25 of those watches to stop in any given  minute. If only a quarter of these ring in to the studio, that is 6 calls,  more than enough to dumbfound a naive audience. Especially when you  add in the calls from people whose watches stopped the day before,  people whose watches didn’t stop but whose grandfather clocks did,  people who died of heart attacks and their bereaved relatives phoned in  to say that their ‘ticker’ gave out, and so on. As this example shows, the fact that certain circumstances can  explain  other events without being in any way involved in  causing  them is very  familiar despite being counter-intuitive. The ‘naive’ audience’s mistake  is a form of parochialism: they observe a phenomenon – people phoning  in because their watches stopped – but they are failing to understand  280 the beginning of infinity it as part of a wider phenomenon, most of which they do not observe.  Though the unobserved parts of that wider phenomenon have in no  way affected what we, the viewers, observe, they are essential to its  explanation. Similarly, common sense and classical physics contain the  parochial error that only one history exists. This error, built into our  language and conceptual framework, makes it sound odd to say that  an event can be in one sense extremely unlikely and in another certain  to happen. But there is nothing odd about it in reality.  We are now seeing the interior of the spaceship as an overwhelmingly  complex jumble of superposed objects. Most locations on board are  packed with people, some of them on very unusual errands, and all  unable to perceive each other. The spaceship itself is on many slightly  different courses, due to slightly different behaviours of the crew. Of  course we are ‘seeing’ this only in our mind’s eye. Our fictional laws  of physics ensure that no observer in the multiverse itself would see  anything like that. Consequently, on closer inspection (in our mind’s  eye), we also see that there is great order and regularity in that apparent  chaos. For instance, although there is a flurry of human figures in the  Captain’s chair, we see that most of them are the Captain; and although  there is a flurry of human figures in the Navigator’s chair, we see that  few of them are the Captain. Regularities of that kind are ultimately  due to the fact that all the universes, despite their differences, obey the  same laws of physics (including their initial conditions).  We also see that any particular instance of the Captain only ever  interacts with one instance of the Navigator, and one instance of the  First Officer; and those instances of the Navigator and First Officer  are precisely the ones that interact with each other. These regularities  are due to the fact that the histories are nearly autonomous: what  happens in each of them depends almost entirely on previous events  in that history alone – with transporter-induced voltage surges being  the only exceptions. In the story so far, this autonomy of the histories  is rather a trivial fact, since we began by making the  universes  autonomous. But it is going to be worth becoming even more pedantic  for a moment: what exactly is the difference between the instance of  you that I can interact with and the ones that are imperceptible to me?  The latter are ‘in other universes’ – but, remember, universes consist  only of the objects in them, so that amounts only to saying I can see  281 The Multiverse the ones that I can see. The upshot is that our laws of physics must  also say that every object carries within it information about which  instances of it could interact with which instances of other objects  (except when the instances are fungible, when there is no such thing  as ‘which’). Quantum theory describes such information. It is known  as  entanglement  information.* So far in the story we have set up a vast, complex world which looks  very unfamiliar in our mind’s eye, but to the overwhelming majority of  the inhabitants looks almost exactly like the single universe of our  everyday
========================================================
 indeed anything at all, other than their meme-mandated behaviours.  It can perpetuate itself only by suppressing its members’ self-expression  387 The Evolution of Culture and breaking their spirits, and its memes are exquisitely adapted to  doing this. Dynamic societies But our society (the West) is not a static society. It is the only known  instance of a long-lived dynamic (rapidly changing) society. It is unique  in history for its ability to mediate long-term, rapid, peaceful change  and improvement, including improvements in the broad consensus  about values and aims, as I described in Chapter 13. This has been  made possible by the emergence of a radically different class of memes  which, though still ‘selfish’, are not necessarily harmful to individuals.  To explain the nature of these new memes, let me pose the question:  what sort of meme can cause itself to be replicated for long periods  in  a rapidly changing environment ? In such an environment, people are  continually being faced with unpredictable problems and opportunities.  Hence their needs and wishes are changing unpredictably too. How  can a meme remain unchanged under such a regime? The memes of a  static society remain unchanged by effectively eliminating all the  individuals’ choices: people choose neither which ideas to acquire nor  which to enact. Those memes also combine to make the society static,  so that people’s circumstances vary as little as possible. But once the  stasis has broken down, and people are choosing, they will choose, in  part, according to their individual circumstances and ideas, in which  case memes will face selection criteria that vary unpredictably from  recipient to recipient as well as over time.  To be transferred to a single person, a meme need seem useful only  to that person. To be transferred to a group of similar people under  unchanging circumstances, it need be only a parochial truth. But what  sort of idea is best suited to getting itself adopted many times in  succession by many people who have diverse, unpredictable objectives?  A  true  idea is a good candidate. But not just any truth will do. It must  seem useful to  all  those people, for it is they who will be choosing  whether to enact it or not. ‘Useful’ in this context does not necessarily  mean functionally useful: it refers to any property that can make people  want to adopt an idea and enact it, such as being interesting, funny,  elegant, easily remembered, morally right and so on. And the best way  388 the beginning of infinity to  seem  useful to diverse people under diverse, unpredictable cir  - cumstances is to  be  useful. Such an idea is, or embodies, a truth in the  broadest sense: factually true if it is an assertion of fact, beautiful if it  is an artistic value or behaviour, objectively right if it is a moral value,  funny if it is a joke, and so on. The ideas with the best chance of surviving through many genera  tions  of change are truths with reach – deep truths. People are fallible; they  often have preferences for false, shallow, useless or morally wrong ideas.  But  which  false ideas they prefer differs from one person to another,  and changes with time. Under changed circumstances, a specious  falsehood or parochial truth can survive only by luck. But a true, deep  idea has an objective reason to be considered useful by people with  diverse purposes over long periods. For instance, Newton’s laws are  useful for building better cathedrals, but also for building better bridges  and designing better artillery. Because of this reach, they get themselves  remembered and enacted by all sorts of people, many of them vehemently  opposed to each other’s objectives, over many generations. This is the  kind of idea that has a chance of becoming a long-lived meme in a  rapidly changing society.  In fact such memes are not merely  capable  of surviving under rapidly  changing criteria of criticism, they positively rely on such criticism for  their faithful replication. Unprotected by any enforcement of the status  quo or suppression of people’s critical faculties, they are criticized,  but  so are their rivals , and the rivals fare worse, and are not enacted. In  the absence of such criticism, true ideas no longer have that advantage  and can deteriorate or be superseded. Rational and anti-rational memes Thus, memes of this new kind, which are created by rational and critical  thought, subsequently also depend on such thought to get themselves  replicated faithfully. So I shall call them  rational memes . Memes of the  older, static-society kind, which survive by disabling their holders’  critical faculties, I shall call  anti-rational memes . Rational and anti- rational memes have sharply differing properties, originating in their  fundamentally different replication strategies. They are about as  different from each other as they both are from genes. 389 The Evolution of Culture If a certain type of hobgoblin has the property
========================================================
 computers  – or of the laws of physics. If measuring the goats in whole numbers  of inches is insufficient for a particular application, use whole numbers  of  tenths  of inches, or billionths. The same holds for all other appli- cations: the laws of physics are such that the behaviour of any physical  object – and that includes any other computer – can be simulated with  any desired accuracy by a universal digital computer. It is just a matter  of approximating continuously variable quantities by a sufficiently fine  grid of discrete ones.  Because of the necessity for error-correction,  all  jumps to universality  occur in digital systems. It is why spoken languages build words out  of a finite set of elementary sounds: speech would not be intelligible  if it were analogue. It would not be possible to repeat, nor even to  remember, what anyone had said. Nor, therefore, does it matter that  universal writing systems cannot perfectly represent analogue inform- ation such as tones of voice. Nothing can represent those perfectly. For  the same reason, the sounds themselves can represent only a finite  number of possible meanings. For example, humans can distinguish  between only about seven different sound volumes. This is roughly  reflected in standard musical notation, which has approximately seven  different symbols for loudness (such as  p ,  mf ,  f , and so on). And, for  the same reason, speakers can only  intend  a finite number of possible  meanings with each utterance. Another striking connection between all those diverse jumps to  universality is that they all happened on Earth. In fact all known jumps  to universality happened under the auspices of human beings – except  one, which I have not mentioned yet, and from which all the others,  historically, emerged. It happened during the early evolution of life. Genes in present-day organisms replicate themselves by a complicated  and very indirect chemical route. In most species they act as templates  for forming stretches of a similar molecule, RNA. Those then act as  programs which direct the synthesis of the body’s constituent chemicals,  143 The Jump to Universality especially enzymes, which are  catalysts . A catalyst is a kind of con  - structor – it promotes a change among other chemicals while remaining  unchanged itself. Those catalysts in turn control all the chemical  production and regulatory functions of an organism, and hence define  the organism itself, crucially including a process that makes a copy of  the DNA. How that intricate mechanism evolved is not essential here,  but for definiteness let me sketch one possibility.  About four billion years ago – soon after the surface of the Earth  had cooled sufficiently for liquid water to condense – the oceans were  being churned by volcanoes, meteor impacts, storms and much stronger  tides than today’s (because the moon was closer). They were also highly  active chemically, with many kinds of molecules being continually  formed and transformed, some spontaneously and some by catalysts.  One such catalyst happened to catalyse the formation of some of the  very kinds of molecules from which it itself was formed. That catalyst  was not alive, but it was the first hint of life. It had not yet evolved to be a well-targeted catalyst, so it also  accelerated the production of some other chemicals, including variants  of itself. Those that were best at promoting their own production (and  inhibiting their own destruction) relative to other variants became  more numerous. They too promoted the construction of variants of  themselves, and so evolution continued. Gradually, the ability of these catalysts to promote their own pro  - duction became robust and specific enough for it to be worth calling  them replicators. Evolution produced replicators that caused themselves  to be replicated ever faster and more reliably.  Different replicators began to join forces in groups, each of whose  members specialized in causing one part of a complex web of chemical  reactions whose net effect was to construct more copies of the entire  group. Such a group was a rudimentary organism. At that point, life  was at a stage roughly analogous to that of non-universal printing, or  Roman numerals: it was no longer a case of each replicator for itself,  but there was still no universal system being customized or programmed  to produce specific substances. The most successful replicators may have been RNA molecules.    They have catalytic properties of their own, depending on the precise  sequence of their constituent molecules (or bases, which are similar to  144 the beginning of infinity those of DNA). As a result, the replication process became ever less  like straightforward catalysis and ever more like programming – in a  language, or genetic code, that used bases as its alphabet. Genes are replicators that can be interpreted as instructions in a  genetic code. Genomes are groups of genes that are dependent on
========================================================
 the  kind of dictatorship that has secret police who come for you in the  middle of the night if you criticize them. Virtually all commentators have responded to these paradoxes and  no-go theorems in a mistaken and rather revealing way: they  regret   them. This illustrates the confusion to which I am referring.  They wish  that these theorems of pure mathematics were false.  If only mathem- atics permitted it, they complain, we human beings could set up a    just society that makes its decisions rationally. But, faced with the  impossibility of that, there is nothing left for us to do but to decide  which injustices and irrationalities we like best, and to enshrine them  in law. As Webster wrote, of the apportionment problem, ‘That which  cannot be done perfectly must be done in a manner as near perfection  as can be. If exactness cannot, from the nature of things, be attained,  344 the beginning of infinity then the nearest practicable approach to exactness ought to be made.’ But what sort of ‘perfection’ is a  logical contradiction ? A logical  contradiction is nonsense. The truth is simpler: if your conception of  justice conflicts with the demands of logic or rationality then it is unjust.  If your conception of rationality conflicts with a mathematical theorem  (or, in this case, with many theorems) then your conception of ration- ality is irrational. To stick stubbornly to logically impossible values  not only guarantees failure in the narrow sense that one can never meet  them, it also forces one to reject optimism (‘every evil is due to lack of  knowledge’), and so deprives one of the means to make progress.  Wishing for something that is logically impossible is a sign that there  is something better to wish for. Moreover, if my conjecture in Chapter  8 is true, an impossible wish is ultimately  uninteresting  as well. We need something better to wish for. Something that is not in    - compatible with logic, reason or progress. We have already encountered  it. It is the basic condition for a political system to be capable of making  sustained progress: Popper’s criterion that the system facilitate the  removal of bad policies and bad governments without violence. That  entails abandoning ‘who should rule?’ as a criterion for judging political  systems. The entire controversy about apportionment rules and all  other issues in social-choice theory has traditionally been framed by  all concerned in terms of ‘who should rule?’: what is the right number  of seats for each state, or for each political party? What does the group  – presumed entitled to rule over its subgroups and individuals – ‘want’,  and what institutions will get it what it ‘wants’? So let us reconsider collective decision-making in terms of Popper’s  criterion instead. Instead of wondering earnestly which of the self- evident yet mutually inconsistent criteria of fairness, representativeness  and so on are the most self-evident, so that they can be entrenched, we  judge such criteria, along with all other actual or proposed political  institutions, according to how well they promote the removal of bad  rulers and bad policies. To do this, they must embody traditions    of peaceful, critical discussion – of rulers, policies and the political  institutions themselves. In this view, any interpretation of the democratic process as merely  a way of consulting the people to find out who should rule or what  policies to implement misses the point of what is happening. An election  345 Choices does not play the same role in a rational society as consulting an oracle  or a priest, or obeying orders from the king, did in earlier societies. The  essence of democratic decision-making is not the choice made by the  system at elections, but the ideas created between elections. And elections  are merely one of the many institutions whose function is to allow such  ideas to be created, tested, modified and rejected. The voters are not a  fount of wisdom from which the right policies can be empirically  ‘derived’. They are attempting, fallibly, to explain the world and thereby  to improve it. They are, both individually and collectively, seeking the  truth – or should be, if they are rational. And there  is  an objective truth  of the matter. Problems are soluble. Society is not a zero-sum game: the  civilization of the Enlightenment did not get where it is today by cleverly  sharing out the wealth, votes or anything else that was in dispute when  it began. It got here by creating  ex nihilo . In particular, what voters are  doing in elections is not synthesizing a decision of a superhuman being,  ‘Society’. They are choosing which experiments are to be attempted  next, and (principally) which are to be abandoned because there is no  longer a good explanation for why they are best. The politicians, and  their policies, are those experiments.  When one uses no-go theorems such as Arrow’s to model real  decision-making, one has to assume – quite
========================================================
 Popper could have made the same point by asking his audience to  imitate , rather than merely to observe. The logic would have been the  same: under what explanatory theory should they ‘imitate’?  Whom  404 the beginning of infinity should they imitate? Popper? In that case, should they walk to the  podium, push him out of the way, and stand where he had been  standing? If not, should they at least turn to face the rear of the room,  to imitate where he was facing? Should they imitate his heavy Austrian  accent, or should they speak in their normal voices, because he was  speaking in his normal voice? Or should they do nothing special at the  time, but merely include such demonstrations in their lectures when  they themselves became professors of philosophy? There are infinitely  many possible interpretations of ‘imitate Popper’, each defining a  different behaviour for the imitator. Many of those ways would look  very different from each other. Each way corresponds to a different  theory of what ideas, in Popper’s mind, were causing the observed  behaviour. So there is no such thing as ‘just imitating the behaviour’ – still less,  therefore, can one discover those  ideas  by imitating it. One needs to  know the ideas  before  one can imitate the behaviour. So imitating  behaviour cannot be how we acquire memes. The hypothetical genes that caused meme replication by imitation  would also have to specify  whom  to imitate. Blackmore, for instance,  suggests that the criterion may be ‘imitate the best imitators’. But this  is impossible for the same reason. One can only judge how well  someone is imitating if one already knows, or has guessed,  what  (which  aspect of behaviour, and whose) they are imitating, and which of the  circumstances they are taking into account and how. The same holds if the behaviour consists of  stating  the memes. As  Popper remarked, ‘It is impossible to speak in such a way that you  cannot be misunderstood.’ One can only state the explicit content,  which is insufficient to define the meaning of a meme or anything else.  Even the most explicit of memes – such as laws – have inexplicit content  without which they cannot be enacted. For example, many laws refer  to what is ‘reasonable’. But no one can define that attribute accurately  enough for, say, a person from a different culture to be able to apply  the definition in judging a criminal case. Hence we certainly do not  learn what ‘reasonable’ means by hearing its meaning  stated . But we  do learn it, and the versions of it that are learned by people in the same  culture are sufficiently close for laws based on it to be practicable.  In any case, as I remarked in the previous chapter, we do not explicitly  405 The Evolution of Creativity know the rules by which we behave. We know the rules, meanings and  patterns of speech of our native language largely inexplicitly, yet we  pass its rules on with remarkable fidelity to the next generation –  including the ability to apply them in situations the new holder has  never experienced, and including patterns of speech that people ex    - plicitly try to prevent the next generation from replicating. The real situation is that people need inexplicit knowledge to under- stand laws and other explicit statements, not vice versa. Philosophers  and psychologists work hard to discover, and to make explicit, the  assumptions that our culture tacitly makes about social institutions,  human nature, right and wrong, time and space, intention, causality,  freedom, necessity and so on. But we do not acquire those assump  -  tions by reading the results of such research: it is entirely the other    way round. If behaviour is impossible to imitate without prior knowledge of the  theory causing the behaviour, how it is that apes, famously, can ape?  They have memes: they can learn a new way of opening a nut by  watching another ape that already knows that way. How is it that apes  are not confused by the infinite ambiguity of what it means to imitate?  Even parrots, famously, parrot: they can commit to memory dozens    of sounds that they have heard, and repeat them later. How do they  cope with the ambiguity of which sounds to imitate, and when to    repeat them? They cope with it by knowing the relevant inexplicit theories in  advance. Or, rather, their genes know them. Evolution has built into  the genes of parrots an implicit definition of what ‘imitating’ means:  to them, it means recording sequences of sounds that meet some inborn  criterion, and later replaying them under conditions that meet some  other inborn criterion. An interesting fact follows, about parrot physi- ology: the parrot’s brain must also contain a translation system that  analyses incoming nerve signals from the ears and generates outgoing  ones that will cause the parrot’s vocal cords to play the same sounds.  That translation requires some quite sophisticated computation, which  is encoded in genes, not memes. It is
========================================================
 supercomputers all have an  identical repertoire of computations.  Another thing that they have in common is that they are all  digital :  they operate on information in the form of discrete values of physical  variables, such as electronic switches being on or off, or cogs being    at one of ten positions. The alternative, ‘analogue’, computers, such    as slide rules, which represent information as continuous physical  variables, were once ubiquitous but are hardly ever used today. That  is because a modern digital computer can be programmed to imitate  any of them, and to outperform them in almost any application. The  jump to universality in digital computers has left analogue computation  behind. That was inevitable, because there is no such thing as a universal  analogue computer.  That is because of the need for  error correction : during lengthy  computations, the accumulation of errors due to things like imperfectly  constructed components, thermal fluctuations, and random outside  influences makes analogue computers wander off the intended compu- tational path. This may sound like a minor or parochial consideration.  But it is quite the opposite. Without error-correction all information  processing, and hence all knowledge-creation, is necessarily bounded.  Error-correction is the beginning of infinity.  For example, tallying is universal only if it is digital. Imagine that  some ancient goatherds had tried to tally the total  length  of their flock  instead of the number. As each goat left the enclosure, they could reel  out some string of the same length as the goat. Later, when the goats  returned, they could reel that length back in. When the whole length  had been reeled back in, that would mean that all the goats had  returned. But in practice the outcome would always be at least a little  long or short, because of the accumulation of measurement errors. For  any given accuracy of measurement, there would be a maximum  number of goats that could be reliably tallied by this ‘analogue tallying’  141 The Jump to Universality system. The same would be true of all arithmetic performed with those  ‘tallies’. Whenever the strings representing several flocks were added  together, or a string was cut in two to record the splitting of a flock,  and whenever a string was ‘copied’ by making another of the same  length, there would be errors. One could mitigate their effect by  performing each operation many times, and then keeping only the  outcome of median length. But the operations of comparing or duplicat- ing lengths can themselves be performed only with finite accuracy, and  so could not reduce the rate of error accumulation per step below that  level of accuracy. That would impose a maximum number of consecutive  operations that could be performed before the result became useless  for a given purpose – which is why analogue computation can never  be universal. What is needed is a system that takes for granted that errors will  occur, but  corrects  them once they do – a case of ‘problems are inev- itable, but they are soluble’ at the lowest level of information-processing  emergence. But, in analogue computation, error correction runs into  the basic logical problem that there is no way of distinguishing an  erroneous value from a correct one at sight, because it is in the very  nature of analogue computation that every value  could  be correct. Any  length of string might be the right length.  And that is not so in a computation that confines itself to whole  numbers. Using the same string, we might represent whole numbers as  lengths of string in whole numbers of inches. After each step, we trim  or lengthen the resulting strings to the nearest inch. Then errors would  no longer accumulate. For example, suppose that the measurements  could all be done to a tolerance of a tenth of an inch. Then all errors  would be detected and eliminated after each step, which would eliminate  the limit on the number of consecutive steps.  So all universal computers are digital; and all use error-correction  with the same basic logic that I have just described, though with many  different implementations. Thus Babbage’s computers assigned only  ten different meanings to the whole continuum of angles at which a  cogwheel might be oriented. Making the representation digital in that  way allowed the cogs to carry out error-correction automatically: after  each step, any slight drift in the orientation of the wheel away from  its ten ideal positions would immediately be corrected back to the  142 the beginning of infinity nearest one as it clicked into place. Assigning meanings to the whole  continuum of angles would nominally have allowed each wheel to  carry (infinitely) more information; but, in reality, information that  cannot be reliably retrieved is not really being stored. Fortunately, the limitation that the information being processed must  be digital does not detract from the universality of digital
========================================================
 ours.  How  should  we understand the existence of the distinctively human  emergent phenomena such as creativity and choice, in the light of the  fact that part of our behaviour is caused by autonomous entities whose  content we do not know? And, worse, given that we are liable to be  systematically misled by those entities about the reasons for our own  thoughts, opinions and behaviour? The basic answer is that it should not come as a surprise that we  can be badly mistaken in any of our ideas, even about ourselves, and  even when we feel strongly that we are right. So we should respond  no differently, in principle, from how we respond to the possibility of  being in error for any other reason. We are fallible, but through  conjecture, criticism and seeking good explanations we may correct  some of our errors. Memes hide, but, just as with the optical blind  spot, there is nothing to prevent our using a combination of explanation  and observation to detect a meme and discover its implicit content  indirectly.  For example, whenever we find ourselves enacting a complex or  narrowly defined behaviour that has been accurately repeated from  one holder to the next, we should be suspicious. If we find that enacting  this behaviour thwarts our efforts to attain our personal objectives, or  is faithfully continued when the ostensible justifications for it disappear,  we should become more suspicious. If we then find ourselves explaining  our own behaviour with bad explanations, we should become still  more suspicious. Of course, at any given point we may fail either to  notice these things or to discover the true explanation of them. But  failure need not be permanent in a world in which all evils are due to  lack of knowledge. We failed at first to notice the non-existence of a  force of gravity. Now we understand it. Locating hang-ups is, in the  last analysis, easier. Another thing that should make us suspicious is the presence of the  conditions  for anti-rational meme evolution, such as deference to  authority, static subcultures and so on. Anything that says ‘Because I  say so’ or ‘It never did me any harm,’ anything that says ‘Let us suppress  396 the beginning of infinity criticism of our idea because it is true,’ suggests static-society thinking.  We should examine and criticize laws, customs and other institutions  with an eye to whether they set up conditions for anti-rational memes  to evolve. Avoiding such conditions is the essence of Popper’s criterion. The Enlightenment is the moment at which explanatory knowledge  is beginning to assume its soon-to-be-normal role as the most important  determinant of physical events. At least it could be: we had better  remember that what we are attempting – the sustained creation of  knowledge – has never worked before. Indeed, everything that we  shall ever try to achieve from now on will never have worked before.  We have, so far, been transformed from the victims (and enforcers) of  an eternal status quo into the mainly passive recipients of the benefits  of relatively rapid innovation in a bumpy transition period. We now  have to accept, and rejoice in bringing about, our next transformation:  to active agents of progress in the emerging rational society – and  universe. terminology Culture  A set of shared ideas that cause their holders to behave alike  in some ways. Rational meme  An idea that relies on the recipients’ critical faculties  to cause itself to be replicated. Anti-rational meme  An idea that relies on disabling the recipients’  critical faculties to cause itself to be replicated. Static culture/society  One whose changes happen on a timescale  longer than its members can notice. Such cultures are dominated by  anti-rational memes. Dynamic culture/society   One that is dominated by rational memes. meanings of ‘the beginning of infinity’  encountered in this chapter – Biological evolution was merely a finite preface to the main story of  evolution, the unbounded evolution of memes. – So was the evolution of anti-rational memes in static societies. 397 The Evolution of Culture summary Cultures consist of memes, and they evolve. In many ways memes are  analogous to genes, but there are also profound differences in the way  they evolve. The most important differences are that each meme has  to include its own replication mechanism, and that a meme exists  alternately in two different physical forms: a mental representation  and a behaviour. Hence also a meme, unlike a gene, is separately  selected, at each replication, for its ability to cause behaviour and for  the ability of that behaviour to cause new recipients to adopt the meme.  The holders of memes typically do not know why they are enacting  them: we enact the rules of grammar, for instance, much more ac    curately  than we are able to state them. There are only two basic strategies of  meme replication: to help prospective holders or to disable the holders’  critical faculties. The two
========================================================
 hostile that they barely permit astrophysicists  at all. So, if we imagine all the values consistent with the emergence of  astrophysicists arrayed on a line, then the anthropic explanation leads  us to expect the measured value to fall at some typical point, not too  close to the middle or to either end. 100 the beginning of infinity However – and here we are reaching Sciama’s main conclusion – that  prediction changes radically if there are  several  constants to explain.  For although any one constant is unlikely to be near the edge of its  range, the more constants there are, the more likely it is that at least  one of them will be. This can be illustrated pictorially as follows, with  our bull’s-eye replaced by a line segment, a square, a cube . . . and we  can imagine this sequence continuing for as many dimensions as there  are fine-tuned constants in nature. Arbitrarily define ‘near the edge’ as  meaning ‘within 10 per cent of the whole range from it’. Then in the  case of one constant, as shown in the diagram, 20 per cent of its possible  values are near one of the two edges of the range, and 80 per cent are  ‘away from the edge’. But with two constants a pair of values has to  satisfy two constraints in order to be ‘away from the edge’. Only 64  per cent of them do so. Hence 36 per cent are near the edge. With three  constants, nearly half the possible choices are near the edge. With 100  constants, over 99.9999999 per cent of them are. Whatever anthropic reasoning predicts about the values of multiple    constants, it predicts will only just happen. So, the more constants are involved, the closer to having no astro- physicists a typical universe- with -astrophysicists is. It is not known  how many constants are involved, but it seems to be several, in which  101 Creation case the overwhelming majority of universes in the anthropically  selected region would be close to its edge. Hence, Sciama concluded,  the anthropic explanation predicts that the universe is  only just  capable  of producing astrophysicists – almost the opposite prediction from the  one that it makes in the case of one constant. On the face of it, this might in turn seem to explain another great  unsolved scientific mystery, known as ‘Fermi’s problem’, named after  the physicist Enrico Fermi, who is said to have asked, ‘ Where are they? ’  Where are the extraterrestrial civilizations? Given the Principle of  Mediocrity, or even just what we know of the galaxy and the universe,  there is no reason to believe that the phenomenon of astrophysicists  is unique to our planet. Similar conditions presumably exist in many  other solar systems, so why would some of them not produce similar  outcomes? Moreover, given the timescales on which stars and galaxies  develop, it is overwhelmingly unlikely that any given extraterrestrial  civilization is currently at a similar state of technological development  to ours: it is likely to be millions of years younger (i.e. non-existent)  or older. The older civilizations have had plenty of time to explore the  galaxy – or at least to send robot space probes or signals. Fermi’s  problem is that we do not see any such civilizations, probes or signals.  Many candidate explanations have been proposed, and none of them,  so far, are very good. The anthropic explanation of fine-tuning, in the  light of Sciama’s argument, might seem to solve the problem neatly:    if the constants of physics in our universe are only just capable of  producing astrophysicists, then it is not surprising that this event has  happened only once, since its happening twice independently in the  same universe would be vanishingly unlikely. Unfortunately, that turns out to be a bad explanation too, because  focusing on fundamental  constants  is parochial: there is no relevant  difference between (1) ‘the same’ laws of physics with different constants  and (2) different laws of physics. And there are infinitely many logically  possible laws of physics. If they were all instantiated in real universes  – as has been suggested by some cosmologists, such as Max Tegmark –    it would be statistically certain that our universe is exactly on the edge  of the astrophysicist-producing class of universes.  We know that that cannot be so from an argument due to Feynman  (which he applied to a slightly different problem). Consider the class  102 the beginning of infinity of all possible universes that contain astrophysicists, and consider what  else  most of them contain. In particular, consider a sphere just large  enough to contain your own brain. If you are interested in explaining  fine-tuning, your brain in its current state counts as an ‘astrophysicist’  for these purposes. In the class of all universes that contain astro- physicists, there are many that contain a sphere whose interior is  perfectly identical to the interior of your sphere, including every detail  of your brain. But in the vast majority of those universes there is
========================================================
 present and future – is as good as it could possibly be. The term  was first used to describe an argument of Leibniz (1646–1716) that  God, being ‘perfect’, would have created nothing less than ‘the best of  all possible worlds’. Leibniz believed that this idea solved the ‘problem  of evil’, which I mentioned in Chapter 4: he proposed that all apparent  evils in the world are outweighed by good consequences that are too  remote to be known. Similarly, all apparently good events that  fail  to  200 the beginning of infinity happen – including all improvements that humans are unsuccessful in  achieving – fail because they would have had bad consequences that  would have outweighed the good. Since consequences are determined by the laws of physics, the larger  part of Leibniz’s claim must be that the laws of physics are the best  possible too. Alternative laws that made scientific progress easier, or  made disease an impossible phenomenon, or made even one disease  slightly less unpleasant – in short, any alternative that would  seem   to be an improvement upon our actual history with all its plagues,  tortures, tyrannies and natural disasters – would in fact have been even  worse on balance, according to Leibniz. That theory is a spectacularly bad explanation. Not only can  any   observed sequence of events be explained as ‘best’ by that method, an  alternative Leibniz could equally well have claimed that we live in the  worst  of all possible worlds, and that every good event is necessary in  order to prevent something even better from happening. Indeed, some  philosophers, such as Arthur Schopenhauer, have claimed just that.  Their stance is called philosophical ‘pessimism’. Or one could claim  that the world is exactly halfway between the best possible and the  worst possible – and so on. Notice that, despite their superficial differ- ences, all those theories have something important in common: if any  of them were true, rational thought would have almost no power to  discover true explanations. For, since we can always imagine states of  affairs that seem better than what we observe, we would always be  mistaken that they  were  better,  no matter how good our explanations  were . So, in such a world, the true explanations of events are never  even imaginable. For instance, in Leibniz’s ‘optimistic’ world, whenever  we try to solve a problem and fail, it is because we have been thwarted  by an unimaginably vast intelligence that determined that it was best  for us to fail. And, still worse, whenever someone rejects reason and  decides instead to rely on bad explanations or logical fallacies – or, for  that matter, on pure malevolence – they still achieve, in every case, a  better outcome on balance than the most rational and benevolent  thought possibly could have. This does not describe an explicable  world. And that would be very bad news for us, its inhabitants. Both  the original ‘optimism’ and the original ‘pessimism’ are close to pure  pessimism as I shall define it. 201 Optimism In everyday usage, a common saying is that ‘an optimist calls a glass  half full while a pessimist calls it half empty’. But those attitudes are  not what I am referring to either: they are matters not of philosophy  but of psychology – more ‘spin’ than substance. The terms can also  refer to moods, such as cheerfulness or depression, but, again, moods  do not necessitate any particular stance about the future: the statesman  Winston Churchill suffered from intense depression, yet his outlook  on the future of civilization, and his specific expectations as wartime  leader, were unusually positive. Conversely the economist Thomas  Malthus, a notorious prophet of doom (of whom more below), is said  to have been a serene and happy fellow, who often had his companions  at the dinner table in gales of laughter.  Blind  optimism  is  a stance towards the future. It consists of proceed- ing as if one knows that the bad outcomes will not happen. The  opposite approach, blind pessimism, often called the  precautionary  principle , seeks to ward off disaster by avoiding everything not known  to be safe. No one seriously advocates either of these two as a universal  policy, but their assumptions and their arguments are common, and  often creep into people’s planning. Blind optimism is also known as ‘overconfidence’ or ‘recklessness’.  An often cited example, perhaps unfairly, is the judgement of the builders  of the ocean liner  Titanic  that it was ‘practically unsinkable’. The largest  ship of its day, it sank on its maiden voyage in 1912. Designed to survive  every foreseeable disaster, it collided with an iceberg in a manner that  had not been foreseen. A blind pessimist argues that there is an inherent  asymmetry between good and bad consequences: a successful maiden  voyage cannot possibly do as much good as a disastrous one can do  harm. As Rees points out, a single catastrophic consequence of an  otherwise beneficial
========================================================
 first is an attribute  of deep ideas, the second an attribute of deep silliness. By confusing  them, one ascribes to the best art and philosophy the qualities of the  worst. Since, in that view, readers, viewers and critics can attribute any  meaning they choose to the second kind of ambiguity, bad philosophy  declares the same to be true of all knowledge: all meanings are equal,  and none of them is objectively true. One then has a choice between  complete nihilism or regarding  all  ‘ambiguity’ as a good thing in those  fields. Horgan chooses the latter option: he classifies art and philosophy  as ‘ironic’ fields, irony being the presence of multiple conflicting mean- ings in a statement.  However, unlike the postmodernists, Horgan thinks that science and  mathematics are the shining exceptions to all that. They alone are  capable of non-ironic knowledge. But there is also, he concludes, such  a thing as  ironic science  – the kind of science that cannot ‘resolve  questions’ because, essentially, it is just philosophy or art. Ironic science  can  continue indefinitely, but that is precisely because it never resolves  anything; it never discovers objective truth. Its only value is in the    eye of the beholder. So the future, according to Horgan, belongs    to ironic knowledge. Objective knowledge has already reached its  ultimate bounds. 449 The Beginning Horgan surveys some of the open questions of fundamental science,  and judges them all either ‘ironic’ or non-fundamental, in support of  his thesis. But that conclusion was made inevitable by his premises  alone. For consider the prospect of  any  future discovery that would  constitute fundamental progress. We cannot know what it is, but bad  philosophy can already split it, on principle, into a new rule of thumb  and a new ‘interpretation’ (or explanation). The new rule of thumb  cannot possibly be fundamental: it will just be another equation.    Only a trained expert could tell the difference between it and the old  equation. The new ‘interpretation’ will by definition be pure philosophy,  and hence must be ‘ironic’. By this method, any potential progress can  be pre-emptively reinterpreted as non-progress.  Horgan rightly points out that his prophecy cannot be proved false  by placing it in the context of previous failed prophecies. The fact that  Michelson was wrong about the achievements of the nineteenth century,  and Lagrange about those of the seventeenth, does not imply that  Horgan was wrong about those of the twentieth. However, it so  happens that our current scientific  knowledge  includes a historically  unusual number of deep, fundamental problems. Never before in the  history of human thought has it been so obvious that our knowledge  is tiny and our ignorance vast. And so, unusually, Horgan’s pessimism  contradicts existing knowledge as well as being a prophetic fallacy.  For example, the problem-situation of fundamental physics today has  a radically different structure from that of 1894. Although physicists  then were aware of some phenomena and theoretical issues which we  now recognize as harbingers of the revolutionary explanations to come,  their importance was unclear at the time. It was hard to distinguish  those harbingers from anomalies that would eventually be cleared up  with existing explanations plus the tweaking of the ‘sixth place of  decimals’ or minor terms in a formula. But today there is no such  excuse for denying that some of our problems are fundamental. Our  best theories are telling us of profound mismatches between themselves  and the reality that they are supposed to explain. One of the most blatant examples of that is that physics currently  has  two  fundamental ‘systems of the world’ – quantum theory and the  general theory of relativity – and they are radically inconsistent. There  are many ways of characterizing this inconsistency – known as the  450 the beginning of infinity problem of quantum gravity – corresponding to the many proposals  for solving it that have been tried without success. One aspect is the  ancient tension between the discrete and the continuous. The resolution  that I described in Chapter 11, in terms of continuous clouds of fungible  instances of a particle with diverse discrete attributes, works only if  the spacetime in which this happens is itself continuous. But if spacetime  is affected by the gravitation of the cloud, then it would acquire discrete  attributes. In cosmology, there has been revolutionary progress even in the few  years since  The End of Science  was written – and also since I wrote  The Fabric of Reality  soon afterwards. At the time, all viable cosmo- logical theories had the expansion of the universe gradually slowing  down, due to gravity, ever since the initial explosion at the Big Bang  and for ever in the future. Cosmologists were trying to determine  whether, despite slowing down, its expansion rate was sufficient to  make the universe expand for ever
========================================================
 However, some flowers are white (at least  to us – they may have colours that we cannot see and insects can), but  we still find their shapes beautiful. All flowers do contrast with their  background in some sense – that is a precondition for being used for  signalling – but a spider in the bath contrasts with its background even  more, and there is no widespread consensus that such a sight is  beautiful. As for symmetry: again, spiders are quite symmetrical, while  some flowers, such as orchids, are very unsymmetrical, yet we do not  find them any less attractive for that. So I do not think that symmetry,  colour and contrast are all that we are seeing in flowers when we  imagine that we are seeing beauty. A sort of mirror image of that objection is that there are other things  in nature that we also find beautiful – things that are not results of either  human creativity or co-evolution across a gap: the night sky; waterfalls;  sunsets. So why not flowers too? But the cases are not alike. Those things  may be attractive to look at, but they have no appearance of design.  They are analogous not to Paley’s watch, but to the sun as a timekeeper.  One cannot explain why the watch is as it is without referring to  timekeeping, because it would be useless for timekeeping if it had been  made slightly differently. But, as I mentioned, the sun would still be    useful for keeping time even if the solar system were altered. Similarly,  Paley might have found a stone that looked attractive. He might well  have taken it home to use as an ornamental paperweight. But he would  not have sat down to write a monograph about how changing any detail  of the stone would have made it incapable of serving that function,  because that would not have been so. The same is true of the night sky,  waterfalls and almost all other natural phenomena. But flowers do have  the appearance of design for beauty: if they looked like leaves, or roots,  they would lose their universal appeal. Displace even one petal, and  there would be diminishment. We know what the watch was designed for, but we do not know  what beauty is. We are in a similar position to an archaeologist who  finds inscriptions in an unknown language in an ancient tomb: they  look like writing and not just meaningless marks on the walls. Con  - ceivably this is mistaken, but they look as though they were inscribed  there for a purpose. Flowers are like that: they have the appearance    364 the beginning of infinity of having been evolved for a purpose which we call ‘beauty’, which we  can (imperfectly) recognize, but whose nature is poorly understood. In the light of these arguments I can see only one explanation for  the phenomenon of flowers being attractive to humans, and for the  various other fragments of evidence I have mentioned. It is that the  attribute we call beauty is of two kinds. One is a parochial kind of  attractiveness, local to a species, to a culture or to an individual. The  other is unrelated to any of those: it is universal, and as objective as  the laws of physics. Creating either kind of beauty requires knowledge;  but the second kind requires knowledge with universal reach. It reaches  all the way from the flower genome, with its problem of competitive  pollination, to human minds which appreciate the resulting flowers as  art. Not great art – human artists are far better, as is to be expected.  But with the hard-to-fake appearance of design for beauty. Now, why do  humans  appreciate objective beauty, if there has been  no equivalent of that co-evolution in our past? At one level the answer  is simply that we are universal explainers and can create knowledge  about anything. But still, why did we want to create aesthetic know- ledge in particular? It is because we  did  face the same problem as the  flowers and the insects. Signalling across the gap between two humans  is analogous to signalling across the gap between two entire species.  A human being, in terms of knowledge content and creative individu- ality, is like a species. All the individuals of any other species have  virtually the same programming in their genes and use virtually the  same criteria for acting and being attracted. Humans are quite unlike  that: the amount of information in a human mind is more than that  in the genome of any species, and overwhelmingly more than the genetic  information unique to one person. So human artists are trying to signal  across the same scale of gap between humans as the flowers and insects  are between species. They can use some species-specific criteria; but  they can also reach towards objective beauty. Exactly the same is true  of all our other knowledge: we can communicate with other people by  sending predetermined messages determined by our genes or culture,  or we can invent something new. But in the latter case, to have any  chance of communicating, we had better strive to rise above parochial- ism and seek universal truths. This may be
========================================================
 preferences? One reason  is that, when females mated with prominent-tailed males, their male  offspring, having more prominent tails, found more mates. Another  may be that an individual able to grow a large, colourful tail is more  92 the beginning of infinity likely to be healthy. In any case, the net effect of all the selection  pressures was to spread genes for large, colourful tails, and genes for  preferring such tails, through the population. The species and the  individuals just had to suffer the consequences. If the best-spreading genes impose sufficiently large disadvantages  on the species, the species becomes extinct. Nothing in biological  evolution prevents that. It has presumably happened many times in  the history of life on Earth, to species less lucky than the peacock.  Dawkins named his tour-de-force account of neo-Darwinism  The  Selfish Gene  because he wanted to stress that evolution does not  especially promote the ‘welfare’ of species or individual organisms.  But, as he also explained, it does not promote the ‘welfare’ of genes  either: it adapts them not for survival in larger numbers, nor indeed  for survival at all, but only for spreading through the population at  the expense of rival genes, particularly slight variants of themselves. Is it sheer luck, then, that most genes do usually confer some, albeit  less than optimal, functional benefits on their species, and on their  individual holders? No. Organisms are the slaves, or tools, that genes  use to achieve their ‘purpose’ of spreading themselves through the  population. (That is the ‘purpose’ that Paley and even Darwin never  guessed.) Genes gain advantages over each other in part by keeping  their slaves alive and healthy, just as human slave owners did. Slave  owners were not working for the benefit of their workforces, nor for  the benefit of individual slaves: it was solely to achieve their own  objectives that they fed and housed their slaves, and indeed forced  them to reproduce. Genes do much the same thing.  In addition, there is the phenomenon of reach: when the knowledge  in a gene happens to have reach, it will help the individual to help itself  in a wider range of circumstances, and by more, than the spreading of  the gene strictly requires. That is why mules stay alive even though they  are sterile. So it is not surprising that genes usually confer  some  benefits  on their species and its members, and do often succeed in increasing  their own absolute numbers. Nor should it be surprising that they  sometimes do the opposite. But what genes are adapted to – what they  do better than almost any variant of themselves – has nothing to do  with the species or the individuals or even their own survival in the  long run. It is getting themselves replicated more than rival genes. 93 Creation Neo-Darwinism and knowledge Neo-Darwinism does not refer, at its fundamental level, to anything  biological. It is based on the idea of a  replicator  (anything that contributes  causally to its own copying).* For instance, a gene conferring the ability  to digest a certain type of food  causes  the organism to remain healthy  in some situations where it would otherwise weaken or die. Hence it  increases the organism’s chances of having offspring in the future, and  those offspring would inherit, and spread,  copies  of the gene. Ideas can be replicators too. For example, a good joke is a replicator:  when lodged in a person’s mind, it has a tendency to cause that person  to tell it to other people, thus copying it into  their  minds. Dawkins  coined the term  memes  (rhymes with ‘dreams’) for ideas that are  replicators. Most ideas are not replicators: they do not cause us to  convey them to other people. Nearly all long-lasting ideas, however,  such as languages, scientific theories and religious beliefs, and the  ineffable states of mind that constitute cultures such as being British,  or the skill of performing classical music, are memes (or ‘memeplexes’  – collections of interacting memes). I shall say more about memes in  Chapter 15. The most general way of stating the central assertion of the neo-  Darwinian theory of evolution is that a population of replicators subject  to variation (for instance by imperfect copying) will be taken over by  those variants that are better than their rivals at causing themselves to  be replicated. This is a surprisingly deep truth which is commonly  criticized either for being too obvious to be worth stating or for being  false. The reason, I think, is that, although it is self-evidently true, it is  not self-evidently the explanation of specific adaptations. Our intuition  prefers explanations in terms of function or purpose: what does a gene  do for its holder, or for its species? But we have just seen that the genes  generally do not optimize such functionality.  So the knowledge embodied in genes is knowledge of how to get  themselves replicated at the expense of their rivals. Genes
========================================================
 why that must be so, starting with the fallibility  of the human mind and the unreliability of sensory experience.  plato : [ Scribbles,   ‘It’s only knowledge  of the material world  that’s  impossible, useless and undesirable.’ ] socrates: He  gave me a marvellous perspective on how we perceive  the world. Each of your eyes is like a dark little cave, one on whose  rear wall some stray shadows fall from outside. You spend your  whole life at the back of that cave, able to see nothing but that rear  wall, so you cannot see reality directly at all.  plato : [ Scribbles,   ‘It is as if we were prisoners, chained inside a cave  and permitted to look only at the rear wall. We can never know the  253 A Dream of Socrates reality outside because we see only fleeting, distorted shadows of it.’ ] [Note: Socrates is slightly improving on Hermes, and Plato has  been increasingly misinterpreting Socrates.] socrates: He  then went on to explain to me that objective knowledge  is indeed possible: it comes from within! It begins as conjecture, and  is then  corrected  by repeated cycles of criticism, including comparison  with the evidence on our ‘wall’.  plato : [ Scribbles,   ‘The only true knowledge is that which comes from  within. (How? Remembered from a previous life?)’ ] socrates: In  this way, we frail and fallible humans can come to know  objective reality – provided we use philosophically sound methods  as I have described (which most people do not). plato : [ Scribbles,   ‘We can come to know the true world beyond the  illusory world of experience. But only by pursuing the kingly art of  philosophy.’ ] chaerephon:  Socrates, I think it  was  the god speaking to you, for I  strongly feel that I have glimpsed a divine truth through you today.  It will take me a long time to reorganize my ideas to take account  of this new epistemology that he revealed to you. It seems a  tremendously far-reaching, and important, subject. socrates:  Indeed. I have some reorganizing to do myself. plato : Socrates, you really ought to write all this down – together  with all your other wisdom – for the benefit of the whole world, and  posterity. socrates: No  need, Aristocles. Posterity is right here, listening.  Posterity is all of  you , my friends. What is the point of writing down  things that are going to be endlessly tinkered with and improved?  Rather than make a permanent record of all my misconceptions as  they are at a particular instant, I would rather offer them to others  in two-way debate. That way I benefit from criticism and may even  make improvements myself. Whatever is valuable will survive such  debates and be passed on without any effort from me. Whatever is  not valuable would only make me look a fool to future generations. plato : If you say so, Master. Since Socrates left us no writings, historians of ideas can only guess at  what he really thought and taught, using the indirect evidence of his  254 the beginning of infinity portrayal by Plato and a few others who were there at the time and  whose accounts have survived. This is known as the ‘Socratic problem’,  and is the source of much controversy. One common view is that the  young Plato conveyed Socrates’ philosophy fairly faithfully, but that  later he used the character of Socrates more as a vehicle for conveying  his own views; that he did not even intend his dialogues to represent  the real Socrates, but used them only as convenient ways of expressing  arguments that have a to-and-fro form.  Perhaps I had better stress – in case it is not already obvious – that  I am doing the same. I do not intend the above dialogue accurately to  represent the philosophical opinions of the historical Socrates and  Plato. I have set it at that moment in history, with those participants,  because Socrates and his circle were among the foremost contributors  to the ‘Golden Age of Athens’, which should have become a beginning  of infinity but did not. And also because one thing that we do know  about the ancient Greeks is that the philosophical  problems  they  considered important have dominated Western philosophy ever since:  How is knowledge obtained? How can we distinguish between true  and false, right and wrong, reason and unreason? Which sorts of  knowledge (moral, empirical, theological, mathematical, justified . . . )  are possible, and which are mere chimeras? And so on. And therefore,  although the theory of knowledge presented in the dialogue is largely  that of the twentieth-century philosopher Karl Popper, together with  some addenda of my own, I guess that Socrates would have understood  and liked it. In some universes that were very like ours at the time, he  thought of it himself. I do want to make one indirect comment on the Socratic problem,  though: we habitually underestimate the difficulty of communication  – just as Socrates does at the end of the dialogue, when he assumes  that each party to a debate necessarily knows what the other is saying,
========================================================
 Enemies  (Routledge, 1945) Further reading John Barrow and Frank Tipler,  The Anthropic Cosmological Principle   (Clarendon Press, 1986) Susan Blackmore,  The Meme Machine  (Oxford University Press, 1999) Nick Bostrom, ‘Are You Living in a Computer Simulation?’,  Philo- sophical Quarterly  53 (2003) David Deutsch, ‘Apart from Universes’, in S. Saunders, J. Barrett, A.  Kent and D. Wallace, eds.,  Many Worlds?: Everett, Quantum Theory,  and Reality  (Oxford University Press, 2010) David Deutsch, ‘It from Qubit’, in John Barrow, Paul Davies and  Charles Harper, eds.,  Science and Ultimate Reality  (Cambridge  University Press, 2003) 461 bibliography David Deutsch, ‘Quantum Theory of Probability and Decisions’,  Pro  - ceedings of the Royal Society  A455 (1999) David Deutsch, ‘The Structure of the Multiverse’,  Proceedings of the  Royal Society  A458 (2002) Richard Feynman,  The Character of Physical Law  (BBC Publications,  1965) Richard Feynman,  The Meaning of It All  (Allen Lane, 1998) Ernest Gellner,  Words and Things  (Routledge & Kegan Paul, 1979) William Godwin,  Enquiry Concerning Political Justice  (1793) Douglas Hofstadter,  Gödel, Escher, Bach: An Eternal Golden Braid   (Basic Books, 1979) Douglas Hofstadter,  I am a Strange Loop  (Basic Books, 2007) Bryan Magee,  Popper  (Fontana, 1973) Pericles, ‘Funeral Oration’ Plato,  Euthyphro Karl Popper,  In Search of a Better World  (Routledge, 1995) Karl Popper,  The World of Parmenides  (Routledge, 1998) Roy Porter,  Enlightenment: Britain and the Creation of the Modern  World  (Allen Lane, 2000) Martin Rees,  Just Six Numbers  (Basic Books, 2001) Alan Turing, ‘Computing Machinery and Intelligence’,  Mind , 59, 236  (October 1950) Jenny Uglow,  The Lunar Men  (Faber, 2002) Vernor Vinge, ‘The Coming Technological Singularity’,  Whole Earth  Review , winter 1993 463 Entries in  bold  refer to defining or  principal occurrences. 641 argument (Hofstadter)   115 –18,  185 see also  domino computer absolute zero  46, 47, 71, 295 abstractions  114–24, 166, 185, 266–7,  447 abstract replicators  95, 114, 266–7 abstraction from experience  16, 128,  129 confusions of abstract attributes with  physical ones of the same name   182–8, 343 finitism and  165–6 money as an abstraction  266–7 people as abstract information  59,  130 Achilles and the tortoise  182–3 adaptation biological  52, 54–5, 56 creationism and the designers of   79–81 of creativity  see  creativity by humans as universal constructors   58–60 and knowledge  55, 56–65, 78–81, 88;  see also  creation of knowledge the reach of human adaptations   56–65 through technology  57–60, 61, 436;  see also  automation Adleman, Leonard  145 Aeschylus  216 aesthetics   367 artistic values  366, 388 and attraction  357–9, 360–65 human appreciation of beauty  353–4,  356–8, 359, 362–7 the objectivity of beauty  122, 353–68 pure and applied art  365–6 see also  art ageing, problem of  213–14 see also  old age agriculture  48, 50, 57, 207, 234, 320,  422, 431, 437, 438, 440 AI  see  artificial intelligence Alabama paradox  330–31, 333 alchemy  1, 425 algebra  36, 136, 377–8 algorithms  35, 36, 117, 295, 362 evolutionary  160 see also  computer programs Alhazen  220 alphabets  126–7, 144 Amadeus  (Shaffer)  353 ambiguity  308, 448 infinite  405, 406, 409 see also  equivocation analogue computers  140 analytic functions  135–6, 452 Analytical Engine  136–8, 139, 140 Andes  426–7 animal minds  154, 268, 320–21, 358–9,  407, 410 anthropic reasoning  98–104,  105 ,  177–80, 452–6 anthropic principle  98 as an explanation of fine-tuning  98– 103, 177–80, 452, 453 and infinite sets  177–80 anthropocentrism  42–4, 51, 53–4,  75 ,  111, 155, 446 anti-anthropocentrism  43–4, 51;  see  also  Mediocrity, Principle of in conceptions of infinity  165–6, 181 Index 464 index anthropocentrism ( cont .) in the interpretation of quantum  theory  308–9, 319 in science fiction  262 see also  parochialism anthropomorphism  59, 148;  see also  animal minds anti-rational memes  see  memes, anti- rational anti-realism  15, 313, 314 antibiotics  436 apes/aping  60, 405, 407–9, 410 see also  imitation Appollonius  132–3, 166 apportionment paradoxes  326–33;  see  also  no-go theorems Archimedes  132, 133, 166 Arecibo observatory  72 Ares  246, 248 argument from design  see under  design Aristarchus of Samos  27 Aristocles  see  Plato Aristodemus  83 Aristophanes  216 Aristotle  216 arithmetic  128–32, 135, 136, 141, 233,  240, 252, 332, 374 arrogance  45, 51–2, 314 Arrow, Kenneth  336–7 Arrow’s theorem  336–8, 340–41, 343,  345 art artistic problems  355–6 artistic values  366, 388 attraction of  357–8 painting  219, 356, 357 pure and applied  365–6 as self-expression  366–7 utilitarian theories of  366 see also  aesthetics; music artificial evolution  158–63 artificial intelligence (AI)  137–8, 148–63 chatbots and  150, 152, 158, 160 and creativity  148–63 Elbot  program  151–2, 156 Eliza  program  148–9, 161 and humour  157 and the
========================================================
 long as it does not lead you to conclude that there is  something worthwhile about the Persephone myth, or the prophet’s  apocalyptic theory or the gambler’s delusion, just because is it testable.  Nor is a person capable of making progress merely by virtue of being  willing to drop a theory when it is refuted: one must also be seeking  a better explanation of the relevant phenomena. That is the scientific  frame of mind. As the physicist Richard Feynman said, ‘Science is what we have  learned about how to keep from fooling ourselves.’ By adopting easily  variable explanations, the gambler and prophet are ensuring that they  will be able to continue fooling themselves no matter what happens.  Just as thoroughly as if they had adopted untestable theories, they are  insulating themselves from facing evidence that they are mistaken  about what is really there in the physical world. The quest for good explanations is, I believe, the basic regulating  principle not only of science, but of the Enlightenment generally. It is  the feature that distinguishes those approaches to knowledge from all  others, and it implies all those other conditions for scientific progress  I have discussed: It trivially implies that prediction alone is insufficient.  23 The Reach of Explanations Somewhat less trivially, it leads to the rejection of authority, because  if we adopt a theory on authority, that means that we would also have  accepted a range of different theories on authority. And hence it also  implies the need for a tradition of criticism. It also implies a meth- odological rule – a  criterion for reality  – namely that we should conclude  that a particular thing is real if and only if it figures in our best  explanation of something. Although the pioneers of the Enlightenment and of the scientific  revolution did not put it this way, seeking good explanations was (and  remains) the spirit of the age. This is how they began to think. It is  what they began to do, systematically for the first time. It is what made  that momentous difference to the rate of progress of all kinds. Long before the Enlightenment, there were individuals who sought  good explanations. Indeed, my discussion here suggests that all progress  then, as now, was due to such people. But in most ages they lacked  contact with a tradition of criticism in which others could carry on their  ideas, and so created little that left any trace for us to detect. We do  know of sporadic traditions of good-explanation-seeking in narrowly  defined fields, such as geometry, and even short-lived traditions of  criticism – mini-enlightenments – which were tragically snuffed out, as  I shall describe in Chapter 9. But the sea change in the values and  patterns of thinking of a whole community of thinkers, which brought  about a sustained and accelerating creation of knowledge, happened  only once in history, with  the  Enlightenment and its scientific revolution.  An entire political, moral, economic and intellectual culture – roughly  what is now called ‘the West’ – grew around the values entailed by the  quest for good explanations, such as tolerance of dissent, openness to  change, distrust of dogmatism and authority, and the aspiration to  progress both by individuals and for the culture as a whole. And the  progress made by that multifaceted culture, in turn, promoted those  values – though, as I shall explain in Chapter 15, they are nowhere close  to being fully implemented. Now consider the true explanation of seasons. It is that the Earth’s  axis of rotation is tilted relative to the plane of its orbit around the  sun. Hence for half of each year the northern hemisphere is tilted  towards the sun while the southern hemisphere is tilted away, and for  the other half it is the other way around. Whenever the sun’s rays are  24 the beginning of infinity falling vertically in one hemisphere (thus providing more heat per    unit area of the surface) they are falling obliquely in the other (thus  providing less).  The true explanation of seasons (not to scale!) That is a good explanation – hard to vary, because all its details play  a functional role. For instance, we know – and can test independently  of our experience of seasons – that surfaces tilted away from radiant  heat are heated less than when they are facing it, and that a spinning  sphere in space points in a constant direction. And we can explain why,  in terms of theories of geometry, heat and mechanics. Also, the same  tilt appears in our explanation of where the sun appears relative to the  horizon at different times of year. In the Persephone myth, in contrast,  the coldness of the world is caused by Demeter’s sadness – but people  do not generally cool their surroundings when they are sad, and we  have no way of knowing that Demeter  is  sad, or that she ever cools  the world, other than the onset of winter itself. One could not substitute  the moon for the sun in the axis-tilt story, because the
========================================================
 things; and The Earth’s biosphere is  incapable  of supporting human life. Consider Hawking’s remark again. It is true that we are on a  (somewhat)  typical planet of a typical star in a typical galaxy. But we  are far from typical of the matter in the universe. For one thing, about  46 the beginning of infinity 80 per cent of that matter is thought to be invisible ‘dark matter’, which  can neither emit nor absorb light. We currently detect it only through  its indirect gravitational effects on galaxies. Only the remaining 20 per  cent is matter of the type that we parochially call ‘ordinary matter’. It  is characterized by glowing continuously. We do not usually think of  ourselves as glowing, but that is another parochial misconception, due  to the limitations of our senses: we emit radiant heat, which is infra- red light, and also light in the visible range, too faint for our eyes    to detect. Concentrations of matter as dense as ourselves and our planet and  star, though numerous, are not exactly typical either. They are isolated,  uncommon phenomena. The universe is mostly vacuum (plus radiation  and dark matter). Ordinary matter is familiar to us only because we  are made of it, and because of our untypical location near large  concentrations of it.  Moreover, we are an uncommon form of ordinary matter. The  commonest form is plasma (atoms dissociated into their electrically  charged components), which typically emits bright, visible light because  it is in stars, which are rather hot. We scums are mainly infra-red  emitters because we contain liquids and complex chemicals which can  exist only at a much lower range of temperatures.  The universe is pervaded with microwave radiation – the afterglow  of the Big Bang. Its temperature is about 2.7 kelvin, which means 2.7  degrees above the coldest possible temperature, absolute zero, or about  270 degrees Celsius colder than the freezing point of water. Only very  unusual circumstances can make anything colder than those micro- waves. Nothing in the universe is known to be cooler than about  one  kelvin – except in certain physics laboratories on Earth. There, the  record low temperature achieved is below one  billionth  of a kelvin. At  those extraordinary temperatures, the glow of ordinary matter is  effectively extinguished. The resulting ‘non-glowing ordinary matter’  on our planet is an exceedingly exotic substance in the universe at  large. It may well be that the interiors of refrigerators constructed by  physicists are by far the coldest and darkest places in the universe. Far  from typical. What is a typical place in the universe like? Let me assume that you  are reading this on Earth. In your mind’s eye, travel straight upwards  47 The Spark a few hundred kilometres. Now you are in the slightly more typical  environment of space. But you are still being heated and illuminated  by the sun, and half your field of view is still taken up by the solids,  liquids and scums of the Earth. A typical location has none of those  features. So, travel a few trillion kilometres further in the same direction.  You are now so far away that the sun looks like other stars. You are  at a much colder, darker and emptier place, with no scum in sight. But  it is not yet typical: you are still inside the Milky Way galaxy, and most  places in the universe are not in any galaxy. Continue until you are  clear outside the galaxy – say, a hundred thousand light years from  Earth. At this distance you could not glimpse the Earth even if you  used the most powerful telescope that humans have yet built. But the  Milky Way still fills much of your sky. To get to a typical place in the  universe, you have to imagine yourself at least a thousand times as far  out as that, deep in intergalactic space. What is it like there? Imagine the whole of space notionally divided  into cubes the size of our solar system. If you were observing from a  typical one of them, the sky would be pitch black. The nearest star  would be so far away that if it were to explode as a supernova, and  you were staring directly at it when its light reached you, you would  not see even a glimmer. That is how big and dark the universe is.    And it is cold: it is at that background temperature of 2.7 kelvin,    which is cold enough to freeze every known substance except helium.  (Helium is believed to remain liquid right down to absolute zero, unless    highly pressurized.) And it is empty: the density of atoms out there is below one per cubic  metre. That is a million times sparser than atoms in the space between  the stars, and those atoms are themselves sparser than in the best  vacuum that human technology has yet achieved. Almost all the atoms  in intergalactic space are hydrogen or helium, so there is no chemistry.  No life could have evolved there, nor any intelligence. Nothing changes  there. Nothing happens. The same is true of the next cube and the next,  and if you were to examine a million
========================================================
 instance, conjurers, politicians and examination candidates are some- times suspected of receiving information through concealed earpieces  and then repeating it mechanically while pretending that it originated  in their brains. Also, when someone is consenting to a medical pro  - cedure, the physician has to make sure that they are not merely uttering  words without knowing what they mean. To test that, one can repeat  a question in a different way, or ask a different question involving  similar words. Then one can check whether the replies change ac    - cordingly. That sort of thing happens naturally in any free-ranging  conversation. A Turing test is similar, but with a different emphasis. When testing  a human, we want to know whether it  is  an unimpaired human (and  not a front for any other human). When testing an AI, we are hoping  156 the beginning of infinity to find a hard-to-vary explanation to the effect that its utterances  cannot  come from any human but only from the AI. In both cases,  interrogating a human as a control for the experiment is pointless. Without a good explanation of how an entity’s utterances were  created, observing them tells us nothing about that. In the Turing test,  at the simplest level, we need to be convinced that the utterances are  not being directly composed by a human masquerading as the AI, as in  the Hofstadter hoax. But the possibility of a hoax is the least of it. For  instance, I guessed above that  Elbot  had recited a stock joke in response  to mistakenly recognizing the keyword ‘spouse’. But the joke would  have quite a different significance if we knew that it was  not  a stock  joke – because no such joke had ever been encoded into the program.  How could we know that? Only from a good explanation. For  instance, we might know it because we ourselves wrote the program.  Another way would be for the author of the program to explain to us  how it works – how it creates knowledge, including jokes. If the  explanation was good, we should know that the program was an AI.  In fact, if we had  only  such an explanation but had not yet seen any  output from the program – and even if it had not been written yet – we  should still conclude that it was a genuine AI program. So there would  be no need for a Turing test. That is why I said that if lack of computer  power were the only thing preventing the achievement of AI, there  would be no need to wait.  Explaining how an AI program works in detail might well be in    - tractably complicated. In practice the author’s explanation would  always be at some emergent, abstract level. But that would not prevent  it from being a good explanation. It would not have to account for the  specific computational steps that composed a joke, just as the theory  of evolution does not have to account for why every specific mutation  succeeded or failed in the history of a given adaptation. It would just  explain how it  could  happen, and why we should expect it to happen,  given how the program works. If that were a good explanation, it would  convince us that the joke – the knowledge in the joke – originated in  the program and not in the programmer. Thus the very same utterance  by the program – the joke – can be either evidence that it is  not  think- ing or evidence that it  is  thinking depending on the best available  explanation of how the program works. 157 Artificial Creativity The nature of humour is not very well understood, so we do not  know whether general-purpose thinking is required to compose jokes.  So it is conceivable that, despite the wide range of subject matter about  which one can joke, there are hidden connections that reduce all joke  making to a single narrow function. In that case there could one day  be general-purpose joke-making programs that are not people, just as  today there are chess-playing programs that are not people. It sounds  implausible, but, since we have no good explanation ruling it out, we  could not rely on joke-making as our only way of judging an AI. What  we could do, though, is have a conversation ranging over a diverse  range of topics, and pay attention to whether the program’s utterances  were or were not adapted, in their meanings, to the various purposes  that came up. If the program really is thinking, then in the course    of such a conversation it will  explain itself  – in one of countless,  unpredictable ways – just as you or I would. There is a deeper issue too. AI abilities must have some sort of  universality: special-purpose thinking would not count as thinking in  the sense Turing intended. My guess is that every AI is a person: a  general-purpose explainer. It is conceivable that there are other levels  of universality between AI and ‘universal explainer/constructor’, and  perhaps separate levels for those associated attributes like conscious- ness. But those attributes all seem to have arrived in one jump to  universality in humans, and, although we have
========================================================
 consecutive cubes in any direction  the story would be the same.  Cold, dark and empty. That unimaginably desolate environment is  typical of the universe – and is another measure of how  un typical the  Earth and its chemical scum are, in a straightforward physical sense.  48 the beginning of infinity The issue of the cosmic significance of this type of scum will shortly  take us back out into intergalactic space. But let me first return to  Earth, and consider the Spaceship Earth metaphor, in  its  straightforward  physical version.  This much is true: if, tomorrow, physical conditions on the Earth’s  surface were to change even slightly by astrophysical standards, then  no humans could live here unprotected, just as they could not survive  on a spaceship whose life-support system had broken down. Yet I am  writing this in Oxford, England, where winter nights are likewise often  cold enough to kill any human unprotected by clothing and other  technology. So, while intergalactic space would kill me in a matter of  seconds, Oxfordshire in its primeval state might do it in a matter of  hours – which can be considered ‘life support’ only in the most contrived  sense. There  is  a life-support system in Oxfordshire today, but it was  not provided by the biosphere. It has been built by humans. It consists  of clothes, houses, farms, hospitals, an electrical grid, a sewage system  and so on. Nearly the whole of the Earth’s biosphere in its primeval  state was likewise incapable of keeping an unprotected human alive  for long. It would be much more accurate to call it a death trap for  humans rather than a life-support system. Even the Great Rift Valley  in eastern Africa, where our species evolved, was barely more hospitable  than primeval Oxfordshire. Unlike the life-support system in that  imagined spaceship, the Great Rift Valley lacked a safe water supply,  and medical equipment, and comfortable living quarters, and was  infested with predators, parasites and disease organisms. It frequently  injured, poisoned, drenched, starved and sickened its ‘passengers’, and  most of them died as a result.  It was similarly harsh to all the other organisms that lived there:    few individuals live comfortably or die of old age in the supposedly  beneficent biosphere. That is no accident: most populations, of most  species, are living close to the edge of disaster and death. It has to be  that way, because as soon as some small group, somewhere, begins to  have a slightly easier life than that, for any reason – for instance, an  increased food supply, or the extinction of a competitor or predator –  then its numbers increase. As a result, its other resources are depleted  by the increased usage; so an increasing proportion of the population  now has to colonize more marginal habitats and make do with inferior  49 The Spark resources, and so on. This process continues until the disadvantages  caused by the increased population have exactly balanced the advantage  conferred by the beneficial change. That is to say, the new birth rate is  again just barely keeping pace with the rampant disabling and killing  of individuals by starvation, exhaustion, predation, overcrowding and  all those other natural processes. That is the situation to which evolution adapts organisms. And that,  therefore, is the lifestyle in which the Earth’s biosphere ‘seems adapted’  to sustaining them. The biosphere only ever achieves stability – and  only temporarily at that – by continually neglecting, harming, disabling  and killing individuals. Hence the metaphor of a spaceship or a life- support system, is quite perverse: when humans design a life-support  system, they design it to provide the maximum possible comfort, safety  and longevity for its users within the available resources; the biosphere  has no such priorities.  Nor is the biosphere a great preserver of  species . In addition to being  notoriously cruel to individuals, evolution involves continual extinctions  of entire species. The average rate of extinction since the beginning of  life on Earth has been about ten species per year (the number is known  only very approximately), becoming much higher during the relatively  brief periods that palaeontologists call ‘mass extinction events’. The  rate at which species have come into existence has on balance only  slightly exceeded the extinction rate, and the net effect is that the  overwhelming majority of species that have ever existed on Earth  (perhaps 99.9 per cent of them) are now extinct. Genetic evidence  suggests that our own species narrowly escaped extinction on at least  one occasion. Several species closely related to ours did become extinct.  Significantly, the ‘life-support system’ itself wiped them out – by means  such as natural disasters, evolutionary changes in other species, and  climate change. Those cousins of ours had not invited extinction by  changing their lifestyles or overloading the biosphere: on the
========================================================
 past and would have in  the future.  But what use are explanations if they cannot make predictions and  so cannot be tested through experience, as they can be in science? This  is really the question: how is progress possible in philosophy? As I  discussed in Chapter 5, it is obtained by seeking good explanations.  The misconception that evidence can play no legitimate role in philo- sophy is a relic of empiricism. Objective progress is indeed possible in  politics just as it is in morality generally and in science. Political philosophy traditionally centred on a collection of issues  that Popper called the ‘who should rule?’ question. Who should wield  power? Should it be a monarch or aristocrats, or priests, or a dictator,  or a small group, or ‘the people’, or their delegates? And that leads to  210 the beginning of infinity derivative questions such as ‘How should a king be educated?’ ‘Who  should be enfranchised in a democracy?’ ‘How does one ensure an  informed and responsible electorate?’ Popper pointed out that this class of questions is rooted in the same  misconception as the question ‘How are scientific theories derived from  sensory data?’ which defines empiricism. It is seeking a system that  derives  or justifies the right choice of leader or government, from  existing data – such as inherited entitlements, the opinion of the  majority, the manner in which a person has been educated, and so on.  The same misconception also underlies blind optimism and pessimism:  they both expect progress to be made by applying a simple rule to  existing knowledge, to establish which future possibilities to ignore  and which to rely on. Induction, instrumentalism and even Lamarckism  all make the same mistake: they expect  explanationless progress.  They  expect knowledge to be created by fiat with few errors, and not by a  process of variation and selection that is making a continual stream  of errors and correcting them. The defenders of hereditary monarchy doubted that any method of  selection of a leader by means of rational thought and debate could  improve upon a fixed, mechanical criterion. That was the precautionary  principle in action, and it gave rise to the usual ironies. For instance,  whenever pretenders to a throne claimed to have a better hereditary  entitlement than the incumbent, they were in effect citing the precaution- ary principle as a justification for sudden, violent, unpredictable change  – in other words, for blind optimism. The same was true whenever  monarchs happened to favour radical change themselves. Consider also  the revolutionary utopians, who typically achieve only destruction and  stagnation. Though they are blind optimists, what defines them as  utopians is their pessimism that their supposed utopia, or their violent  proposals for achieving and entrenching it, could ever be improved upon.  Additionally, they are revolutionaries in the first place because they are  pessimistic that many other people can be persuaded of the final truth  that they think they know. Ideas have consequences, and the ‘who should rule?’ approach to  political philosophy is not just a mistake of academic analysis: it has  been part of practically every bad political doctrine in history. If the  political process is seen as an engine for putting the right rulers in  211 Optimism power, then it justifies violence, for until that right system is in place,  no ruler is legitimate; and once it is in place, and its designated rulers  are ruling, opposition to them is opposition to rightness. The problem  then becomes how to thwart anyone who is working against the rulers  or their policies. By the same logic, everyone who thinks that existing  rulers or policies are bad must infer that the ‘who should rule?’ question  has been answered wrongly, and therefore that the power of the rulers  is not legitimate, and that opposing it is legitimate, by force if necessary.  Thus the very question ‘Who should rule?’ begs for violent, authoritarian  answers, and has often received them. It leads those in power into  tyranny, and to the entrenchment of bad rulers and bad policies; it  leads their opponents to violent destructiveness and revolution. Advocates of violence usually have in mind that none of those  things need happen if only everyone agreed on who should rule. But  that means agreeing about what is right, and, given agreement on  that, rulers would then have nothing to do. And, in any case, such  agreement is neither possible nor desirable: people are different, and  have unique ideas; problems are inevitable, and progress consists of  solving them. Popper therefore applies his basic ‘how can we detect and eliminate  errors?’ to political philosophy in the form  how can we rid ourselves  of bad governments without violence?  Just as science seeks explanations  that are experimentally testable, so a rational political system makes  it as easy as possible to detect, and persuade others, that a leader or
========================================================
 eponymous part of the world. The  mathematician and philosopher Nicholas de Condorcet, for instance,  was French yet belonged more to what I am calling the ‘British’  Enlightenment, while Karl Popper, the twentieth century’s foremost  proponent of the British Enlightenment, was born in Austria.  The Continental Enlightenment was impatient for the perfected state  – which led to intellectual dogmatism, political violence and new forms  of tyranny. The French Revolution of 1789 and the Reign of Terror  that followed it are the archetypal examples. The British Enlightenment,  which was evolutionary and cognizant of human fallibility, was im    - patient for institutions that did not stifle gradual, continuing change.  It was also enthusiastic for small improvements, unbounded in the  future. (See, for instance, the historian Jenny Uglow’s book  Lunar  Men .) This is, I believe, the movement that was successful in its pursuit  of progress, so in this book when I refer to ‘the’ Enlightenment I mean  the ‘British’ one.  To investigate the ultimate reach of humans (or of people, or of  progress), we should not be considering places like the Earth and the  moon, which are unusually rich in resources. Let us go back to that  typical place. While the Earth is inundated with matter, energy and  evidence, out there in intergalactic space all three are at their lowest  possible supply. There is no rich supply of minerals, no vast nuclear  reactor overhead delivering free energy, no lights in the sky or diverse  local events to provide evidence of the laws of nature. It is empty, cold  and dark.  Or is it? Actually, that is yet another parochial misconception.  Intergalactic space is indeed very empty by human standards. But each  of those solar-system-sized cubes still contains over a billion tonnes of  matter – mostly in the form of ionized hydrogen. A billion tonnes is  more than enough mass to build, say, a space station and a colony of  67 The Spark scientists creating an open-ended stream of knowledge –  if  anyone were  present who knew how to do that.  No human today knows how. For instance, one would first have to  transmute some of the hydrogen into other elements. Collecting it from  such a diffuse source would be far beyond us at present. And, although  some types of transmutation are already routine in the nuclear industry,  we do not know how to transmute hydrogen into other elements on  an industrial scale. Even a simple nuclear-fusion reactor is currently  beyond our technology. But physicists are confident that it is not  forbidden by any laws of physics, in which case, as always, it can only  be a matter of knowing how. No doubt a billion-tonne space station is not large enough to thrive  in the very long run. The inhabitants will want to enlarge it. But that  presents no problem of principle. As soon as they started to trawl their  cube for hydrogen, more would drift in from the surrounding space,  supplying the cube with millions of tonnes of hydrogen per year. (There  is also believed to be an even greater mass of ‘dark matter’ in the cube,  but we do not know how to do anything useful with it, so let us ignore  it in this thought experiment.) As for the cold, and the lack of available energy – as I said, the  transmutation of hydrogen releases the energy of nuclear fusion. That  would be a sizeable power supply, orders of magnitude more than the  combined power consumption of everyone on Earth today. So the cube  is not as lacking in resources as a parochial first glance would suggest.  How would the space station get its vital supply of evidence? Using  the elements created by transmutation, one could construct scientific  laboratories, as in the projected moon base. On Earth, when chemistry  was in its infancy, making discoveries often depended on travelling all  over the planet to find materials to experiment on. But transmutation  makes that irrelevant; and chemical laboratories on the space station  would be able to synthesize arbitrary compounds of arbitrary elements.  The same is true of elementary particle physics: in that field, almost  anything will do as a source of evidence, because every atom is poten- tially a cornucopia of particles just waiting to display themselves if one  hits the atom hard enough (using a particle accelerator) and observes  with the right instruments. In biology, DNA and all other biochemical  molecules could be synthesized and experimented on. And, although  68 the beginning of infinity biology field trips would be difficult (because the closest natural eco  - system would be millions of light years away), arbitrary life forms  could be created and studied in artificial ecosystems, or in virtual-reality  simulations of them. As for astronomy – the sky there is pitch black  to the human eye, but to an observer with a telescope, even one of  present-day design, it would be packed with galaxies. A somewhat  bigger telescope could see stars in those galaxies in
========================================================
 such as morality – in which there were no such  222 the beginning of infinity thing as objective progress. But truth does exist in all those fields, and  progress towards it is made by seeking good explanations. Problems  are inevitable, because our knowledge will always be infinitely far from  complete. Some problems are hard, but it is a mistake to confuse hard  problems with problems unlikely to be solved. Problems are soluble,  and each particular evil is a problem that can be solved. An optimistic  civilization is open and not afraid to innovate, and is based on traditions  of criticism. Its institutions keep improving, and the most important  knowledge that they embody is knowledge of how to detect and  eliminate errors. There may have been many short-lived enlightenments  in history. Ours has been uniquely long-lived. 223 10 A Dream of Socrates socrates  is staying at an inn near the Temple of the Oracle at Delphi.  Together with his friend  chaerephon , he has today asked the Oracle  who the wisest man in the world is,* so that they might go and learn  from him. But, to their annoyance, the priestess (who provides the  Oracle’s voice on behalf of the god Apollo) merely announced, ‘No  one is wiser than Socrates.’ Sleeping now on an uncomfortable bed in  a tiny and exorbitantly expensive room,  socrates  hears a deep,  melodious voice intoning his name. hermes:  Greetings, Socrates. socrates: [ Draws the blanket over his head. ] Go away. I’ve already  made too many offerings today and you’re not going to wring any  more out of me. I am too ‘wise’ for that, hadn’t you heard? hermes: I  seek no offering. socrates:  Then what do you want? [ He turns and sees  hermes ,  who is naked. ] Well, I’m sure that some of my associates camped  outside will be glad to – hermes: It is  not them I seek, but you, O Socrates. socrates:  Then you shall be disappointed, stranger. Now kindly leave  me to my hard-earned rest. hermes: V ery well. [ He makes towards the door. ] socrates: W ait. hermes: [ Turns and raises a quizzical eyebrow. ] *In the story as told by Plato in his  Apology , Chaerophon asks the Oracle  whether   there is anyone wiser than Socrates, and is told no. But would he really have wasted  this expensive and solemn privilege on a question with only two possible answers, one  flattering, the other frustrating, and neither very interesting? 224 the beginning of infinity socrates: [ slowly and deliberately ] I am asleep. Dreaming. And you  are the god Apollo. hermes:  What makes you think so? socrates:  These precincts are sacred to you. It is night-time and there  is no lamp, yet I see you clearly. This is not possible in real life. So  you must be coming to me in a dream.  hermes: You  reason coolly. Are you not afraid? socrates:  Bah! I ask you in return: are you a benevolent or a  malevolent god? If benevolent, then what do I have to fear? If  malevolent, then I disdain to fear you. We Athenians are a proud  people – and protected by our goddess, as you surely know. Twice  we defeated the Persian Empire against overwhelming odds,* and  now we are defying Sparta. It is our custom to defy anyone who  seeks our submission.  hermes:  Even a god? socrates: A  benevolent god would not seek it. On the other hand,  it is also our custom to give a hearing to anyone who offers us honest  criticism, seeking to persuade us freely to change our minds. For we  want to do what is right.  hermes:  Those two customs are two sides of the same valuable coin,  Socrates. I give you Athenians great credit for honouring them. socrates: My  city is surely deserving of your favour. But why would  an immortal want to converse with such a confused and ignorant  person as me? I think I can guess your reason: you have repented of  your little joke via the Oracle, haven’t you? Indeed, it was rather  cruel of you to send us only a mocking answer, considering the  distance we have come and the offerings we have made. So please  tell me the truth this time, O fount of wisdom: who is really the  wisest man in the world? hermes: I  reveal no facts. socrates: [ Sighs. ] Then I beg you – I have always wanted to know  this: what is the nature of virtue? hermes: I  reveal no moral truths either. *In this dialogue, Socrates sometimes exaggerates the attributes and achievements    of his beloved home city-state, Athens. In this case he is ignoring the contributions of  other Greek city-states to the defeats of two invasion attempts by the Persian Empire,  both of them before he was born. 225 A Dream of Socrates socrates: Yet, as a  benevolent god, you must have come here to impart  some  sort of knowledge. What sort will you deign to grant me? hermes:  Knowledge about knowledge, Socrates.  Epistemology.  I have  already mentioned some. socrates: You  have? Oh – you said that you honour Athenians for  our openness to persuasion. And for our defiance of bullies. But it  is well known that those are virtues! Surely telling me what I already
========================================================
 particular copper atom at the tip of the nose of the statue  of Sir Winston Churchill that stands in Parliament Square in London.  Let me try to explain why that copper atom is there. It is because Churchill  served as prime minister in the House of Commons nearby; and because  his ideas and leadership contributed to the Allied victory in the Second  World War; and because it is customary to honour such people by putting  up statues of them; and because bronze, a traditional material for such  statues, contains copper, and so on. Thus we explain a low-level physical  observation – the presence of a copper atom at a particular location –  through extremely high-level theories about emergent phenomena such  as ideas, leadership, war and tradition.  There is no reason why there should exist, even in principle, any lower- level  explanation  of the presence of that copper atom than the one I have  just given. Presumably a reductive ‘theory of everything’ would in  principle make a low-level  prediction  of the probability that such a statue  will exist, given the condition of (say) the solar system at some earlier  date. It would also in principle describe how the statue probably got  there. But such descriptions and predictions (wildly infeasible, of course)  would explain nothing. They would merely describe the trajectory that  each copper atom followed from the copper mine, through the smelter  and the sculptor’s studio and so on . . . In fact such a prediction would  have to refer to atoms all over the planet, engaged in the complex motion  we call the Second World War, among other things. But even if you had  the superhuman capacity to follow such lengthy predictions of the copper  atom’s being there, you would still not be able to say ‘Ah yes, now I  understand  why  they are there’. [You] would have to inquire into  what  it was  about that configuration of atoms, and those trajectories, that gave  110 the beginning of infinity them the propensity to deposit a copper atom at this location. Pursuing  that inquiry would be a creative task, as discovering new explanations  always is. You would have to discover that certain atomic configurations  support emergent phenomena such as leadership and war, which are  related to one another by high-level explanatory theories. Only when  you knew those theories could you understand why that copper atom is  where it is. Even in physics, some of the most fundamental explanations, and  the predictions that they make, are not reductive. For instance, the  second law of thermodynamics says that high-level physical processes  tend towards ever greater disorder. A scrambled egg never becomes  un      scrambled by the whisk, and never extracts energy from the pan to  propel itself upwards into the shell, which never seamlessly reseals  itself. Yet, if you could somehow make a video of the scrambling  process with enough resolution to see the individual molecules, and  play it backwards, and examine any part of it at that scale, you would  see nothing but molecules moving and colliding in strict obedience to  the low-level laws of physics. It is not yet known how, or whether, the  second law of thermodynamics can be derived from a simple statement  about individual atoms. There is no reason why it should be. There is often a moral overtone  to reductionism (science  should be  essentially reductive). This is related  both to instrumentalism and to the Principle of Mediocrity, which I  criticized in Chapters 1 and 3. Instrumentalism is rather like reductionism  except that, instead of rejecting only high-level explanations, it tries to  reject all explanations. The Principle of Mediocrity is a milder form of  reductionism: it rejects only high-level explanations that involve people.  While I am on the subject of bad philosophical doctrines with moral  overtones, let me add  holism , a sort of mirror image of reductionism.  It is the idea that the only valid explanations (or at least the only  significant ones) are of parts in terms of wholes. Holists also often share  with reductionists the mistaken belief that science  can only  (or should  only) be reductive, and therefore they oppose much of science. All those  doctrines are irrational for the same reason: they advocate accepting  or rejecting theories on grounds other than whether they are good  explanations. Whenever a high-level explanation does follow logically from  111 The Reality of Abstractions low-level ones, that also means that the high-level one  implies something  about the low-level ones. Thus, additional high-level theories, provided  that they were all consistent, would place more and more constraints  on what the low-level theories could be. So it could be that all the  high-level explanations that exist, taken together,  imply  all the low-level  ones, as well as vice versa. Or it could be that some low-level, some  intermediate-level and some high-level explanations, taken together,  imply  all
========================================================
 understand why it is perhaps not quite  311 A Physicist’s History of Bad Philosophy as bizarre and isolated an event as it may appear, one has to consider  the broader context of bad philosophy. Error is the normal state of our knowledge, and is no disgrace. There  is nothing bad about  false  philosophy. Problems are inevitable, but  they can be solved by imaginative, critical thought that seeks good  explanations. That is good philosophy, and good science, both of which  have always existed in some measure. For instance, children have  always learned language by making, criticizing and testing conjectures  about the connection between words and reality. They could not  possibly learn it in any other way, as I shall explain in Chapter 16.  Bad philosophy has always existed too. For instance, children have  always been told, ‘Because I say so.’ Although that is not always  intended as a philosophical position, it is worth analysing it as one,  for in four simple words it contains remarkably many themes of false  and  bad philosophy. First, it is a perfect example of bad explanation:  it could be used to ‘explain’ anything .  Second, one way it achieves that  status is by addressing only the form of the question and not the  substance: it is about who said something, not what they said. That is  the opposite of truth-seeking. Third, it reinterprets a request for  true  explanation  (why should something-or-other be as it is?) as a request  for  justification  (what entitles you to assert that it is so?), which is  the justified-true-belief chimera. Fourth, it confuses the nonexistent  authority for ideas  with  human  authority (power) – a much-travelled  path in bad political philosophy. And, fifth, it claims by this means to  stand outside the jurisdiction of normal criticism.  Bad philosophy before the Enlightenment was typically of the  because-I-say-so variety. When the Enlightenment liberated philosophy  and science, they both began to make progress, and increasingly there  was good philosophy. But, paradoxically,  bad  philosophy became  worse . I have said that empiricism initially played a positive role in the  history of ideas by providing a defence against traditional authorities  and dogma, and by attributing a central role – albeit the wrong one  – to experiment in science. At first, the fact that empiricism is an  impossible account of how science works did almost no harm, because  no one took it literally. Whatever scientists may have  said  about  where their discoveries came from, they eagerly addressed interesting  312 the beginning of infinity problems, conjectured good explanations, tested them, and only lastly  claimed to have induced the explanations from experiment. The bottom  line was that they succeeded: they made progress. Nothing prevented  that harmless (self-)deception, and nothing was inferred from it. Gradually, though, empiricism did begin to be taken literally, and so  began to have increasingly harmful effects. For instance, the doctrine  of  positivism , developed during the nineteenth century, tried to eliminate  from scientific theories everything that had not been ‘derived from  observation’. Now, since nothing is ever derived from obser  vation, what  the positivists tried to eliminate depended entirely on their own whims  and intuitions. Occasionally these were even good. For instance, the  physicist Ernst Mach (father of Ludwig Mach of the Mach–Zehnder  interferometer), who was also a positivist philosopher, influenced  Einstein, spurring him to eliminate untested assumptions from physics  – including Newton’s assumption that time flows at the same rate for  all observers. That happened to be an excellent idea. But Mach’s  positivism also caused him to oppose the resulting theory of relativity,  essentially because it claimed that spacetime really exists even though  it cannot be ‘directly’ observed. Mach also resolutely denied the existence  of atoms, because they were too small to observe. We laugh at this  silliness now – when we have microscopes that can see atoms – but the  role of philosophy should have been to laugh at it  then . Instead, when the physicist Ludwig Boltzmann used atomic theory to  unify thermodynamics and mechanics, he was so vilified by Mach and  other positivists that he was driven to despair, which may have contributed  to his suicide just before the tide turned and most branches of physics  shook off Mach’s influence. From then on there was nothing to dis  - courage atomic physics from thriving. Fortunately also, Einstein soon  rejected positivism and became a forthright defender of realism. That  was why he never accepted the Copenhagen interpretation. I wonder:  if Einstein had continued to take positivism seriously, could he ever have  thought of the general theory of relativity, in which spacetime not only  exists but is a dynamic, unseen entity bucking and twisting under the  influence of massive objects? Or would spacetime theory have
========================================================
 for? If we did not know better, the natural answer would be that they  were using it as we do today, for innovation and for understanding    the world, in order to improve their lives. For instance, individuals  who could improve stone tools would have ended up with better tools,  400 the beginning of infinity and hence with better food and more surviving offspring. They would  also have been able to make better weapons, thus denying the holders  of rival genes access to food and mates – and so on. Yet if that had  happened, the palaeontological record would show those improvements  happening on a timescale of generations. But it does not. Moreover, during the period when creativity was evolving, the ability  to replicate memes was evolving too. It is believed that some members  of the species  Homo erectus  living 500,000 years ago knew how to  make camp fires. That knowledge was in their memes, not in their  genes. And, once creativity and meme transmission are both present,  they greatly enhance each other’s evolutionary value, for then anyone  who improves something also has the means to bequeath the innovation  to all future generations, thus multiplying the benefit to the relevant  genes. And memes can be improved much faster by creativity than by  random trial and error. Since there is no upper limit to the value of  ideas, the conditions would have been there for a runaway co-evolution  between the two adaptations: creativity and the ability to use memes. Yet, again, there is something wrong with that scenario. The two  adaptations presumably did co-evolve, but the driving force behind  that evolution cannot have been that people were improving on ideas  and passing the improvements on to their children, because, again, if  they had been, they would have been making cumulative improvements  on a timescale of generations. Before the beginning of agriculture, about  12,000 years ago, many thousands of years passed between noticeable  changes. It is as though each small genetic improvement in creativity  produced just one noticeable innovation and then nothing more – rather  like today’s experiments in ‘artificial evolution’. But how can that be?  Unlike present-day artificial-evolution and AI research, our ancestors  were evolving  real  creativity, which is the capacity to create an endless  stream of innovations.  Their ability to innovate was increasing rapidly, but they were barely  innovating. This is a puzzle not because it is odd behaviour, but because,  if innovation was that rare, how could there have been a differential  effect on the reproduction of individuals with more or less ability to  innovate? That there were thousands of years between noticeable  changes presumably means that in most generations even the most  creative individuals in the population would not have been making  401 The Evolution of Creativity any innovations. Hence their greater ability to innovate would have  caused no selection pressure in their favour. Why did tiny improvements  in that ability keep spreading rapidly through the population? Our  ancestors must have been using their creativity – and using it to its  limits, and frequently – for  something . But evidently not for innovation.  What else could it have been used for? One theory is that it did not evolve to provide any functional  advantage, but merely through sexual selection: people used it to create  displays to attract mates – colourful clothing, decorations, story-telling,  wit and the like. A preference to mate with the individuals with the  most creative displays co-evolved with the creativity to meet that  preference in an evolutionary spiral – so the theory goes – just like  peahens’ preferences and peacocks’ tails. But creativity is an unlikely target for sexual selection. It is a sophis- ticated adaptation which, to this day, we are unable to reproduce  artificially. So it is presumably much harder to evolve than attributes  like coloration or the size and shape of body parts – some of which,  it is thought, did indeed evolve by sexual selection in humans and many  other animals. Creativity, as far as we know, evolved only once. More- over, its most visible effects are cumulative: it would be hard to detect  small differences in the creativity of potential mates on any one oc    - casion, especially if that creativity was not being used for practical  purposes. (Consider how hard it would be, today, to detect tiny  genetic   differences in people’s artistic abilities by means of an art competition.  In practice, any such differences would be swamped by other factors.)  So why did we not evolve multi-coloured hair or fingernails instead of  the capacity to create new knowledge, or any one of countless other  attributes that would have been far easier to evolve, and far easier to  assess reliably?  A more plausible variant of the sexual-selection theory is that people  chose mates according to social status, rather than favouring creativity
========================================================
 different histories: in one, comprising  half  the original  set of universes, the couple in question are still single; in the second,  comprising a  quarter  of the original set, they are married; and in the  third, comprising the remaining quarter, they are divorced.  Thus the three histories do not occupy equal proportions of the  multiverse. There are twice as many universes in which the couple never  married as there are universes in which they divorced.  Now suppose that scientists on the starship know about the multi- verse and understand the physics of the transporter. (Though note that  we have not yet given them a way of discovering those things.) Then  they know that, when they run the transporter, an infinite number of  fungible instances of themselves, all sharing the same history, are doing  so at the same time. They know that a voltage surge will occur in half  the universes in that history, which means that it will split into two  histories of equal measure. Hence they know that, if they use a voltmeter  capable of detecting the surge, half of the instances of themselves are  going to find that it has recorded one, and the other half are not. But  they also know that it is meaningless to ask (not merely impossible to  know)  which  event they will experience. Consequently they can make  two closely related predictions. One is that, despite the perfect deter- minism of everything that is happening,  nothing  can reliably predict  for them whether the voltmeter will detect a surge.  The other prediction is simply that the voltmeter will record a surge  with probability one-half. Thus the outcomes of such experiments are  subjectively random  (from the perspective of any observer) even though  278 the beginning of infinity everything that is happening is completely determined objectively. This  is also the origin of quantum-mechanical randomness and probability  in real physics: it is due to the measure that the theory provides for  the multiverse, which is in turn due to what kinds of physical processes  the theory allows and forbids. Notice that when a random outcome (in this sense) is about to  happen, it is a situation of diversity within fungibility: the diversity is  in the variable ‘what outcome they are  going  to see’. The logic of the  situation is the same as in cases like that of the bank account I discussed  above, except that this time the fungible entities are people. They are  fungible, yet half of them are going to see the surge and the other half  not. In practice they could test this prediction by doing the experiment  many times. Every formula purporting to predict the sequence of  outcomes will eventually fail: that tests the unpredictability. And in  the overwhelming majority of universes (and histories) the surge will  happen approximately half the time: that tests the predicted value of  the probability. Only a tiny proportion of the instances of the observers  will see anything different.  Our story continues. In one of the histories, the newspapers on the  astronauts’ home planets report the engagement. They fill many  column-inches with reports about the accident that brought the  astronauts together and so on. In the other history, where there is no  astronaut-engagement news, one newspaper fills the same space on the  page with a short story. It happens to be about a romance on a starship.  Some of the sentences in that story are identical to sentences in the  news items in the other history. The same words, printed in the same  column in the same newspaper, are fungible between the two histories;  but they are fiction in one history and fact in the other. So here the  fact/fiction attribute has diversity within fungibility. The number of distinct histories will now increase rapidly. Whenever  the transporter is used, it takes only microseconds for the sphere of  differentiation to engulf the whole starship, so, if it is typically used  ten times per day, the number of distinct histories inside the whole  starship will double about ten times a day. Within a month there will  be more distinct histories than there are atoms in our visible universe.  Most of them will be extremely similar to many others, because in only  279 The Multiverse a small proportion will the precise timing and magnitude of the voltage  surge be just right to precipitate a noticeable,  Sliding Doors -type  change. Nevertheless, the number of histories continues to increase  exponentially, and soon there are so many variations on events that  several significant changes have been caused  somewhere  in the multi- versal diversity of the starship. So the total number of such histories  increases exponentially too, even though they continue to constitute  only a small proportion of all histories that are present. Soon after that, in an even smaller but still exponentially grow-  ing number of histories, uncanny chains of ‘accidents’ and ‘unlikely  coincidences’ will have come to dominate events. I
========================================================
 first is an attribute  of deep ideas, the second an attribute of deep silliness. By confusing  them, one ascribes to the best art and philosophy the qualities of the  worst. Since, in that view, readers, viewers and critics can attribute any  meaning they choose to the second kind of ambiguity, bad philosophy  declares the same to be true of all knowledge: all meanings are equal,  and none of them is objectively true. One then has a choice between  complete nihilism or regarding  all  ‘ambiguity’ as a good thing in those  fields. Horgan chooses the latter option: he classifies art and philosophy  as ‘ironic’ fields, irony being the presence of multiple conflicting mean- ings in a statement.  However, unlike the postmodernists, Horgan thinks that science and  mathematics are the shining exceptions to all that. They alone are  capable of non-ironic knowledge. But there is also, he concludes, such  a thing as  ironic science  – the kind of science that cannot ‘resolve  questions’ because, essentially, it is just philosophy or art. Ironic science  can  continue indefinitely, but that is precisely because it never resolves  anything; it never discovers objective truth. Its only value is in the    eye of the beholder. So the future, according to Horgan, belongs    to ironic knowledge. Objective knowledge has already reached its  ultimate bounds. 449 The Beginning Horgan surveys some of the open questions of fundamental science,  and judges them all either ‘ironic’ or non-fundamental, in support of  his thesis. But that conclusion was made inevitable by his premises  alone. For consider the prospect of  any  future discovery that would  constitute fundamental progress. We cannot know what it is, but bad  philosophy can already split it, on principle, into a new rule of thumb  and a new ‘interpretation’ (or explanation). The new rule of thumb  cannot possibly be fundamental: it will just be another equation.    Only a trained expert could tell the difference between it and the old  equation. The new ‘interpretation’ will by definition be pure philosophy,  and hence must be ‘ironic’. By this method, any potential progress can  be pre-emptively reinterpreted as non-progress.  Horgan rightly points out that his prophecy cannot be proved false  by placing it in the context of previous failed prophecies. The fact that  Michelson was wrong about the achievements of the nineteenth century,  and Lagrange about those of the seventeenth, does not imply that  Horgan was wrong about those of the twentieth. However, it so  happens that our current scientific  knowledge  includes a historically  unusual number of deep, fundamental problems. Never before in the  history of human thought has it been so obvious that our knowledge  is tiny and our ignorance vast. And so, unusually, Horgan’s pessimism  contradicts existing knowledge as well as being a prophetic fallacy.  For example, the problem-situation of fundamental physics today has  a radically different structure from that of 1894. Although physicists  then were aware of some phenomena and theoretical issues which we  now recognize as harbingers of the revolutionary explanations to come,  their importance was unclear at the time. It was hard to distinguish  those harbingers from anomalies that would eventually be cleared up  with existing explanations plus the tweaking of the ‘sixth place of  decimals’ or minor terms in a formula. But today there is no such  excuse for denying that some of our problems are fundamental. Our  best theories are telling us of profound mismatches between themselves  and the reality that they are supposed to explain. One of the most blatant examples of that is that physics currently  has  two  fundamental ‘systems of the world’ – quantum theory and the  general theory of relativity – and they are radically inconsistent. There  are many ways of characterizing this inconsistency – known as the  450 the beginning of infinity problem of quantum gravity – corresponding to the many proposals  for solving it that have been tried without success. One aspect is the  ancient tension between the discrete and the continuous. The resolution  that I described in Chapter 11, in terms of continuous clouds of fungible  instances of a particle with diverse discrete attributes, works only if  the spacetime in which this happens is itself continuous. But if spacetime  is affected by the gravitation of the cloud, then it would acquire discrete  attributes. In cosmology, there has been revolutionary progress even in the few  years since  The End of Science  was written – and also since I wrote  The Fabric of Reality  soon afterwards. At the time, all viable cosmo- logical theories had the expansion of the universe gradually slowing  down, due to gravity, ever since the initial explosion at the Big Bang  and for ever in the future. Cosmologists were trying to determine  whether, despite slowing down, its expansion rate was sufficient to  make the universe expand for ever
========================================================
 past and would have in  the future.  But what use are explanations if they cannot make predictions and  so cannot be tested through experience, as they can be in science? This  is really the question: how is progress possible in philosophy? As I  discussed in Chapter 5, it is obtained by seeking good explanations.  The misconception that evidence can play no legitimate role in philo- sophy is a relic of empiricism. Objective progress is indeed possible in  politics just as it is in morality generally and in science. Political philosophy traditionally centred on a collection of issues  that Popper called the ‘who should rule?’ question. Who should wield  power? Should it be a monarch or aristocrats, or priests, or a dictator,  or a small group, or ‘the people’, or their delegates? And that leads to  210 the beginning of infinity derivative questions such as ‘How should a king be educated?’ ‘Who  should be enfranchised in a democracy?’ ‘How does one ensure an  informed and responsible electorate?’ Popper pointed out that this class of questions is rooted in the same  misconception as the question ‘How are scientific theories derived from  sensory data?’ which defines empiricism. It is seeking a system that  derives  or justifies the right choice of leader or government, from  existing data – such as inherited entitlements, the opinion of the  majority, the manner in which a person has been educated, and so on.  The same misconception also underlies blind optimism and pessimism:  they both expect progress to be made by applying a simple rule to  existing knowledge, to establish which future possibilities to ignore  and which to rely on. Induction, instrumentalism and even Lamarckism  all make the same mistake: they expect  explanationless progress.  They  expect knowledge to be created by fiat with few errors, and not by a  process of variation and selection that is making a continual stream  of errors and correcting them. The defenders of hereditary monarchy doubted that any method of  selection of a leader by means of rational thought and debate could  improve upon a fixed, mechanical criterion. That was the precautionary  principle in action, and it gave rise to the usual ironies. For instance,  whenever pretenders to a throne claimed to have a better hereditary  entitlement than the incumbent, they were in effect citing the precaution- ary principle as a justification for sudden, violent, unpredictable change  – in other words, for blind optimism. The same was true whenever  monarchs happened to favour radical change themselves. Consider also  the revolutionary utopians, who typically achieve only destruction and  stagnation. Though they are blind optimists, what defines them as  utopians is their pessimism that their supposed utopia, or their violent  proposals for achieving and entrenching it, could ever be improved upon.  Additionally, they are revolutionaries in the first place because they are  pessimistic that many other people can be persuaded of the final truth  that they think they know. Ideas have consequences, and the ‘who should rule?’ approach to  political philosophy is not just a mistake of academic analysis: it has  been part of practically every bad political doctrine in history. If the  political process is seen as an engine for putting the right rulers in  211 Optimism power, then it justifies violence, for until that right system is in place,  no ruler is legitimate; and once it is in place, and its designated rulers  are ruling, opposition to them is opposition to rightness. The problem  then becomes how to thwart anyone who is working against the rulers  or their policies. By the same logic, everyone who thinks that existing  rulers or policies are bad must infer that the ‘who should rule?’ question  has been answered wrongly, and therefore that the power of the rulers  is not legitimate, and that opposing it is legitimate, by force if necessary.  Thus the very question ‘Who should rule?’ begs for violent, authoritarian  answers, and has often received them. It leads those in power into  tyranny, and to the entrenchment of bad rulers and bad policies; it  leads their opponents to violent destructiveness and revolution. Advocates of violence usually have in mind that none of those  things need happen if only everyone agreed on who should rule. But  that means agreeing about what is right, and, given agreement on  that, rulers would then have nothing to do. And, in any case, such  agreement is neither possible nor desirable: people are different, and  have unique ideas; problems are inevitable, and progress consists of  solving them. Popper therefore applies his basic ‘how can we detect and eliminate  errors?’ to political philosophy in the form  how can we rid ourselves  of bad governments without violence?  Just as science seeks explanations  that are experimentally testable, so a rational political system makes  it as easy as possible to detect, and persuade others, that a leader or
========================================================
 positivism  312, 313,  325 logical  313, 314,  325 postmodernism  314, 448 potential infinity  165 ‘potentialities’, quantum  309 precautionary principle  see under  pessimism predators  48, 52, 89, 91, 144, 203, 359,  360–61 predictions  5, 6, 7, 8, 11, 17, 20–22, 24,  44, 136, 153, 175, 181, 182, 189,  206, 256, 277–8, 281, 293, 300,  300, 322, 359, 415, 451 and anthropic reasoning  99–103,  178, 179–80 481 index of Darwinism  96, 371 and explanation  27–9, 70–73, 112– 13, 116, 117, 209, 324 high-level  107–9, 110 and instrumentalism  15–16, 112 fundamental limitations on  see  unpredictability low-level  107–9 and multiple universes  99–103,  177–80 and prophecy  198, 432, 439;  Malthusian prophetic fallacy  206,  214, 432;  see also  prophecy pure  see  rules of thumb not the purpose of science  14–15 quantum theory and  307, 315 useless when separated from  explanation/interpretation  22,  315–16, 325 by supercomputer simulation  107,  437–9, 441 testable  13, 14–15, 27–8;  see also   experimental testing preferences  21, 122, 335–46, 350, 353,  359, 379, 382, 386, 388, 391, 453 aesthetics and  356, 363, 366 of a group  336–7, 345–6;  see also  will  of the people in mating  see  sexual selection prehistory  12, 62, 95–6, 128, 206, 220,  399–400, 416, 426, 428 prey  55, 57, 144, 203, 360–61, 411 priests  4, 209, 223, 226, 345, 413 primates  57, 80 see also  apes/aping prime numbers  115–19, 185–6 prime pairs conjecture  185–6 Principle of Mediocrity  43–4, 45, 51–4,  64,  76 , 101, 110, 166, 434 printing  134, 136, 137, 143 movable-type  134 privilege among abstractions  165, 175, 186,  190–91, 378 among people 175, 223n, 328–39, 346 among animals  35, 408 see also  authority probability  5–6, 9, 31, 99–100, 150,  277–8, 452–3 and infinity  see under  infinity and multiple universes  see under  multiple universes problems   31 connectedness of different kinds  63 and decision-making  341–2 are inevitable  61,  64 , 66, 97, 192,  206, 208, 211, 222, 311, 423,  435–6, 437 insoluble problems  53, 193, 213 and optimism  197–222, 435 problem solving  17–18, 62, 64–5,  432; and the plurality voting  system  350; positive conception of   17–18, 446–7 are soluble   65 , 66, 76, 141, 154,  191–3, 208, 211, 212, 222, 311,  345, 423, 435; in mathematics   185, 191–3 programs  see  computer programs progress artistic  355–6, 367 to better problems  447 celebration of  419 conditions for vii–viii, 12–13, 14, 19,  22, 32, 122–3, 203, 211, 217, 312,  320, 321, 344, 346, 385, 429, 456,  459 in cosmology, recent  450 counteracts bad philosophy  324 critics of ‘so-called progress’  vii, 390,  394, 434, 436, 449 and explanation  4–33, 122–3 dependent on existing knowledge  39,  60, 113 desire for  11, 19, 63, 133 Enlightenment, quintessential idea  of  65 evolution does not necessarily  constitute  91–2, 378–9, 382–3 explanationless  210 the jump to universality  125–47, 414 to less mistaken misconceptions  351,  446 limit to, supposed  53, 165–6, 205–6,  213, 445, 446 mathematical  189, 447  moral  23, 63, 121, 123 moving closer to reality through  scientific instruments  34–41 optimism and  196–222 in philosophy  119, 209 rates of  55, 58 risks of  197, 201, 204, 311 and stasis  vii, 19, 247–52 482 index progress ( cont .) sustainability of  423–41 technological, dependent on  explanatory  55;  see also   technology unbounded/infinite  vii–viii, 60–67,  69, 76, 81, 97, 165, 175, 195, 366,  423, 450, 457–9, 165 unpredictable  371 see also  Enlightenment; innovation;  inspiration/perspiration;  perfectibility; problem-solving;  tradition of criticism proof   194 and intuition  189 and understanding  191–3 see also under  physics; computation prophecy  14, 21, 22, 198, 206, 432, 444 disaster prophecies  205–6, 219, 432,  435; and the postponement of  disaster  437 distinguished from prediction  198 distinguished from speculation  458 in economic forecasts  439 gambler’s  14, 21, 22, 102 Horgan’s prophetic fallacy  448–9 Malthusian prophetic fallacy  206,  214, 432 pessimistic bias of  198, 206, 320, 444 prophetic optimism  204 proportional allocation/representation   326–33, 339, 346, 347–8 protons  290;  see also  hydrogen Providence  52, 120, 189 proxies  72, 74, 317–18, 320–21 psychology  113, 157–8;  see also  behaviourism pulsars  97, 38, 113, 290 puppy (and properties of a singularity)  174–6, 179 Pythagoras 119, 230, 233 qualia  153–4, 162– 163 , 319, 320–21,  354, 367, 458, 459 unpredictability of qualia  153–4, 268,  367 quantum computation  136, 187, 189,  295–6,  304 quantum parallelism  295–6 quantum suicide argument  453 quantum theory  60, 107, 181, 198,  258–304 and bad philosophy  305–6, 307–11 birth of  306–7 and the discrete  274, 294, 295, 298– 9,  303 , 304, 450 Everett interpretation  310 and the general theory or relativity   449–50 Heisenberg uncertainty principle  289,  291,  303–4 and motion  266, 274–5, 282, 289–90,  306  potentialities  309 quantum gravity  178, 449–50 quantum-mechanical motion
========================================================
 be, the purpose of science. Consider an audience watching  a conjuring trick. The problem facing them has much the same logic  as a scientific problem. Although in nature there is no conjurer trying  to deceive us intentionally, we can be mystified in both cases for  essentially the same reason: appearances are not self-explanatory. If  the explanation of a conjuring trick were evident in its appearance,  there would be no trick. If the explanations of physical phenomena  were evident in their appearance, empiricism would be true and there  would be no need for science as we know it. The problem is not to predict the trick’s appearance. I may, for  instance, predict that if a conjurer seems to place various balls under  various cups, those cups will later appear to be empty; and I may  predict that if the conjurer appears to saw someone in half, that person  15 The Reach of Explanations will later appear on stage unharmed. Those are testable predictions. I  may experience many conjuring shows and see my predictions vindi- cated every time. But that does not even address, let alone solve, the  problem of how the trick works. Solving it requires an explanation: a  statement of the reality that accounts for the appearance. Some people may enjoy conjuring tricks without ever wanting to  know how they work. Similarly, during the twentieth century, most  philosophers, and many scientists, took the view that science is incapable  of discovering anything about reality. Starting from empiricism, they  drew the inevitable conclusion (which would nevertheless have horrified  the early empiricists) that science cannot validly do more than predict  the outcomes of observations, and that it should never purport to  describe the reality that brings those outcomes about. This is known as  instrumentalism.  It denies that what I have been calling ‘explanation’  can exist at all. It is still very influential. In some fields (such as statistical  analysis) the very word ‘explanation’ has come to mean prediction, so  that a mathematical formula is said to ‘explain’ a set of experimental  data. By ‘reality’ is meant merely the  observed data  that the formula is  supposed to approximate. That leaves no term for assertions about  reality itself, except perhaps ‘useful fiction’. Instrumentalism is one of many ways of denying  realism ,   the common- sense, and true, doctrine that the physical world really exists, and is  accessible to rational inquiry. Once one has denied this, the logical  implication is that all claims about reality are equivalent to myths,    none of them being better than the others in any objective sense. That  is  relativism ,   the doctrine that statements in a given field cannot be  objectively true or false: at most they can be judged so relative to some  cultural or other arbitrary standard. Instrumentalism, even aside from the philosophical enormity of  reducing science to a collection of statements about human experiences,  does not make sense in its own terms. For there is no such thing as a  purely predictive, explanationless theory. One cannot make even the  simplest prediction without invoking quite a sophisticated explanatory  framework. For example, those predictions about conjuring tricks  apply specifically to conjuring tricks. That is explanatory information,  and it tells me, among other things, not to ‘extrapolate’ the predictions  to another type of situation, however successful they are at predicting  16 the beginning of infinity conjuring tricks. So I know not to predict that saws in general are  harmless to humans; and I continue to predict that if  I  were to place  a ball under a cup, it really would go there and stay there.  The concept of a conjuring trick, and of the distinction between it  and other situations, is familiar and unproblematic – so much so that it  is easy to forget that it depends on substantive explanatory theories  about all sorts of things such as how our senses work, how solid matter  and light behave, and also subtle cultural details. Knowledge that is both  familiar and uncontroversial is  background knowledge.  A predictive  theory whose explanatory content consists only of back  ground know- ledge is a  rule of thumb . Because we usually take background knowledge  for granted, rules of thumb may seem to be explanationless predictions,  but that is always an illusion.  There is always an explanation, whether we know it or not, for why  a rule of thumb works. Denying that some regularity in nature has an  explanation is effectively the same as believing in the supernatural –  saying, ‘That’s not conjuring, it’s actual magic.’ Also, there is always  an explanation when a rule of thumb  fails , for rules of thumb are  always parochial: they hold only in a narrow range of familiar circum- stances. So, if an unfamiliar feature were introduced into a cups-  and-balls trick, the rule of thumb I stated might easily make a false  prediction. For instance, I could not
========================================================
 because of the special measures that prevent this. Hence, information  is processed separately in each of that vast number of autonomous  histories. Finally, an interference process involving all the affected qubits  combines the information in those histories into a single history. Because  of the intervening computation, which has processed the information,  the final state is not the same as the initial one, as in the simple inter- ference experiment I discussed above, namely        ,   but is some  function of it, like this: 296 the beginning of infinity A typical quantum computation.  Y 1  . . .  Y many  are intermediate results that  depend on the input  X . All of them are needed to compute the output  f  ( X ) efficiently. Just as the starship crew members could achieve the effect of large  amounts of computation by sharing information with their doppel- gängers computing the same function on different inputs, so an  algorithm that makes use of quantum parallelism does the same. But,  while the fictional effect is limited only by starship regulations that we  may invent to suit the plot, quantum computers are limited by the laws  of physics that govern quantum interference. Only certain types of  parallel computation can be performed with the help of the multiverse  in this way. They are the ones for which the mathematics of quantum  interference happens to be just right for combining into a single history  the information that is needed for the final result. In such computations, a quantum computer with only a few hundred  qubits could perform far more computations in parallel than there are  atoms in the visible universe. At the time of writing, quantum computers  with about ten qubits have been constructed. ‘Scaling’ the technology  to larger numbers is a tremendous challenge for quantum technology,  but it is gradually being met. I mentioned above that, when a large object is affected by a small  influence, the usual outcome is that the large object is strictly unaffected.  I can now explain why. For example, in the Mach–Zehnder interfero- meter, shown earlier, two instances of a single photon travel on two  different paths. On the way, they strike two different mirrors. Interference  will happen only if the photon does not become entangled with the  mirrors – but it  will  become entangled if either mirror retains the slightest  record that it has been struck (for that would be a differential effect of  the instances on the two different paths). Even a single quantum of  change in the amplitude of the mirror’s vibration on its supports, for  Y 1 f(X) X … Y 2 Y (many) splitting interference යය 297 The Multiverse instance, would be enough to prevent the interference (the subsequent  merging of the photon’s two instances).  When one of the instances of the photon bounces off either mirror,  its momentum changes, and hence by the principle of the conservation  of momentum (which holds universally in quantum physics, just as in  classical physics), the mirror’s momentum must change by an equal  and opposite amount. Hence it seems that, in each history, one mirror  but not the other must be left vibrating with slightly more or less energy  after the photon has struck it. That energy change would be a record  of which path the photon took, and hence the mirrors would be  entangled with the photon. Fortunately, that is not what happens. Remember that, at a sufficiently  fine level of detail, what we crudely see as a single history of the mirror,  resting passively or vibrating gently on its supports, is actually a vast  number of histories with instances of all its atoms continually splitting  and rejoining. In particular, the total energy of the mirror takes a vast  number of possible values around the average, ‘classical’ one. Now,  what happens when a photon strikes the mirror, changing that total  energy by one quantum?  Oversimplifying for a moment, imagine just five of those countless  instances of the mirror, with each instance having a different vibrational  energy ranging from two quanta below the average to two quanta  above it. Each instance of the photon strikes one instance of the mirror  and imparts one additional quantum of energy to it. So, after that  impact, the average energy of the instances of the mirror will have  increased by one quantum, and there will now be instances with  energies ranging from one quantum below the old average to three  above. But since, at this fine level of detail, there is no autonomous  history associated with any of those values of the energy, it is not  meaningful to ask whether an instance of the mirror with a particular  energy after the impact is  the same  one that previously had that energy.  The objective physical fact is only that, of the five instances of the  mirror, four have energies that were present before, and one does not.  Hence, only that one – whose energy is three quanta higher than the  previous average – carries any record of the