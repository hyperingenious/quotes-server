======================================================== up. The cause, it seemed, was some ineffable
thing in modern air. Not only that, but the mystery air additive somehow
supercharged modern brains specifically for the most abstract tests. What
manner of change, Flynn wondered, could be at once so large and yet so
particular?
Through the late 1920s and early 1930s, remote reaches of the Soviet Union
were forced through social and economic changes that would normally take
generations. Individual farmers in isolated areas of what is now Uzbekistan
had long survived by cultivating small gardens for food, and cotton for
everything else. Nearby in the mountain pasturelands of present-day
Kyrgyzstan, herders kept animals. The population was entirely illiterate, and
a hierarchical social structure was enforced by strict religious rules. The
socialist revolution dismantled that way of life almost overnight.
The Soviet government forced all that agricultural land to become large
collective farms and began industrial development. The economy quickly
became interconnected and complex. Farmers had to form collective work
strategies, plan ahead for production, divvy up functions, and assess work

along the way. Remote villages began communicating with distant cities. A
network of schools opened in regions with 100 percent illiteracy, and adults
began learning a system of matching symbols to sounds. Villagers had used
numbers before, but only in practical transactions. Now they were taught the
concept of a number as an abstraction that existed even without reference to
counting animals or apportioning food. Some village women remained fully
illiterate but took short courses on how to teach kindergartners. Other
women were admitted for longer study at a teachers’ school. Classes in
preschool education and the science and technology of agriculture were
offered to students who had no formal education of any kind. Secondary
schools and technical institutes soon followed. In 1931, amid that incredible
transformation, a brilliant young Russian psychologist named Alexander
Luria recognized a fleeting “natural experiment,” unique in the history of the
world. He wondered if changing citizens’ work might also change their
minds.
When Luria arrived, the most remote villages had not yet been touched by
the warp-speed restructuring of traditional society. Those villages gave him a
control group. He learned the local language and brought fellow
psychologists to engage villagers in relaxed social situations—teahouses or
pastures—and discuss questions or tasks designed to discern their habits of
mind.
Some were very simple: present skeins of wool or silk in an array of hues
and ask participants to describe them. The collective farmers and farm
leaders, as well as the female students, easily picked out blue, red, and
yellow, sometimes with variations, like dark blue or light yellow. The most
remote villagers, who were still “premodern,” gave more diversified
descriptions: cotton in bloom, decayed teeth, a lot of water, sky, pistachio.
Then they were asked to sort the skeins into groups. The collective farmers,
and young people with even a little formal education, did so easily, naturally
forming color groups. Even when they did not know the name of a particular
color, they had little trouble putting together darker and lighter shades of the
same one. The remote villagers, on the other hand, refused, even those
whose work was embroidery. “It can’t be done,” they said, or, “None of
them are the same, you can’t put them together.” When prodded vigorously,
and only if they were allowed to make many small groups, some relented
and created sets that were apparently random. A few others appeared to sort
the skeins according to color saturation, without regard to the color.

Geometric shapes followed suit. The greater the dose of modernity, the
more likely an individual grasped the abstract concept of “shapes” and made
groups of triangles, rectangles, and circles, even if they had no formal
education and did not know the shapes’ names. The remote villagers,
meanwhile, saw nothing alike in a square drawn with solid lines and the
same exact square drawn with dotted lines. To Alieva, a twenty-six-year-old
remote villager, the solid-line square was obviously a map, and the dotted-
line square was a watch. “How can a map and a watch be put together?” she
asked, incredulous. Khamid, a twenty-four-year-old remote villager, insisted
that filled and unfilled circles could not go together because one was a coin
and the other a moon.
The pattern continued for every genre of question. Pressed to make
conceptual groupings—akin to the similarities questions on IQ tests—remote
villagers reverted to practical narratives based on their direct experience.
When psychologists attempted to explain a “which one does not belong”
grouping exercise to thirty-nine-year-old Rakmat, they gave him the
example of three adults and one child, with the child obviously different
from the others. Except Rakmat could not see it that way. “The boy must
stay with the others!” he argued. The adults are working, “and if they have to
keep running out to fetch things, they’ll never get the job done, but the boy
can do the running for them.” Okay, then, how about a hammer, a saw, a
hatchet, and a log—three of them are tools. They are not a group, Rakmat
replied, because they are useless without the log, so why would they be
together?
Other villagers removed either the hammer or the hatchet, which they saw
as less versatile for use with the log, unless they considered pounding the
hatchet into the log with the hammer, in which case it could stay. Perhaps,
then, bird/ rifle/ dagger/ bullet? You can’t possibly remove one and have a
group, a remote villager insisted. The bullet must be loaded in the rifle to kill
the bird, and “then you have to cut the bird up with the dagger, since there’s
no other way to do it.” These were just the introductions explaining the
grouping task, not the actual questions. No amount of cajoling, explanation,
or examples could get remote villagers to use reasoning based on any
concept that was not a concrete part of their daily lives .
The farmers and students who had begun to join the modern world were
able to practice a kind of thinking called “eduction,” to work out guiding
principles when given facts or materials, even in the absence of instructions,

and even when they had never seen the material before. This, it turns out, is
precisely what Raven’s Progressive Matrices tests. Imagine presenting the
villagers living in premodern circumstances with abstract designs from the
Raven’s test.
Some of the changes wrought by modernity and collective culture seem
almost magical. Luria found that most remote villagers were not subject to
the same optical illusions as citizens of the industrialized world, like the
Ebbinghaus illusion. Which middle circle below looks bigger?
If you said the one on the right, you’re probably a citizen of the
industrialized world. The remote villagers saw, correctly, that they are the
same, while the collective farmers and women in teachers’ school picked the
one on the right. Those findings have been repeated in other traditional
societies, and scientists have suggested it may reflect the fact that premodern
people are not as drawn to the holistic context—the relationship of the
various circles to one another—so their perception is not changed by the
presence of extra circles. To use a common metaphor, premodern people
miss the forest for the trees; modern people miss the trees for the forest.
Since Luria’s voyage to the interior, scientists have replicated his work in
other cultures. The Kpelle people in Liberia were subsistence rice farmers,

but in the 1970s roads began snaking toward them, connecting the Kpelle to
cities. Given similarities tests, teenagers who were engaged with modern
institutions grouped items by abstract categories (“All of these things can
keep us warm”), while the traditional teens generated groups that were
comparatively arbitrary, and changed frequently even when they were asked
to repeat the exact same task. Because the touched-by-modernity teens had
constructed meaningful thematic groups, they also had far superior recall
when asked later to recount the items. The more they had moved toward
modernity, the more powerful their abstract thinking, and the less they had to
rely on their concrete experience of the world as a reference point.
In Flynn’s terms, we now see the world through “scientific spectacles.” He
means that rather than relying on our own direct experiences, we make sense
of reality through classification schemes, using layers of abstract concepts to
understand how pieces of information relate to one another. We have grown
up in a world of classification schemes totally foreign to the remote
villagers; we classify some animals as mammals, and inside of that class
make more detailed connections based on the similarity of their physiology
and DNA.
Words that represent concepts that were previously the domain of scholars
became widely understood in a few generations. The word “percent” was
almost absent from books in 1900. By 2000 it appeared about once every
five thousand words. (This chapter is 5,500 words long.) Computer
programmers pile layers of abstraction. (They do very well on Raven’s.) In
the progress bar on your computer screen that fills up to indicate a
download, abstractions are legion, from the fundamental—the programming
language that created it is a representation of binary code, the raw 1s and 0s
the computer uses—to the psychological: the bar is a visual projection of
time that provides peace of mind by estimating the progress of an immense
number of underlying activities.
Lawyers might consider how results of one court case brought by an
individual in Oklahoma could be relevant to a different one brought by a
company in California. In order to prep, they might try out different
hypothetical arguments while putting themselves in the======================================================== On Wednesday, he
performed so beautifully that the audience demanded an encore of all seven
movements. In 1998, alongside Sir Edmund Hillary, who with Tenzing
Norgay was the first to summit Mount Everest, Smith was awarded
Smithsonian’s Bicentennial Medal for outstanding cultural contributions.
Pianist Dave Brubeck earned the medal as well. His song “Take Five”
was chosen by NPR listeners as the quintessential jazz tune of all time.
Brubeck’s mother tried to teach him piano, but he refused to follow
instructions. He was born cross-eyed, and his childhood reluctance was
related to his inability to see the musical notation. His mother gave up, but
he listened when she taught others and tried to imitate. Brubeck still could
not read music when he dropped out of veterinary premed at the College of
the Pacific and walked across the lawn to the music department, but he was
a masterful faker. He put off studying piano for instruments that would
more easily allow him to improvise his way through exercises. Senior year,
he could hide no longer. “I got a wonderful piano teacher,” he recalled,
“who figured out I couldn’t read in about five minutes.” The dean informed
Brubeck that he could not graduate and furthermore was a disgrace to the
conservatory. Another teacher who had noticed his creativity stuck up for
him, and the dean cut a deal. Brubeck was allowed to graduate on the
condition that he promise never to embarrass the institution by teaching.

Twenty years later, the college apparently felt it had sufficiently escaped
embarrassment, and awarded him an honorary doctorate.
Perhaps the greatest improv master of all could not read, period—words
or music. Django Reinhardt was born in Belgium in 1910, in a Romani
caravan. His early childhood talents were chicken stealing and trout tickling
—feeling along a riverbank for fish and rubbing their bellies until they
relaxed and could be tossed ashore. Django grew up outside Paris in an area
called la Zone, where the city’s cesspool cleaners unloaded waste each
night. His mother, Négros, was too busy supporting the family making
bracelets out of spent artillery shell casings she gathered from a World War
I battlefield to lord over anyone’s music practice. Django went to school if
he felt like it, but he mostly didn’t. He crashed movie theaters and shot
billiards, and was surrounded by music. Wherever Romani gathered, there
were banjos, harps, pianos, and especially violins.
The violin’s portability made it the classic Romani instrument, and
Django started there, but he didn’t love it. He learned in the call-and-
response style. An adult would play a section of music and he would try to
copy it. When he was twelve, an acquaintance gave him a hybrid banjo-
guitar. He had found his thing, and became obsessed. He experimented with
different objects as picks when his fingers needed a break: spoons, sewing
thimbles, coins, a piece of whalebone. He teamed up with a banjo-playing
hunchback named Lagardère, and they wandered the Paris streets, busking
and improvising duets.
In his mid teens, Django was at a restaurant in Paris where the city’s
accordionists had gathered. He and his banjo-guitar were asked to the stage
to play for the other musicians. Django launched into a polka that was
known as a skill-proving piece for accordionists because it was so hard to
play. When he finished the traditional form, rather than stopping he
careened into a series of lightning improvisations, bending and twisting the
song into creations none of the veteran musicians had ever heard. Django
was playing “with a drawn knife,” as the lingo went. He was looking for a
fight by warping a sacred dancehall tune, but he was so original that he got
away with it. His creativity was unbound. “I wonder if, in his younger
days,” one of his music partners said, “he even knew that printed music
existed.” Django would soon need all the versatility he had learned.
He was eighteen when a candle in his wagon ignited a batch of celluloid
flowers that his wife, Bella, had fashioned for a funeral. The wagon

exploded into an inferno. Django was burned over half his body and ended
up bedridden for a year and a half. For the rest of his life the pinkie and ring
finger of his left hand, his fret hand, were dangling flesh, useless on the
strings. Django was used to improvising. Like Pelegrina of the figlie del
coro when she lost her teeth, he pivoted. He taught himself how to play
chords with a thumb and two fingers. His left hand had to sprint up and
down the neck of his guitar, the index and middle finger flitting waterbug-
like over the strings. He re-emerged with a new way of handling the
instrument, and his creativity erupted.
With a French violinist, Django fused dancehall musette with jazz and
invented a new form of improvisational music that defied easy
characterization, so it was just called “Gypsy jazz.” Some of his
spontaneous compositions became “standards,” pieces that enter the canon
from which other musicians improvise. He revolutionized the now-familiar
virtuosic guitar solo that pervaded the next generation’s music, from Jimi
Hendrix, who kept an album of Django’s recordings and named one of his
groups Band of Gypsys, to Prince ( self-taught, played more than a half-
dozen different genres of instruments on his debut album). Long before
Hendrix melted “The Star-Spangled Banner” into his own wondrous
creation, Django did it with the French national anthem, “La Marseillaise.”
Even though he never learned to read music (or words—a fellow
musician had to teach him to sign his autograph for fans), Django composed
a symphony, playing on his guitar what he wanted each instrument in the
ensemble to do while another musician struggled to transcribe it.
He died of a brain hemorrhage at forty-three, but music he made nearly a
century ago continues to show up in pop culture, including Hollywood
blockbusters like The Matrix and The Aviator, and in the hit BioShock video
games. The author of The Making of Jazz anointed the man who could
neither read music nor study it with the traditional fingerings “without
question, the single most important guitarist in the history of jazz.”
Cecchini has bushy eyebrows and a beard that parts and closes quickly like
ruffled shrubbery when he talks excitedly. Like now: he’s talking Django,
and he’s a huge fan. He used to have a black poodle named Django. He
opens a sepia-toned YouTube clip and whispers conspiratorially, “Watch
this.”

There is Django, bow tie, pencil mustache, and slicked-back hair. The
two useless fingers on his left hand are tucked into a claw. Suddenly, the
hand shoots all the way up the guitar neck, and then all the way back down,
firing a rapid succession of notes. “That’s amazing!” Cecchini says. “The
synchronization between the left and right hand is phenomenal.”
The strict deliberate practice school describes useful training as focused
consciously on error correction. But the most comprehensive examination
of development in improvisational forms, by Duke University professor
Paul Berliner, described the childhoods of professionals as “one of
osmosis,” not formal instruction. “Most explored the band room’s diverse
options as a prelude to selecting an instrument of specialization,” he wrote.
“It was not uncommon for youngsters to develop skills on a variety of
instruments.” Berliner added that aspiring improvisational musicians
“whose educational background has fostered a fundamental dependence on
[formal] teachers must adopt new approaches to learning.” A number of
musicians recounted
Brubeck-like scenarios to Berliner, the time a teacher found out that they
could not read music but had become adept enough at imitation and
improvisation that “they had simply pretended to follow the notation.”
Berliner relayed the advice of professional musicians to a young
improvisational learner as “not to think about playing—just play. ”
While I was sitting with Cecchini, he reeled off an impressive
improvisation. I asked him to repeat it so I could record it. “I couldn’t play
that again if you put a gun to my head,” he said. Charles Limb, a musician,
hearing specialist, and auditory surgeon at the University of California, San
Francisco, designed an iron-free keyboard so that jazz musicians could
improvise while inside an MRI scanner. Limb saw that brain areas
associated with focused attention, inhibition, and self-censoring turned
down when the musicians were creating. “It’s almost as if the brain turned
off its own ability to criticize itself,” he told National Geographic. While
improvising, musicians do pretty much the opposite of consciously
identifying errors and stopping to correct them.
Improv masters learn like babies: dive in and imitate and improvise first,
learn the formal rules later. “At the beginning, your mom didn’t give you a
book and say, ‘This is a noun, this is a pronoun, this is a dangling
participle,’” Cecchini told me. “You acquired the sound first. And then you
acquire the grammar later.”

Django Reinhardt was once in a taxi with Les Paul, inventor of the solid-
body electric guitar. Paul was a self-taught musician, and the only person in
both the Rock and Roll and National Inventors halls of fame. Reinhardt
tapped Paul on the shoulder and asked if he could read music. “I said no, I
didn’t,” Paul recounted, “and he laughed till he was crying and said, ‘Well, I
can’t read either. I don’t even know what a C is; I just play them.’”
Cecchini told me that he was regularly stunned when he would ask an
exceptional jazz performer onstage to play a certain note, and find the
musician could not understand him. “It’s an old joke among jazz
musicians,” Cecchini said. “You ask, ‘Can you read music?’ And the guy
says, ‘Not enough to hurt my playing.’” There is truth in the joke. Cecchini
has taught musicians who played professionally for the Chicago Symphony,
which in 2015 was rated as the top orchestra in the country and fifth in the
world by a panel of critics.======================================================== every area. This must change, he argues, if students are to
capitalize on their unprecedented capacity for abstract thought. They must be

taught to think before being taught what to think about. Students come
prepared with scientific spectacles, but do not leave carrying a scientific-
reasoning Swiss Army knife.
Here and there, professors have begun to pick up the challenge. A class at
the University of Washington titled “Calling Bullshit” (in staid coursebook
language: INFO 198/ BIOL 106B), focused on broad principles fundamental
to understanding the interdisciplinary world and critically evaluating the
daily firehose of information. When the class was first posted in 2017,
registration filled up in the first minute.
Jeannette Wing, a computer science professor at Columbia University and
former corporate vice president of Microsoft Research, has pushed broad
“computational thinking” as the mental Swiss Army knife. She advocated
that it become as fundamental as reading, even for those who will have
nothing to do with computer science or programming. “Computational
thinking is using abstraction and decomposition when attacking a large
complex task,” she wrote. “It is choosing an appropriate representation for a
problem.”
Mostly, though, students get what economist Bryan Caplan called narrow
vocational training for jobs few of them will ever have. Three-quarters of
American college graduates go on to a career unrelated to their major—a
trend that includes math and science majors—after having become
competent only with the tools of a single discipline.
One good tool is rarely enough in a complex, interconnected, rapidly
changing world. As the historian and philosopher Arnold Toynbee said when
he described analyzing the world in an age of technological and social
change, “No tool is omnicompetent.”
Flynn’s passion resonated deeply with me. Before turning to journalism, I
was in grad school, living in a tent in the Arctic, studying how changes in
plant life might impact the subterranean permafrost. Classes consisted of
stuffing my brain with the details of Arctic plant physiology. Only years later
—as an investigative journalist writing about poor scientific research—did I
realize that I had committed statistical malpractice in one section of the
thesis that earned me a master’s degree from Columbia University. Like
many a grad student, I had a big database and hit a computer button to run a
common statistical analysis, never having been taught to think deeply (or at

all) about how that statistical analysis even worked. The stat program spit
out a number summarily deemed “statistically significant.” Unfortunately, it
was almost certainly a false positive, because I did not understand the
limitations of the statistical test in the context in which I applied it. Nor did
the scientists who reviewed the work. As statistician Doug Altman put it,
“Everyone is so busy doing research they don’t have time to stop and think
about the way they’re doing it.” I rushed into extremely specialized scientific
research without having learned scientific reasoning. (And then I was
rewarded for it, with a master’s degree, which made for a very wicked
learning environment.) As backward as it sounds, I only began to think
broadly about how science should work years after I left it.
Fortunately, as an undergrad, I did have a chemistry professor who
embodied Flynn’s ideal. On every exam, amid typical chemistry questions,
was something like this: “How many piano tuners are there in New York
City?” Students had to estimate, just by reasoning, and try to get the right
order of magnitude. The professor later explained that these were “Fermi
problems,” because Enrico Fermi—who created the first nuclear reactor
beneath the University of Chicago football field—constantly made back-of-
the-envelope estimates to help him approach problems. 
*
 The ultimate lesson
of the question was that detailed prior knowledge was less important than a
way of thinking.
On the first exam, I went with gut instinct (“I have no clue, maybe ten
thousand?”)— way too high . By the end of the class, I had a new tool in my
conceptual Swiss Army knife, a way of using what little I did know to make
a guess at what I didn’t. I knew the population of New York City; most
single people in studio apartments probably don’t have pianos that get tuned,
and most of my friends’ parents had one to three children, so how many
households are in New York? What portion might have pianos? How often
are pianos tuned? How long might it take to tune a piano? How many homes
can one tuner reach in a day? How many days a year does a tuner work?
None of the individual estimates has to be particularly accurate in order to
get a reasonable overall answer. Remote Uzbek villagers would not perform
well on Fermi problems, but neither did I before taking that class. It was
easy to learn, though. Having grown up in the twentieth century, I was
already wearing the spectacles, I just needed help capitalizing on them. I
remember nothing about stoichiometry, but I use Fermi thinking regularly,

breaking down a problem so I can leverage what little I know to start
investigating what I don’t, a “similarities” problem of sorts.
Fortunately, several studies have found that a little training in broad
thinking strategies, like Fermi-izing, can go a long way, and can be applied
across domains. Unsurprisingly, Fermi problems were a topic in the “Calling
Bullshit” course. It used a deceptive cable news report as a case study to
demonstrate “how Fermi estimation can cut through bullshit like a hot knife
through butter.” It gives anyone consuming numbers, from news articles to
advertisements, the ability quickly to sniff out deceptive stats. That’s a pretty
handy hot butter knife. I would have been a much better researcher in any
domain, including Arctic plant physiology, had I learned broadly applicable
reasoning tools rather than the finer details of Arctic plant physiology.
Like chess masters and firefighters, premodern villagers relied on things
being the same tomorrow as they were yesterday. They were extremely well
prepared for what they had experienced before, and extremely poorly
equipped for everything else. Their very thinking was highly specialized in a
manner that the modern world has been telling us is increasingly obsolete.
They were perfectly capable of learning from experience, but failed at
learning without experience. And that is what a rapidly changing, wicked
world demands—conceptual reasoning skills that can connect new ideas and
work across contexts. Faced with any problem they had not directly
experienced before, the remote villagers were completely lost. That is not an
option for us. The more constrained and repetitive a challenge, the more
likely it will be automated, while great rewards will accrue to those who can
take conceptual knowledge from one problem or domain and apply it in an
entirely new one.
The ability to apply knowledge broadly comes from broad training. A
particular skilled group of performers in another place and time turned broad
training into an art form. Their story is older, and yet a much better parable
than chess prodigies for the modern age.
OceanofPDF.com

CHAPTER 3
When Less of the Same Is More
ANYWHERE A TRAVELER to seventeenth-century Venice turned an ear, they
could hear music exploding from its traditional bounds. Even the name of
the musical era, “Baroque,” is taken from a jewelers’ term to describe a
pearl that was extravagantly large and unusually shaped.
Instrumental music—music that did not depend on words—underwent a
complete revolution. Some of the instruments were brand-new, like the
piano; others were enhanced—violins made by Antonio Stradivari would
sell centuries later for millions of dollars. The modern system of major and
minor keys was created. Virtuosos, the original musical celebrities, were
anointed. Composers seized on their skill and wrote elaborate solos to push
the boundaries of the best players’ abilities. The concerto was born—in
which a virtuoso soloist plays back and forth against an orchestra—and
Venetian composer Antonio Vivaldi (known as il Prete Rosso, the Red
Priest, for his flame-red hair) became the form’s undisputed champion. The
Four Seasons is as close to a pop hit as three-hundred-year-old music gets.
(A mashup with a song from Disney’s Frozen has ninety million YouTube
plays.)
Vivaldi’s creativity was facilitated by a particular group of musicians
who could learn new music quickly on a staggering array of instruments.
They drew emperors, kings, princes, cardinals, and countesses from across
Europe to be regaled by the most innovative music of the time. They were
the all-female cast known as the figlie del coro, literally, “daughters of the
choir.” Leisure activities like horseback riding and field sports were scarce
in the floating city, so music bore the full weight of entertainment for its

citizens. The sounds of violins, flutes, horns, and voices spilled into the
night from every bobbing barge and gondola. And in a time and place
seething with music, the figlie dominated for a century.
“Only in Venice,” a prominent visitor wrote, “can one see these musical
prodigies.” They were both ground zero of a musical revolution and an
oddity. Elsewhere, their instruments were reserved for men. “They sing like
angels, play the violin, the flute, the organ, the oboe, the cello, and the
bassoon,” an astonished French politician remarked. “In short, no
instrument is large enough to frighten them.” Others were less diplomatic.
Aristocratic British writer Hester Thrale complained, “The sight of girls
handling the double bass, and blowing into the bassoon did not much please
me.” After all, “suitable feminine instruments” were more along the lines of
the harpsichord or musical glasses.
The figlie left the king of Sweden in awe. Literary rogue Casanova
marveled at the standing-room-only crowds. A dour French concert
reviewer singled out a======================================================== many different
sources”; “they appear to flit among ideas”; “broad range of interests”;
“they read more (and more broadly) than other technologists and have a
wider range of outside interests”; “need to learn significantly across
multiple domains”; “Serial innovators also need to communicate with
various individuals with technical expertise outside of their own domain.”
Get the picture?
Charles Darwin “could be considered a professional outsider,” according
to creativity researcher Dean Keith Simonton. Darwin was not a university
faculty member nor a professional scientist at any institution, but he was
networked into the scientific community. For a time, he focused narrowly
on barnacles, but got so tired of it that he declared, “I am unwilling to spend
more time on the subject,” in the introduction to a barnacle monograph.
Like the 3M generalists and polymaths, he got bored sticking in one area, so
that was that. For his paradigm-shattering work, Darwin’s broad network
was crucial. Howard Gruber, a psychologist who studied Darwin’s journals,
wrote that Darwin only personally carried out experiments “opportune for
experimental attack by a scientific generalist such as he was.” For
everything else, he relied on correspondents, Jayshree Seth style. Darwin
always juggled multiple projects, what Gruber called his “network of
enterprise.” He had at least 231 scientific pen pals who can be grouped
roughly into thirteen broad themes based on his interests, from worms to

human sexual selection. He peppered them with questions. He cut up their
letters to paste pieces of information in his own notebooks, in which “ideas
tumble over each other in a seemingly chaotic fashion.” When his chaotic
notebooks became too unwieldy, he tore pages out and filed them by themes
of inquiry. Just for his own experiments with seeds, he corresponded with
geologists, botanists, ornithologists, and conchologists in France, South
Africa, the United States, the Azores, Jamaica, and Norway, not to mention
a number of amateur naturalists and some gardeners he happened to know.
As Gruber wrote, the activities of a creator “may appear, from the outside,
as a bewildering miscellany,” but he or she can “map” each activity onto
one of the ongoing enterprises. “In some respects,” Gruber concluded,
“Charles Darwin’s greatest works represent interpretative compilations of
facts first gathered by others.” He was a lateral-thinking integrator .
Toward the end of their book Serial Innovators, Abbie Griffin and her
coauthors depart from stoically sharing their data and observations and offer
advice to human resources managers. They are concerned that HR policies
at mature companies have such well-defined, specialized slots for
employees that potential serial innovators will look like “round pegs to the
square holes” and get screened out. Their breadth of interests do not neatly
fit a rubric. They are “π-shaped people” who dive in and out of multiple
specialties. “Look for wide-ranging interests,” they advised. “Look for
multiple hobbies and avocations. . . . When the candidate describes his or
her work, does he or she tend to focus on the boundaries and the interfaces
with other systems?” One serial innovator described his network of
enterprise as “a bunch of bobbers hanging in the water that have little
thoughts attached to them.” Hamilton creator Lin-Manuel Miranda painted
the same idea elegantly: “I have a lot of apps open in my brain right now.”
Griffin’s research team noticed that serial innovators repeatedly claimed
that they themselves would be screened out under their company’s current
hiring practices. “A mechanistic approach to hiring, while yielding highly
reproducible results, in fact reduces the numbers of high-potential [for
innovation] candidates,” they wrote. When I first spoke with him, Andy
Ouderkirk was developing a class at the University of Minnesota partly
about how to identify potential innovators. “We think a lot of them might be
frustrated by school,” he said, “because by nature they’re very broad.”
Facing uncertain environments and wicked problems, breadth of
experience is invaluable. Facing kind problems, narrow specialization can

be remarkably efficient. The problem is that we often expect the
hyperspecialist, because of their expertise in a narrow area, to magically be
able to extend their skill to wicked problems. The results can be disastrous.
OceanofPDF.com

CHAPTER 10
Fooled by Expertise
THE BET WAS ON , and it was over the fate of humanity.
On one side was Stanford biologist Paul Ehrlich. In congressional
testimony, on The Tonight Show (twenty times), and in his 1968 bestseller
The Population Bomb, Ehrlich insisted that it was too late to prevent a
doomsday apocalypse from overpopulation. On its lower left corner, the
book cover bore an image of a fuse burning low, and a reminder that the
“bomb keeps ticking.” Resource shortages would cause hundreds of
millions of starvation deaths within a decade, Ehrlich warned. The New
Republic alerted the world that the global population had already
outstripped the food supply. “The famine has started,” it proclaimed. It was
cold, hard math: human population was growing exponentially, the food
supply was not. Ehrlich was a butterfly specialist, and an accomplished one.
He knew full well that nature did not regulate animal populations delicately.
Populations exploded, blew past the available resources, and crashed. “The
shape of the population growth curve is one familiar to the biologist,” he
wrote.
Ehrlich played out hypothetical scenarios in his book, representing “the
kinds of disasters that will occur.” In one scenario, during the 1970s the
United States and China start blaming one another for mass starvation and
end up in a nuclear war. That’s the moderate scenario. In the bad one,
famine rages across the planet. Cities alternate between riots and martial
law. The American president’s environmental advisers recommend a one-
child policy and sterilization of people with low IQ scores. Russia, China,
and the United States are dragged into nuclear war, which renders the

northern two-thirds of Earth uninhabitable. Pockets of society persist in the
Southern Hemisphere, but the environmental degradation soon extinguishes
the human race. In the “cheerful” scenario, population controls begin. The
pope announces that Catholics should reproduce less, and gives his blessing
to abortion. Famine spreads, and countries teeter. By the mid-1980s, the
major death wave ends and agricultural land can begin to be rehabilitated.
The cheerful scenario only forecast half a billion or so deaths by starvation.
“I challenge you to create one more optimistic,” Ehrlich wrote, adding that
he would not count scenarios involving benevolent aliens with care
packages.
Economist Julian Simon took up Ehrlich’s challenge to create a more
optimistic picture. The late 1960s was the prime of the “green revolution.”
Technology from other sectors—water control techniques, hybridized seeds,
management strategies—moved into agriculture, and global crop yields
were increasing. Simon saw that innovation was altering the equation. More
people would actually be the solution, because it meant more good ideas
and more technological breakthroughs. So Simon proposed a bet. Ehrlich
could choose five metals that he expected to become more expensive as
resources were depleted and chaos ensued over the next decade. The
material stakes were $1,000 worth of Ehrlich’s five metals. If, ten years
hence, prices had gone down, Ehrlich would have to pay the price
difference to Simon. If prices went up, Simon would be on the hook for the
difference. Ehrlich’s liability was capped at $1,000, whereas Simon’s risk
had no roof. The bet was made official in 1980.
In October 1990, Simon found a check for $576.07 in his mailbox.
Ehrlich got smoked. The price of every one of the metals declined.
Technological change not only supported a growing population, but the
food supply per person increased year after year, on every continent. The
proportion of people who are undernourished is too high until it is zero, but
it has never been so low as it is now. In the 1960s, 50 of every 100,000
global citizens died annually from famine; now that number is 0.5. Even
without the pope’s assistance, the world’s population growth rate began a
precipitous decline that continues today. When child mortality declined and
education (especially for women) and development increased, birth rates
decreased. Humanity will need more innovation as absolute world
population continues to grow, but the growth rate is declining, rapidly. The
United Nations projects that by the end of the century human population

will be near a peak—the growth rate approaching zero—or it could even be
in decline.
Ehrlich’s starvation predictions were almost magically bad. He made
them just as technological development was dramatically altering the global
predicament, and right before the rate of population growth started a long
deceleration. And yet, the very same year he conceded the bet, Ehrlich
doubled down in another book. Sure, the timeline had been a little off, but
“now the population bomb has detonated.” Despite one erroneous
prediction after another, Ehrlich amassed an enormous following and
continued to receive prestigious awards. Simon became a standard-bearer
for scholars who felt that Ehrlich had ignored economic principles, and for
anyone angry at an incessant flow of dire predictions that did not manifest.
The kind of excessive regulations Ehrlich advocated, the Simon camp
argued, would quell the very innovation that had delivered humanity from
catastrophe. Both men became luminaries in their respective domains. And
both were mistaken.
When economists later examined metal prices for every ten-year window
from 1900 to 2008, during which time world population quadrupled, they
saw that Ehrlich would have won the bet 62 percent of the time. The catch:
commodity prices are a bad======================================================== “necessary nor healthy” for children to
be directed toward one career before they can make that decision
themselves—a decision that, again, took her years of adulthood. “So what
does grit look like early in life?” she wrote. “A young child who decides
today that she wants to become a doctor but thinks tomorrow that she’d
rather build houses. A teenager who decides, no, she won’t go out for track
this year and instead will see what it’s like to write for the school
newspaper.” Specialization has benefits, she added, “but before
specialization comes sampling, the exploration of possibilities that, really,
you cannot know anything about until you try them . . . Don’t confuse the
healthy development of a work ethic with the premature commitment to a
singular passion.”
As one researcher suggested to me: “When you get fit, it will look like
grit.” That is, if you help someone find a good fit, they are more likely to
display the characteristics of grit—like sticking with something—even if
they didn’t before.
Needless to say, most people aren’t going to be Tillman Scholars,
executives, or William Shakespeare. And while many of the stories in
Range portray uncommon achievements, I hoped those would serve as
memorable portals of engagement into research that applies to a much
broader swath of humanity.
In fact, international research that studied thousands of workers—more
than three-quarters of whom did not have tertiary education—
produced findings that resonate with a major theme of the book: that
sometimes the actions that provide a head start will undermine long-term
development , whether that is choosing a career or a course of study, or
simply developing a skill or learning new material.
A 2017 study published by four economists in the U.S., Germany, and
China, analyzed education and employment data in eleven countries with

large vocational education or apprenticeship programs, comparing people
within each country who had similar backgrounds—including test scores,
family background, and years of education—but differed in whether they
received career-focused or broader, general education. Naturally, there was
considerable variation between countries and certainly between individuals,
but the general pattern was: people who got narrow, career-focused
education were more likely to be employed right out of school and earned
more right away, but over time both advantages evaporated; decades later,
they had spent less overall time in the labor market and had lower lifetime
earnings. The early specializers often won in the short term, and lost in the
long run. Workers who received general education, the economists
concluded, were better positioned to adapt to change in a wicked world
where work next year might not look like work last year.
The pattern was particularly pronounced in two countries with famously
extensive apprenticeship programs—Denmark and Germany—an important
finding given that, over the last decade, U.S. politicians on both sides of the
aisle have advocated for a move toward the German apprenticeship model.
In 2017, President Trump issued an executive order to expand
apprenticeship programs to prepare workers for “today’s rapidly changing
economy”. The economists, on the other hand, concluded that the more
rapidly a nation’s economy is changing, the greater the long-term advantage
of general education. Of the three countries with widespread apprenticeship
programs—Denmark, Germany, Switzerland—early specialization only
resulted in a lifetime earnings advantage in Switzerland, which has had
easily the slowest growing economy of those three nations in recent
decades. “This comparison is consistent with the idea that those with
general education are more adaptable to changed economic demands,” two
of the economists wrote. “Vocational education has been promoted largely
as a way of improving the transition from schooling to work, but it also
appears to reduce the adaptability of workers to technological and structural
change in the economy.”
Does that mean we should have no early vocational training or
apprenticeships at all? I certainly don’t think so, and one of the economists
who did this work pointed out that apprenticeships still work well in
specific areas, like the building trades, but also that those trades are a small
portion of unfilled jobs. In my opinion, we should preserve a variety of
pathways, to fit a variety of life circumstances. But I also think we need to

be aware of how easy it is to be fooled by head starts, assuming that they
represent terminally stable trajectories, whether the head start be for child
athletes, college students learning math, or workers entering the labor force.
“The advantages of vocational training in smoothing entry into the labor
market,” the economists wrote, “have to be set against disadvantages later
in life, disadvantages that are likely to be more severe as we move more
into being a knowledge economy.”
The question of how broad or specialized to be is important to just about
everyone at some point or another, but usually only discussed with
intuition. Like any complex question that involves human beings, there is
no one-size-fits-all answer. My hope was to make discussions about that
crucial topic more interesting and productive.
I’m a science writer, so in doing that, I wanted to rely heavily on
scientific research that bears on the question from various angles. In talking
with readers after the hardcover publication, though, I’ve increasingly felt
that the stories of late starts or zig-zagging career paths have a specific
importance. The “availability heuristic” is a well-known cognitive bias in
which we tend to rely on the first example that comes to mind when making
a decision or judging an idea. Tiger stories have supplied millions of brains
with availability-heuristic ammo when they think of specialization. I hope
some of the stories in this book—from Federer and Van Gogh to Frances
Hesselbein (now 104, still working)—might stick in readers’ minds and
surface when the Tiger stories do, adding much needed balance to how we
consider the topic.
When I was allotted this space for an afterword to the paperback edition,
I first thought I should stuff it with research that I had to cut from the
hardcover due to space constraints. (My editor talked me off that cliff.)
Instead, I’d like to share one more memorable story.
When asked, Titus Kaphar reflexively says that nobody in his family went
to college. His mother didn’t go to college. His father went to prison, but
not to college. Neither his grandmother nor his grandfather went to college.
Nobody went to college. Except—he corrects himself—a distant cousin
went to college. Kaphar himself certainly wasn’t going to college. He often
didn’t even go to high school, hence the 0.65 GPA.

And yet, in his twenties, he decided to enroll in a few junior college
classes. Allow him to explain: he wanted to date a soon-to-be teacher; she
was four years older, contemplating grad school, and not impressed that he
had no plans for his future.
“So I just went over to the junior college,” Kaphar told me, “kind of as a
joke, not really taking it seriously, because, you know, I’m not an academic.
I’m not a person who does really well in school.” He picked a few classes
more or less at random. He came back and told the object of his affection.
They had a quick laugh about it.
One class was art history. Why? “I probably read the word ‘art’ and
thought, ‘art should be easy,’” Kaphar told me. In an unexpected way, it
was. Kaphar realized that he could remember details of paintings—not just
remember what he had seen, but associate the painting with the style of a
particular artist or artistic movement. “I remember one day we were talking
about Van Gogh,” he recalled, “and I remember seeing the image and being
very aware of where the painting sat in the history of art, where it sat in the
timeline the professor was trying to lay out for us.” He began to contribute
to class discussions. His confidence grew, and he got a B in the class.
“That was a new experience for me,” he told me, “a B overall in the class
at something that was academic. It made me go, ‘Hold up, wait a minute.’ I
realized this was, in fact, something that I was enjoying, and I could feel
myself wanting to push harder. When it became difficult, that grit became
more apparent.” He took more classes, and tested strategies until he found
something that worked, like dictating essays into a recorder for his first
draft, and studying for history tests by focusing on connecting the images in
a book to the surrounding material. Suddenly, college didn’t seem like such
a crazy idea.
Kaphar proceeded to San José State University to study fine arts. In an
art history survey course, to his dismay, the professor skipped over the
section on black painters. So Kaphar decided to compile his own syllabus
on the topic. “I think to some degree, it was like being a reporter doing an
investigation,” he said. A friend’s grandmother was a sculptor, and gave him
books. The first was on the Harlem Renaissance. “That sort of opened the
floodgates,” he recalled. He realized that the traditional artistic canon used
in class was not some magical pantheon, but just another syllabus
assembled by humans. He collected more books, and introduced new names
into class discussion.

Eventually, he took an actual painting class, but the teacher told the class
that he didn’t believe in painting anymore, so he gave no instruction
whatsoever on technique. Instead, Kaphar started looking over other
students’ shoulders and self-teaching. “It was like, last semester I painted an
apple, and it looked like a cannonball,” he told me. “This semester that
actually looks like an apple. Now, it’s loose, a child could do the same
thing, but it looks like an apple. Awesome, let’s keep going.” He began to
study paintings and then try to reverse-engineer them. When he noticed that
shadows in Velázquez paintings didn’t======================================================== decades to realize he had been wrong, and that savants
have more in common with prodigies like the Polgar sisters than he thought.
They do not merely regurgitate. Their brilliance, just like the Polgar
brilliance, relies on repetitive structures, which is precisely what made the
Polgars’ skill so easy to automate.
With the advances made by the AlphaZero chess program (owned by an AI
arm of Google’s parent company), perhaps even the top centaurs would be
vanquished in a freestyle tournament. Unlike previous chess programs,
which used brute processing force to calculate an enormous number of
possible moves and rate them according to criteria set by programmers,
AlphaZero actually taught itself to play. It needed only the rules, and then to
play itself a gargantuan number of times, keeping track of what tends to
work and what doesn’t, and using that to improve. In short order, it beat the
best chess programs. It did the same with the game of Go, which has many
more possible positions. But the centaur lesson remains: the more a task
shifts to an open world of big-picture strategy, the more humans have to
add.
AlphaZero programmers touted their impressive feat by declaring that
their creation had gone from “tabula rasa” (blank slate) to master on its
own. But starting with a game is anything but a blank slate. The program is
still operating in a constrained, rule-bound world. Even in video games that
are less bound by tactical patterns, computers have faced a greater
challenge.

The latest video game challenge for artificial intelligence is StarCraft, a
franchise of real-time strategy games in which fictional species go to war
for supremacy in some distant reach of the Milky Way. It requires much
more complex decision making than chess. There are battles to manage,
infrastructure to plan, spying to do, geography to explore, and resources to
collect, all of which inform one another. Computers struggled to win at
StarCraft , Julian Togelius, an NYU professor who studies gaming AI, told
me in 2017. Even when they did beat humans in individual games, human
players adjusted with “long-term adaptive strategy” and started winning.
“There are so many layers of thinking,” he said. “We humans sort of suck at
all of them individually, but we have some kind of very approximate idea
about each of them and can combine them and be somewhat adaptive. That
seems to be what the trick is.”
In 2019, in a limited version of StarCraft , AI beat a pro for the first time.
(The pro adapted and earned a win after a string of losses.) But the game’s
strategic complexity provides a lesson: the bigger the picture, the more
unique the potential human contribution. Our greatest strength is the exact
opposite of narrow specialization. It is the ability to integrate broadly.
According to Gary Marcus, a psychology and neural science professor who
sold his machine learning company to Uber, “In narrow enough worlds,
humans may not have much to contribute much longer. In more open-ended
games, I think they certainly will. Not just games, in open ended real-world
problems we’re still crushing the machines.”
The progress of AI in the closed and orderly world of chess, with instant
feedback and bottomless data, has been exponential. In the rule-bound but
messier world of driving, AI has made tremendous progress, but challenges
remain. In a truly open-world problem devoid of rigid rules and reams of
perfect historical data, AI has been disastrous. IBM’s Watson destroyed at
Jeopardy! and was subsequently pitched as a revolution in cancer care,
where it flopped so spectacularly that several AI experts told me they
worried its reputation would taint AI research in health-related fields. As
one oncologist put it, “The difference between winning at Jeopardy! and
curing all cancer is that we know the answer to Jeopardy! questions.” With
cancer, we’re still working on posing the right questions in the first place.
In 2009, a report in the esteemed journal Nature announced that Google
Flu Trends could use search query patterns to predict the winter spread of
flu more rapidly than and just as accurately as the Centers for Disease

Control and Prevention. But Google Flu Trends soon got shakier, and in the
winter of 2013 it predicted more than double the prevalence of flu that
actually occurred in the United States. Today, Google Flu Trends is no
longer publishing estimates, and just has a holding page saying that “it is
still early days” for this kind of forecasting. Tellingly, Marcus gave me this
analogy for the current limits of expert machines: “AI systems are like
savants.” They need stable structures and narrow worlds.
When we know the rules and answers, and they don’t change over time—
chess, golf, playing classical music—an argument can be made for savant-
like hyperspecialized practice from day one. But those are poor models of
most things humans want to learn.
When narrow specialization is combined with an unkind domain, the
human tendency to rely on experience of familiar patterns can backfire
horribly—like the expert firefighters who suddenly make poor choices
when faced with a fire in an unfamiliar structure. Chris Argyris, who helped
create the Yale School of Management, noted the danger of treating the
wicked world as if it is kind. He studied high-powered consultants from top
business schools for fifteen years, and saw that they did really well on
business school problems that were well defined and quickly assessed. But
they employed what Argyris called single-loop learning, the kind that favors
the first familiar solution that comes to mind. Whenever those solutions
went wrong, the consultant usually got defensive. Argyris found their
“brittle personalities” particularly surprising given that “the essence of their
job is to teach others how to do things differently.”
Psychologist Barry Schwartz demonstrated a similar, learned inflexibility
among experienced practitioners when he gave college students a logic
puzzle that involved hitting switches to turn light bulbs on and off in
sequence, and that they could play over and over. It could be solved in
seventy different ways, with a tiny money reward for each success. The
students were not given any rules, and so had to proceed by trial and error. 
*
If a student found a solution, they repeated it over and over to get more
money, even if they had no idea why it worked. Later on, new students were
added, and all were now asked to discover the general rule of all solutions.
Incredibly, every student who was brand-new to the puzzle discovered the
rule for all seventy solutions, while only one of the students who had been
getting rewarded for a single solution did. The subtitle of Schwartz’s paper:

“How Not to Teach People to Discover Rules”—that is, by providing
rewards for repetitive short-term success with a narrow range of solutions.
All this is bad news for some of the business world’s favorite successful-
learning analogies—the Polgars, Tiger, and to some degree analogies based
in any sport or game. Compared to golf, a sport like tennis is much more
dynamic, with players adjusting to opponents every second, to surfaces, and
sometimes to their own teammates. (Federer was a 2008 Olympic gold
medalist in doubles.) But tennis is still very much on the kind end of the
spectrum compared to, say, a hospital emergency room, where doctors and
nurses do not automatically find out what happens to a patient after their
encounter. They have to find ways to learn beyond practice, and to
assimilate lessons that might even contradict their direct experience.
The world is not golf, and most of it isn’t even tennis. As Robin Hogarth
put it, much of the world is “Martian tennis.” You can see the players on a
court with balls and rackets, but nobody has shared the rules. It is up to you
to derive them, and they are subject to change without notice .
We have been using the wrong stories. Tiger’s story and the Polgar story
give the false impression that human skill is always developed in an
extremely kind learning environment. If that were the case, specialization
that is both narrow and technical and that begins as soon as possible would
usually work. But it doesn’t even work in most sports.
If the amount of early, specialized practice in a narrow area were the key
to innovative performance, savants would dominate every domain they
touched, and child prodigies would always go on to adult eminence. As
psychologist Ellen Winner, one of the foremost authorities on gifted
children, noted, no savant has ever been known to become a “Big-C
creator,” who changed their field.
There are domains beyond chess in which massive amounts of narrow
practice make for grandmaster-like intuition. Like golfers, surgeons
improve with repetition of the same procedure. Accountants and bridge and
poker players develop accurate intuition through repetitive experience.
Kahneman pointed to those domains’ “robust statistical regularities.” But
when the rules are altered just slightly, it makes experts appear to have
traded flexibility for narrow skill. In research in the game of bridge where
the order of play was altered, experts had a more difficult time adapting to

new rules than did nonexperts. When experienced accountants were asked
in a study to use a new tax law for deductions that replaced a previous one,
they did worse than novices. Erik Dane, a Rice University professor who
studies organizational behavior, calls this phenomenon “cognitive
entrenchment.” His suggestions for avoiding it are about the polar opposite
of the strict version of the ten-thousand-hours school of thought: vary
challenges within a domain drastically, and, as a fellow researcher put it,
insist on “having one foot outside your world.”
Scientists and members of the general public are about equally likely to
have artistic hobbies, but scientists inducted into the highest national
academies are much more likely to have avocations outside of their
vocation. And those who======================================================== similar.
“That’s usually exactly the wrong way to go about it regardless of what
you’re using analogy for,” Lovallo told me.
The good news is that it is easy to ride analogies from the intuitive inside
view to the outside view. In 2001, the Boston Consulting Group, one of the
most successful in the world, created an intranet site to provide consultants
with collections of material to facilitate wide-ranging analogical thinking.
The interactive “exhibits” were sorted by discipline (anthropology,
psychology, history, and others), concept (change, logistics, productivity,
and so on), and strategic theme (competition, cooperation, unions and
alliances, and more). A consultant generating strategies for a post-merger
integration might have perused the exhibit on how William the Conqueror
“merged” England with the Norman Kingdom in the eleventh century. An
exhibit that described Sherlock Holmes’s observational strategies could
have provided ideas for learning from details that experienced professionals
take for granted. And a consultant working with a rapidly expanding start-
up might have gleaned ideas from the writing of a Prussian military
strategist who studied the fragile equilibrium between maintaining
momentum after a victory and overshooting a goal by so much that it turns

into a defeat. If that all sounds incredibly remote from pressing business
concerns, that is exactly the point.
Dedre Gentner wanted to find out if everyone can be a bit more like Kepler,
capable of wielding distant analogies to understand problems. So she helped
create the “Ambiguous Sorting Task. ”
It consists of twenty-five cards, each one describing a real-world
phenomenon, like how internet routers or economic bubbles work. Each
card falls into two main categories, one for its domain (economics, biology,
and so on) and one for its deep structure. Participants are asked to sort the
cards into like categories.
For a deep structure example, you might put economic bubbles and
melting polar ice caps together as positive-feedback loops. (In economic
bubbles, consumers buy stocks or property with the idea that the price will
increase; that buying causes the price to increase, which leads to more
buying. When ice caps melt, they reflect less sunlight back to space, which
warms the planet, causing more ice to melt.) Or perhaps you would put the
act of sweating and actions of the Federal Reserve together as negative-
feedback loops. (Sweating cools the body so that more sweating is no
longer required. The Fed lowers interest rates to spur the economy; if the
economy grows too quickly, the Fed raises rates to slow down the activity it
launched.) The way gas prices lead to an increase in grocery prices and the
steps needed for a message to traverse neurons in your brain are both
examples of causal chains, where one event leads to another, which leads to
another, in linear order.
Alternatively, you might group Federal Reserve rate changes, economic
bubbles, and gas price changes together because they are all in the same
domain: economics. And you might put sweating and neurotransmission
together under biology.
Gentner and colleagues gave the Ambiguous Sorting Task to
Northwestern University students from an array of majors and found that all
of the students figured out how to group phenomena by domains. But fewer
could come up with groupings based on causal structure. There was a group
of students, however, who were particularly good at finding common deep
structures: students who had taken classes in a range of domains, like those
in the Integrated Science Program.

Northwestern’s website for the program features an alum’s description:
“Think of the Integrated Science Program as a biology minor, chemistry
minor, physics minor, and math minor combined into a single major. The
primary intent of this program is to expose students to all fields of the
natural and mathematical sciences so that they can see commonalities
among different fields of the natural sciences. . . . The ISP major allows you
to see connections across different disciplines.”
A professor I asked about the Integrated Science Program told me that
specific academic departments are generally not big fans. They want
students to take more specialized classes in a single department. They are
concerned about the students falling behind. They would rather rush them to
specialization than equip them with ideas from what Gentner referred to as
a “variety of base domains,” which foster analogical thinking and
conceptual connections that can help students categorize the type of
problem they are facing. That is precisely a skill that sets the most adept
problem solvers apart.
In one of the most cited studies of expert problem solving ever
conducted, an interdisciplinary team of scientists came to a pretty simple
conclusion: successful problem solvers are more able to determine the deep
structure of a problem before they proceed to match a strategy to it. Less
successful problem solvers are more like most students in the Ambiguous
Sorting Task: they mentally classify problems only by superficial, overtly
stated features, like the domain context. For the best performers, they wrote,
problem solving “begins with the typing of the problem.”
As education pioneer John Dewey put it in Logic, The Theory of Inquiry,
“a problem well put is half-solved.”
Before he began his tortuous march of analogies toward reimagining the
universe, Kepler had to get very confused on his homework. Unlike Galileo
and Isaac Newton, he documented his confusion. “What matters to me,”
Kepler wrote, “is not merely to impart to the reader what I have to say, but
above all to convey to him the reasons, subterfuges, and lucky hazards
which led me to my discoveries. ”
Kepler was a young man when he showed up to work at Tycho Brahe’s
observatory—so cutting edge at the time that it cost 1 percent of the
national budget of Denmark. He was given the assignment nobody wanted:

Mars and its perplexing orbit. The orbit had to be a circle, Kepler was told,
so he had to figure out why Brahe’s observations didn’t match that. Every
once in a while, Mars appears to reverse course in the sky, do a little loop,
and then carry on in the original direction, a feat known as retrograde
motion. Astronomers proposed elaborate contortions to explain how Mars
could accomplish this while riding the interlocking spheres of the sky.
As usual, Kepler could not accept contortions. He asked peers for help,
but his pleas fell on deaf ears. His predecessors had always managed to
explain away the Mars deviations without scrapping the overall scheme.
Kepler’s short Mars assignment (he guessed it would take eight days)
turned into five years of calculations trying to describe where Mars
appeared in the sky at any given moment. No sooner had Kepler done it
with great accuracy than he threw it away.
It was close, but not perfect. The imperfection was minuscule. Just two of
Brahe’s observations differed from Kepler’s calculations of where Mars
should be, and by just eight minutes of arc, a sliver of sky one-eighth the
width of a pinkie finger held at arm’s length. Kepler could have assumed
his model was correct and those two observations were slightly off, or he
could dispense with five years of work. He chose to trash his model. “If I
had believed we could ignore these eight minutes,” he wrote, “I would have
patched my hypothesis accordingly.” The assignment no one wanted
became Kepler’s keyhole view into a new understanding of the universe. He
was in uncharted territory. The analogies began in earnest, and he
reinvented astronomy. Light, heat, smells, boats, brooms, magnets—it
began with those pesky observations that didn’t quite fit, and ended in the
complete undoing of Aristotle’s clockwork universe.
Kepler did something that turns out to be characteristic of today’s world-
class research labs. Psychologist Kevin Dunbar began documenting how
productive labs work in the 1990s, and stumbled upon a modern version of
Keplerian thinking. Faced with an unexpected finding, rather than assuming
the current theory is correct and that an observation must be off, the
unexpected became an opportunity to venture somewhere new—and
analogies served as the wilderness guide.
When Dunbar started, he simply set out to document the process of
discovery in real time. He focused on molecular biology labs because they
were blazing new trails, particularly in genetics and treatments for viruses,
like HIV. He spent a year with four labs in the United States, playing a fly

on the wall, visiting the labs every day for months, and later extended the
work to more labs in the United States, Canada, and Italy. He became such
a familiar presence that scientists called him to make sure he knew about
impromptu meetings. The surface features of the labs were very different.
One had dozens of members, others were small. A few were all men, one
was all women. All had international reputations.
The weekly lab meetings made the most interesting viewing. Once a
week, the entire team came together—lab director, grad students,
postdoctoral fellows, technicians—to discuss some challenge a lab member
was facing. The meetings were nothing like the heads-down, solitary work
in stereotypical portrayals of scientists, huddled over their test tubes.
Dunbar saw free-flowing and spontaneous exchange. Ideas were batted
back and forth, new experiments proposed, obstacles discussed. “Those are
some of the most creative moments in science,” he told me. So he recorded
them.
The first fifteen minutes could be housekeeping—whose turn it was to
order supplies, or who had left a mess. Then the action started. Someone
presented an unexpected or confusing finding, their version of Kepler’s
Mars orbit. Prudently, scientists’ first instinct was to blame themselves,
some error in calculation or poorly calibrated equipment. If it kept up, the
lab accepted the result as real, and ideas about what to try and what might
be======================================================== in

cancer, but rather in cancer related to a single organ, and the trend advances
each year. Surgeon and writer Atul Gawande pointed out that when doctors
joke about left ear surgeons, “we have to check to be sure they don’t exist.”
In the ten-thousand-hours-themed bestseller Bounce , British journalist
Matthew Syed suggested that the British government was failing for a lack
of following the Tiger Woods path of unwavering specialization. Moving
high-ranking government officials between departments, he wrote, “is no
less absurd than rotating Tiger Woods from golf to baseball to football to
hockey.”
Except that Great Britain’s massive success at recent Summer Olympics,
after decades of middling performances, was bolstered by programs set up
specifically to recruit adults to try new sports and to create a pipeline for late
developers—“slow bakers,” as one of the officials behind the program
described them to me. Apparently the idea of an athlete, even one who wants
to become elite, following a Roger path and trying different sports is not so
absurd. Elite athletes at the peak of their abilities do spend more time on
focused, deliberate practice than their near-elite peers. But when scientists
examine the entire developmental path of athletes, from early childhood, it
looks like this:

Eventual elites typically devote less time early on to deliberate practice in
the activity in which they will eventually become experts. Instead, they
undergo what researchers call a “sampling period.” They play a variety of
sports, usually in an unstructured or lightly structured environment; they
gain a range of physical proficiencies from which they can draw; they learn
about their own abilities and proclivities; and only later do they focus in and
ramp up technical practice in one area. The title of one study of athletes in
individual sports proclaimed “Late Specialization” as “the Key to Success”;
another, “Making It to the Top in Team Sports: Start Later, Intensify, and Be
Determined.”

When I began to write about these studies, I was met with thoughtful
criticism, but also denial. “Maybe in some other sport,” fans often said, “but
that’s not true of our sport.” The community of the world’s most popular
sport, soccer, was the loudest. And then, as if on cue, in late 2014 a team of
German scientists published a study showing that members of their national
team, which had just won the World Cup, were typically late specializers
who didn’t play more organized soccer than amateur-league players until age
twenty-two or later. They spent more of their childhood and adolescence
playing nonorganized soccer and other sports. Another soccer study
published two years later matched players for skill at age eleven and tracked
them for two years. Those who participated in more sports and nonorganized
soccer, “but not more organized soccer practice/ training,” improved more
by age thirteen. Findings like these have now been echoed in a huge array of
sports, from hockey to volleyball.
The professed necessity of hyperspecialization forms the core of a vast,
successful, and sometimes well-meaning marketing machine, in sports and
beyond. In reality, the Roger path to sports stardom is far more prevalent
than the Tiger path, but those athletes’ stories are much more quietly told, if
they are told at all. Some of their names you know, but their backgrounds
you probably don’t.
I started writing this introduction right after the 2018 Super Bowl, in
which a quarterback who had been drafted into professional baseball before
football (Tom Brady), faced off against one who participated in football,
basketball, baseball, and karate and had chosen between college basketball
and football (Nick Foles). Later that very same month, Czech athlete Ester
Ledecká became the first woman ever to win gold in two different sports
(skiing and snowboarding) at the same Winter Olympics. When she was
younger, Ledecká participated in multiple sports (she still plays beach
volleyball and windsurfs), focused on school, and never rushed to be number
one in teenage competition categories. The Washington Post article the day
after her second gold proclaimed, “In an era of sports specialization,
Ledecká has been an evangelist for maintaining variety.” Just after her feat,
Ukrainian boxer Vasyl Lomachenko set a record for the fewest fights needed
to win world titles in three different weight classes. Lomachenko, who took
four years off boxing as a kid to learn traditional Ukrainian dance, reflected,
“I was doing so many different sports as a young boy—gymnastics,

basketball, football, tennis—and I think, ultimately, everything came
together with all those different kinds of sports to enhance my footwork.”
Prominent sports scientist Ross Tucker summed up research in the field
simply: “We know that early sampling is key, as is diversity.”
In 2014, I included some of the findings about late specialization in sports in
the afterword of my first book, The Sports Gene . The following year, I got
an invitation to talk about that research from an unlikely audience—not
athletes or coaches, but military veterans. In preparation, I perused scientific
journals for work on specialization and career-swerving outside of the sports
world. I was struck by what I found. One study showed that early career
specializers jumped out to an earnings lead after college, but that later
specializers made up for the head start by finding work that better fit their
skills and personalities. I found a raft of studies that showed how
technological inventors increased their creative impact by accumulating
experience in different domains, compared to peers who drilled more deeply
into one; they actually benefited by proactively sacrificing a modicum of
depth for breadth as their careers progressed. There was a nearly identical
finding in a study of artistic creators.
I also began to realize that some of the people whose work I deeply
admired from afar—from Duke Ellington (who shunned music lessons to
focus on drawing and baseball as a kid) to Maryam Mirzakhani (who
dreamed of becoming a novelist and instead became the first woman to win
math’s most famous prize, the Fields Medal)—seemed to have more Roger
than Tiger in their development stories. I delved further and encountered
remarkable individuals who succeeded not in spite of their range of
experiences and interests, but because of it: a CEO who took her first job
around the time her peers were getting ready to retire; an artist who cycled
through five careers before he discovered his vocation and changed the
world; an inventor who stuck to a self-made antispecialization philosophy
and turned a small company founded in the nineteenth century into one of
the most widely resonant names in the world today.
I had only dipped my toe into research on specialization in the wider
world of work, so in my talk to the small group of military veterans I mostly
stuck to sports. I touched on the other findings only briefly, but the audience
seized on it. All were late specializers or career changers, and as they filed

up one after another to introduce themselves after the talk, I could tell that
all were at least moderately concerned, and some were borderline ashamed
of it.
They had been brought together by the Pat Tillman Foundation, which, in
the spirit of the late NFL player who left a professional football career to
become an Army Ranger, provides scholarships to veterans, active-duty
military, and military spouses who are undergoing career changes or going
back to school. They were all scholarship recipients, former paratroopers and
translators who were becoming teachers, scientists, engineers, and
entrepreneurs. They brimmed with enthusiasm, but rippled with an
undercurrent of fear. Their LinkedIn profiles didn’t show the linear
progression toward a particular career they had been told employers wanted.
They were anxious starting grad school alongside younger (sometimes much
younger) students, or changing lanes later than their peers, all because they
had been busy accumulating inimitable life and leadership experiences.
Somehow, a unique advantage had morphed in their heads into a liability.
A few days after I spoke to the Tillman Foundation group, a former Navy
SEAL who came up after the talk emailed me: “We are all transitioning from
one career to another. Several of us got together after you had left and
discussed how relieved we were to have heard you speak.” I was slightly
bemused to find that a former Navy SEAL with an undergraduate degree in
history and geophysics pursuing graduate degrees in business and public
administration from Dartmouth and Harvard could feel behind. But like the
others in the room, he had been told, implicitly or explicitly, that changing
directions was dangerous.
The talk was greeted with so much enthusiasm that the foundation invited
me to give a keynote speech at the annual conference in 2016, and then to
small group gatherings in different cities. Before each occasion, I read more
studies and spoke with more researchers and found more evidence that it
takes time—and often forgoing a head start—to develop personal and
professional range, but it is worth it.
I dove into work showing that highly credentialed experts can become so
narrow-minded that they actually get worse with experience, even while
becoming more confident—a dangerous combination. And I was stunned
when cognitive psychologists I spoke with led me to an enormous and too
often ignored body of work demonstrating that learning itself is best done
slowly to accumulate lasting knowledge, even when that means performing

poorly on tests of immediate progress. That is, the most effective learning
looks inefficient; it looks like falling behind.
Starting something new in middle age might look that way too. Mark
Zuckerberg famously noted that “young people are just smarter.” And yet a
tech founder who is fifty years old is nearly twice as likely to start a
blockbuster======================================================== concept of Kahneman and Tversky, ref1
instinctive decisions, ref1
instructors, ref1
Integrated Science Program at Northwestern University, ref1 , ref2
Intelligence Advanced Research Projects Activity (IARPA), ref1
interdisciplinary approaches, ref1 , ref2 , ref3n , ref4 , ref5
“interleaving” (mixed practice), ref1
inventors
failures and breakthroughs of, ref1
impacts of specialist vs. generalist, ref1 , ref2 , ref3
See also innovation and innovators
IQ scores and the Flynn effect, ref1 , ref2
Israel Defense Forces, ref1
Jackson, Kirabo, ref1
Japan, ref1 , ref2n
jazz musicians, ref1 , ref2
Jentleson, Katherine, ref1
Jeopardy! ref1
Jobs, Steve, ref1
Jordan, Hillary, ref1
Junger, Sebastian, ref1
Kaggle competitions, ref1
Kahan, Dan, ref1 , ref2
Kahneman, Daniel
on experience-expertise link, ref1 , ref2
high school curriculum project of, ref1
“inside view” concept of, ref1
on repetitive domains, ref1
Kaphar, Titus, ref1
Kasparov, Garry, ref1 , ref2 , ref3 , ref4
Kepler, Johannes
analogies used by, ref1 , ref2 , ref3 , ref4
and Copernican model of planets, ref1
documentation of discovery process, ref1
and Mars orbit, ref1
and planetary motion, ref1
“kind” learning environments
about, ref1
domains that benefit from, ref1
and early specialization, ref1 , ref2
efficiency of specialization in, ref1 , ref2
pattern recognition in, ref1 , ref2 , ref3
Klein, Gary, ref1 , ref2 , ref3
Knight, Phil, ref1
Konnikova, Maria, ref1
Kornell, Nate, ref1 , ref2
Kpelle people in Liberia, ref1
Kranz, Gene, ref1 , ref2

Lakhani, Karim, ref1 , ref2
language and abstract thinking, ref1
lateral thinking, ref1
about, ref1
of Darwin, ref1
and Dyson’s birds/frogs analogy, ref1
and Game Boy by Nintendo, ref1
and impact of specialist vs. generalist inventors, ref1 , ref2
and Ouderkirk’s multilayer optical film, ref1
of polymaths, ref1
and Unusual (or Alternative) Uses Task, ref1
with withered technology, ref1 , ref2 , ref3
of Yokoi at Nintendo, ref1 , ref2
late starts/specialization
in art world, ref1
in athletics, ref1 , ref2 , ref3 , ref4
and changing employment, ref1
as integral to success, ref1
in literary world, ref1
of Malamud, ref1
and “match quality” in vocations, ref1 , ref2
of Tillman Scholars, ref1 , ref2
learning environments, ref1 , ref2 , ref3 , ref4 . See also “kind” learning environments
learning fast and slow, ref1
and blocked practice, ref1
and “desirable difficulties,” ref1 , ref2 , ref3
and distributed practice/“spacing,” ref1 , ref2
in early childhood education programs, ref1 , ref2n
efficiency in, ref1
and generation effect, ref1
and hint-giving practices, ref1 , ref2 , ref3
“making connections” vs. “using procedures” question types, ref1 , ref2 , ref3
and mistakes, ref1 , ref2n
and mixed practice/“interleaving,” ref1
and open/closed skills, ref1
and short/long term performance, ref1 , ref2 , ref3 , ref4
and testing/self-testing, ref1 , ref2 , ref3
Ledecká, Ester, ref1
Lee, Stan, ref1
Lefai, Etienne, ref1
Lemke, Leslie, ref1
Lesmes, Tony, ref1 , ref2 , ref3
Levitt, Steven, ref1 , ref2n
Lewis, Sarah, ref1
Liberia, Kpelle people of, ref1
Limb, Charles, ref1
LinkedIn, ref1
local search strategies, ref1
Logic, The Theory of Inquiry (Dewey), ref1
Lomachenko, Vasyl, ref1

Lopes-Schliep, Priscilla, ref1 , ref2
Lovallo, Dan, ref1 , ref2
Lucas, William, ref1
Luria, Alexander, ref1 , ref2 , ref3
Lyell, Charles, ref1
Ma, Yo-Yo, ref1
Mackie, Glen, ref1
Maclean, Norman, ref1 , ref2
“making connections” question type, ref1 , ref2 , ref3
Malamud, Ofer, ref1
Mann Gulch fire, ref1
Marcus, Gary, ref1
Mars, orbit of, ref1
marshmallow test, ref1 , ref2n
Marvel Comics, ref1
match quality
about, ref1
in Army officer training, ref1 , ref2 , ref3n , ref4
of early and late specializers, ref1 , ref2
and grit/persistence, ref1 , ref2
and personal changes, ref1 , ref2
process for, ref1
sampling as means of maximizing, ref1
and Smithies, ref1
and value of experimentation, ref1
and Van Gogh, ref1
and winding career paths of dark horses, ref1
math-skills acquisition, ref1
McDonald, Allan, ref1 , ref2 , ref3 , ref4 , ref5 , ref6
medical field, ref1 , ref2 , ref3 , ref4 , ref5 , ref6
Melero, Eduardo, ref1
Mellers, Barbara, ref1 , ref2 , ref3
memories, ref1 , ref2
Metcalfe, Janet, ref1
Michelangelo, ref1 , ref2
military servicemen, ref1
Millennial generation, ref1n
Miller, Robert A., ref1 , ref2
Miranda, Lin-Manuel, ref1
Mirzakhani, Maryam, ref1 , ref2
Mischel, Walter, ref1 , ref2
mistakes, benefits of, ref1 , ref2n
mixed practice (“interleaving”), ref1
Miyazaki, Hayao, ref1
modern world, exposure to, ref1
Monet, Claude, ref1
Moore, Joanne, ref1n
Moravec’s paradox, ref1
motor skills learning, correcting, ref1n

mountain climbers, Himalayan, ref1
movie industry forecasts, ref1
Mozart, Wolfgang, ref1 , ref2 , ref3
Mudbound (Jordan), ref1
Mulloy, Larry, ref1 , ref2 , ref3 , ref4 , ref5
multilayer optical film, ref1
Murakami, Haruki, ref1
The Musical Mind (Sloboda), ref1
music and musicians, ref1
backgrounds of legendary artists, ref1
and creativity, ref1
and deliberate practice, ref1 , ref2
early specialization of, ref1 , ref2 , ref3
figlie del coro of Venice, ref1
and improvisation, ref1 , ref2
and inability to read music, ref1 , ref2 , ref3 , ref4 , ref5 , ref6
and instrument selection, ref1 , ref2
jazz, ref1 , ref2
and lessons, ref1
and multi-instrument approach, ref1 , ref2 , ref3 , ref4
quitting, ref1
and role of practice, ref1 , ref2
and sampling period, ref1 , ref2
self-taught, ref1 , ref2 , ref3
and Suzuki Method, ref1
traits of exceptional musicians, ref1
Mutual of America insurance company, ref1
Naifeh, Steven, ref1 , ref2 , ref3 , ref4
The Name of the Wind (Rothfuss), ref1
Napoleon, ref1
NASA
and Challenger disaster, ref1 , ref2 , ref3 , ref4 , ref5 , ref6
and Columbia disaster, ref1
communication in, ref1 , ref2
culture of, ref1 , ref2 , ref3 , ref4 , ref5 , ref6
and Gravity Probe B project, ref1
and InnoCentive solutions, ref1
Nash, Steve, ref1
National Assessment of Educational Progress, ref1
National Spelling Bee, ref1 , ref2 , ref3
naturalistic decision making (NDM), ref1
nature vs. nurture, ref1
Netflix’s recommendation algorithm, ref1
Nicolas, Ashley, ref1 , ref2
Nike, ref1
Nintendo, ref1 , ref2
Nobel laureates, ref1
Nolan, Christopher ref1
Northwestern University, ref1 , ref2

Novoselov, Konstantin, ref1 , ref2
officer training, military, ref1 , ref2 , ref3
Ogas, Ogi, ref1 , ref2 , ref3 , ref4
Ohsumi, Yoshinori, ref1
Okada, Satoru, ref1
O’Neal, Shaquille, ref1
open/closed skills, ref1
open-ended problem solving, ref1
open-mindedness, active, ref1 , ref2
ospedali , ref1 , ref2
Ouderkirk, Andy, ref1 , ref2 , ref3 , ref4
outsider advantage, ref1
and Appert’s food preservation solutions, ref1
and Bingham, ref1 , ref2
in Eli Lilly’s chemistry problems, ref1
and Exxon Valdez oil tanker disaster, ref1
and InnoCentive solutions, ref1 , ref2 , ref3 , ref4
and interdisciplinary opportunities, ref1 , ref2
in Kaggle competitions, ref1
and Viles’s genetics sleuthing, ref1
overpopulation debate, ref1
Palomeras, Neus, ref1
parents and household rules, ref1 , ref2
Pasteur, Louis, ref1
patents, ref1 , ref2 , ref3
Patil, Shefali, ref1 , ref2
pattern repetition and recognition
in chess, ref1 , ref2 , ref3 , ref4
and chunking, ref1
and cognitive entrenchment, ref1 , ref2
computer aided, ref1
in firefighting, ref1 , ref2 , ref3 , ref4
and forecasting, ref1
in golf, ref1
and kind learning environments, ref1 , ref2 , ref3
of military commanders, ref1
of savants, ref1
Pat Tillman Foundation, ref1 , ref2 , ref3 , ref4
Paul, Les, ref1
Peele, Jordan, ref1
Pegau, Scott, ref1
Pelegrina della Pietà, ref1
The Perfect Storm (Junger), ref1
persistence. See grit
personality traits, ref1 , ref2n , ref3
Pfirman, Stephanie, ref1n
Pinker, Steven, ref1n
poker players, ref1
Polgar, Laszlo, ref1 , ref2 , ref3

Polgar sisters, ref1 , ref2 , ref3 , ref4 , ref5
Pollock, Jackson, ref1
polymaths, ref1 , ref2
population of planet Earth, ref1
practice. See deliberate practice ; repetitive practice
predictions and forecasters, ref1
and active open-mindedness, ref1 , ref2
and breadth of experience, ref1 , ref2
and counterarguments, ref1
flu predictions, ref1
and hedgehog/foxes model, ref1 , ref2 , ref3 , ref4 , ref5
and IARPA tournament, ref1
and importance of curiosity, ref1 , ref2
improving accuracy in, ref1
learning from results of, ref1
and narrow experts as resource, ref1
and overpopulation debate of Ehrlich and Simon, ref1
superforecasters, ref1 , ref2
Tetlock’s research on, ref1
premodern people, ref1 , ref2 , ref3
PricewaterhouseCoopers, ref1
primates, ref1
Prince, ref1
producers, ref1
professors, ref1
R3 Initiative (Rigor, Responsibility, Reproducibility), ref1
Ramón y Cajal, Santiago, ref1
Raven’s Progressive Matrices, ref1 , ref2 , ref3
Redberg, Rita F., ref1
Reinhardt, Django, ref1 , ref2
repetitive practice
and chunking, ref1 , ref2
and “closed” skills, ref1
and comic book creators, ref1
domains that benefit from, ref1
in domains without clearly repetitive patterns, ref1
and mixed practice/“interleaving,” ref1
in music training, ref1
and specialized teams, ref1
and value of struggle, ref1
See also pattern repetition and recognition
research and development (R&D), ref1
Rhimes, Shonda, ref1n
Rhoades, Quentin, ref1 , ref2
Rhoten, Diana, ref1n
Richland, Lindsey, ref1 , ref2 , ref3 , ref4
Richter, Sviatoslav, ref1
Roberts, Brent W., ref1 , ref2n
Rose, Todd, ref1 , ref2

Rothfuss, Patrick, ref1
Rousseau, Jean-Jacques, ref1
routines of specialists, ref1
Rowling, J. K., ref1
rules in households, ref1
Sabin, Paul, ref1
sampling period, ref1 , ref2 , ref3 , ref4
savants, ref1 , ref2
Schultz, Theodore, ref1
Schwartz, Barry, ref1
“Science, the Endless Frontier” (Bush), ref1
science and scientists
atypical knowledge combinations in, ref1
and commercial applications, ref1
critical thinking skills of, ref1 , ref2
hyperspecialization in, ref1 , ref2n
and interdisciplinary approaches, ref1 , ref2n , ref3 , ref4
with outside avocations, ref1
parallel trenches of, ref1 , ref2
rush to======================================================== became the youngest
grandmaster ever, male or female. When Susan was asked on television if
she wanted to win the world championship in the men’s or women’s
category, she cleverly responded that she wanted to win the “absolute
category.”
None of the sisters ultimately reached Laszlo’s highest goal of becoming
the overall world champion, but all were outstanding. In 1996, Susan
participated in the women’s world championship, and won. Sofia peaked at
the rank of international master, a level down from grandmaster. Judit went
furthest, climbing up to eighth in the overall world ranking in 2004.
Laszlo’s experiment had worked. It worked so well that in the early
1990s he suggested that if his early specialization approach were applied to
a thousand children, humanity could tackle problems like cancer and AIDS.
After all, chess was just an arbitrary medium for his universal point. Like
the Tiger Woods story, the Polgar story entered an endless pop culture loop
in articles, books, TV shows, and talks as an example of the life-hacking
power of an early start. An online course called “Bring Up Genius!”
advertises lessons in the Polgar method to “build up your own Genius Life
Plan.” The bestseller Talent Is Overrated used the Polgar sisters and Tiger
Woods as proof that a head start in deliberate practice is the key to success
in “virtually any activity that matters to you.”
The powerful lesson is that anything in the world can be conquered in the
same way. It relies on one very important, and very unspoken, assumption:
that chess and golf are representative examples of all the activities that
matter to you.

Just how much of the world, and how many of the things humans want to
learn and do, are really like chess and golf?
Psychologist Gary Klein is a pioneer of the “naturalistic decision
making” (NDM) model of expertise; NDM researchers observe expert
performers in their natural course of work to learn how they make high-
stakes decisions under time pressure. Klein has shown that experts in an
array of fields are remarkably similar to chess masters in that they
instinctively recognize familiar patterns.
When I asked Garry Kasparov, perhaps the greatest chess player in
history, to explain his decision process for a move, he told me, “I see a
move, a combination, almost instantly,” based on patterns he has seen
before. Kasparov said he would bet that grandmasters usually make the
move that springs to mind in the first few seconds of thought. Klein studied
firefighting commanders and estimated that around 80 percent of their
decisions are also made instinctively and in seconds. After years of
firefighting, they recognize repeating patterns in the behavior of flames and
of burning buildings on the verge of collapse. When he studied nonwartime
naval commanders who were trying to avoid disasters, like mistaking a
commercial flight for an enemy and shooting it down, he saw that they very
quickly discerned potential threats. Ninety-five percent of the time, the
commanders recognized a common pattern and chose a common course of
action that was the first to come to mind.
One of Klein’s colleagues, psychologist Daniel Kahneman, studied
human decision making from the “heuristics and biases” model of human
judgment. His findings could hardly have been more different from Klein’s.
When Kahneman probed the judgments of highly trained experts, he often
found that experience had not helped at all. Even worse, it frequently bred
confidence but not skill.
Kahneman included himself in that critique. He first began to doubt the
link between experience and expertise in 1955, as a young lieutenant in the
psychology unit of the Israel Defense Forces. One of his duties was to
assess officer candidates through tests adapted from the British army. In one
exercise, teams of eight had to get themselves and a length of telephone
pole over a six-foot wall without letting the pole touch the ground, and
without any of the soldiers or the pole touching the wall. 
*
 The difference in
individuals’ performances were so stark, with clear leaders, followers,
braggarts, and wimps naturally emerging under the stress of the task, that

Kahneman and his fellow evaluators grew confident they could analyze the
candidates’ leadership qualities and identify how they would perform in
officer training and in combat. They were completely mistaken. Every few
months, they had a “statistics day” where they got feedback on how
accurate their predictions had been. Every time, they learned they had done
barely better than blind guessing. Every time, they gained experience and
gave confident judgments. And every time, they did not improve.
Kahneman marveled at the “complete lack of connection between the
statistical information and the compelling experience of insight.” Around
that same time, an influential book on expert judgment was published that
Kahneman told me impressed him “enormously.” It was a wide-ranging
review of research that rocked psychology because it showed experience
simply did not create skill in a wide range of real-world scenarios, from
college administrators assessing student potential to psychiatrists predicting
patient performance to human resources professionals deciding who will
succeed in job training. In those domains, which involved human behavior
and where patterns did not clearly repeat, repetition did not cause learning.
Chess, golf, and firefighting are exceptions, not the rule.
The difference between what Klein and Kahneman documented in
experienced professionals comprised a profound conundrum: Do specialists
get better with experience, or not?
In 2009, Kahneman and Klein took the unusual step of coauthoring a
paper in which they laid out their views and sought common ground. And
they found it. Whether or not experience inevitably led to expertise, they
agreed, depended entirely on the domain in question. Narrow experience
made for better chess and poker players and firefighters, but not for better
predictors of financial or political trends, or of how employees or patients
would perform. The domains Klein studied, in which instinctive pattern
recognition worked powerfully, are what psychologist Robin Hogarth
termed “kind” learning environments. Patterns repeat over and over, and
feedback is extremely accurate and usually very rapid. In golf or chess, a
ball or piece is moved according to rules and within defined boundaries, a
consequence is quickly apparent, and similar challenges occur repeatedly.
Drive a golf ball, and it either goes too far or not far enough; it slices,
hooks, or flies straight. The player observes what happened, attempts to
correct the error, tries again, and repeats for years. That is the very
definition of deliberate practice, the type identified with both the ten-

thousand-hours rule and the rush to early specialization in technical
training. The learning environment is kind because a learner improves
simply by engaging in the activity and trying to do better. Kahneman was
focused on the flip side of kind learning environments; Hogarth called them
“wicked.”
In wicked domains, the rules of the game are often unclear or incomplete,
there may or may not be repetitive patterns and they may not be obvious,
and feedback is often delayed, inaccurate, or both.
In the most devilishly wicked learning environments, experience will
reinforce the exact wrong lessons. Hogarth noted a famous New York City
physician renowned for his skill as a diagnostician. The man’s particular
specialty was typhoid fever, and he examined patients for it by feeling
around their tongues with his hands. Again and again, his testing yielded a
positive diagnosis before the patient displayed a single symptom. And over
and over, his diagnosis turned out to be correct. As another physician later
pointed out, “He was a more productive carrier, using only his hands, than
Typhoid Mary.” Repetitive success, it turned out, taught him the worst
possible lesson. Few learning environments are that wicked, but it doesn’t
take much to throw experienced pros off course. Expert firefighters, when
faced with a new situation, like a fire in a skyscraper, can find themselves
suddenly deprived of the intuition formed in years of house fires, and prone
to poor decisions. With a change of the status quo, chess masters too can
find that the skill they took years to build is suddenly obsolete .
In a 1997 showdown billed as the final battle for supremacy between
natural and artificial intelligence, IBM supercomputer Deep Blue defeated
Garry Kasparov. Deep Blue evaluated two hundred million positions per
second. That is a tiny fraction of possible chess positions—the number of
possible game sequences is more than atoms in the observable universe—
but plenty enough to beat the best human. According to Kasparov, “Today
the free chess app on your mobile phone is stronger than me.” He is not
being rhetorical.
“Anything we can do, and we know how to do it, machines will do it
better,” he said at a recent lecture. “If we can codify it, and pass it to
computers, they will do it better.” Still, losing to Deep Blue gave him an
idea. In playing computers, he recognized what artificial intelligence

scholars call Moravec’s paradox: machines and humans frequently have
opposite strengths and weaknesses.
There is a saying that “chess is 99 percent tactics.” Tactics are short
combinations of moves that players use to get an immediate advantage on
the board. When players study all those patterns, they are mastering tactics.
Bigger-picture planning in chess—how to manage the little battles to win
the war—is called strategy. As Susan Polgar has written, “you can get a lot
further by being very good in tactics”—that is, knowing a lot of patterns
—“and have only a basic understanding of strategy.”
Thanks to their calculation power, computers are tactically flawless
compared to humans. Grandmasters predict the near future, but computers
do it better. What if, Kasparov wondered, computer======================================================== shoes of an opposing
attorney to predict how they will argue. Conceptual schemes are flexible,
able to arrange information and ideas for a wide variety of uses, and to

transfer knowledge between domains. Modern work demands knowledge
transfer: the ability to apply knowledge to new situations and different
domains. Our most fundamental thought processes have changed to
accommodate increasing complexity and the need to derive new patterns
rather than rely only on familiar ones. Our conceptual classification schemes
provide a scaffolding for connecting knowledge, making it accessible and
flexible.
Research on thousands of adults in six industrializing nations found that
exposure to modern work with self-directed problem solving and
nonrepetitive challenges was correlated with being “cognitively flexible.” As
Flynn makes sure to point out, this does not mean that brains now have more
inherent potential than a generation ago, but rather that utilitarian spectacles
have been swapped for spectacles through which the world is classified by
concepts. 
*
 Even recently, within some very traditional or orthodox religious
communities that have modernized but that still block women from engaging
in modern work, the Flynn effect has proceeded more slowly for women
than for men in the same community. Exposure to the modern world has
made us better adapted for complexity, and that has manifested as flexibility,
with profound implications for the breadth of our intellectual world.
In every cognitive direction, the minds of premodern citizens were
severely constrained by the concrete world before them. With cajoling, some
solved the following logic sequence: “Cotton grows well where it is hot and
dry. England is cold and damp. Can cotton grow there or not?” They had
direct experience growing cotton, so some of them could answer (tentatively
and when pushed) for a country they had never visited. The same exact
puzzle with different details stumped them: “In the Far North, where there is
snow, all bears are white. Novaya Zemlya is in the Far North and there is
always snow there. What colors are the bears there?” That time, no amount
of pushing could get the remote villagers to answer. They would respond
only with principles. “Your words can be answered only by someone who
was there,” one man said, even though he had never been to England but had
just answered the cotton question. But even a faint taste of modern work
began to change that. Given the white bear puzzle, Abdull, forty-five and
barely literate but chairman of a collective farm, would not give an answer
confidently, but he did exercise formal logic. “To go by your words,” he
said, “they should all be white.”

The transition completely transformed the villagers’ inner worlds. When
the scientists from Moscow asked the villagers what they would like to know
about them or the place they came from, the isolated farmers and herders
generally could not come up with a single question. “I haven’t seen what
people do in other cities,” one said, “so how can I ask?” Whereas those
engaged in collective farming were readily curious. “Well, you just spoke
about white bears,” said thirty-one-year-old Akhmetzhan, a collective
farmer. “I don’t understand where they come from.” He stopped for a
moment to ponder. “And then you mentioned America. Is it governed by us
or by some other power?” Nineteen-year-old Siddakh, who worked on a
collective farm and had studied in a school for two years, was brimming
with imaginative questions that probed self-improvement, from the personal
to the local and global: “Well, what could I do to make our kolkhozniks
[collective farmers] better people? How can we obtain bigger plants, or plant
ones which will grow to be like big trees? And then I’m interested in how
the world exists, where things come from, how the rich became rich and why
the poor are poor.”
Where the very thoughts of premodern villagers were circumscribed by
their direct experiences, modern minds are comparatively free. This is not to
say that one way of life is uniformly better than another. As Arab
historiographer Ibn Khaldun, considered a founder of sociology, pointed out
centuries ago, a city dweller traveling through the desert will be completely
dependent on a nomad to keep him alive. So long as they remain in the
desert, the nomad is a genius.
But it is certainly true that modern life requires range, making connections
across far-flung domains and ideas. Luria addressed this kind of
“categorical” thinking, which Flynn would later style as scientific spectacles.
“[It] is usually quite flexible,” Luria wrote. “Subjects readily shift from one
attribute to another and construct suitable categories. They classify objects
by substance (animals, flowers, tools), materials (wood, metal, glass), size
(large, small), and color (light, dark), or other property. The ability to move
freely, to shift from one category to another, is one of the chief
characteristics of ‘abstract thinking.’”
Flynn’s great disappointment is the degree to which society, and particularly
higher education, has responded to the broadening of the mind by pushing

specialization, rather than focusing early training on conceptual, transferable
knowledge.
Flynn conducted a study in which he compared the grade point averages
of seniors at one of America’s top state universities, from neuroscience to
English majors, to their performance on a test of critical thinking. The test
gauged students’ ability to apply fundamental abstract concepts from
economics, social and physical sciences, and logic to common, real-world
scenarios. Flynn was bemused to find that the correlation between the test of
broad conceptual thinking and GPA was about zero. In Flynn’s words, “the
traits that earn good grades at [the university] do not include critical ability
of any broad significance.” 
*
Each of twenty test questions gauged a form of conceptual thinking that
can be put to widespread use in the modern world. For test items that
required the kind of conceptual reasoning that can be gleaned with no formal
training—detecting circular logic, for example—the students did well. But in
terms of frameworks that can best put their conceptual reasoning skills to
use, they were horrible. Biology and English majors did poorly on
everything that was not directly related to their field. None of the majors,
including psychology, understood social science methods. Science students
learned the facts of their specific field without understanding how science
should work in order to draw true conclusions. Neuroscience majors did not
do particularly well on anything. Business majors performed very poorly
across the board, including in economics. Econ majors did the best overall.
Economics is a broad field by nature, and econ professors have been shown
to apply the reasoning principles they’ve learned to problems outside their
area. 
*
 Chemists, on the other hand, are extraordinarily bright, but in several
studies struggled to apply scientific reasoning to nonchemistry problems.
Students Flynn tested often mistook subtle value judgments for scientific
conclusions, and in a question that presented a tricky scenario and required
students not to mistake a correlation for evidence of causation, they
performed worse than random. Almost none of the students in any major
showed a consistent understanding of how to apply methods of evaluating
truth they had learned in their own discipline to other areas. In that way, the
students had something in common with Luria’s remote villagers—even the
science majors were typically unable to generalize research methods from
their own field to other fields. Flynn’s conclusion: “There is no sign that any

department attempts to develop [anything] other than narrow critical
competence.”
Flynn is now in his eighties. He has a full white beard, the wind-buffeted
cheeks of a lifelong runner, and piles of white curls that tuft and billow like
cumulus clouds around his head. His house on a hill in Dunedin looks out
over a gently rolling green farmscape.
When he recounts his own education at the University of Chicago, where
he was captain of the cross-country team, he raises his voice. “Even the best
universities aren’t developing critical intelligence,” he told me. “They aren’t
giving students the tools to analyze the modern world, except in their area of
specialization. Their education is too narrow.” He does not mean this in the
simple sense that every computer science major needs an art history class,
but rather that everyone needs habits of mind that allow them to dance across
disciplines.
Chicago has long prided itself on a core curriculum dedicated to
interdisciplinary critical thinking. The two-year core, according to the
university, “is intended as an introduction to the tools of inquiry used in
every discipline—science, mathematics, humanities, and social sciences.
The goal is not just to transfer knowledge, but to raise fundamental questions
and to become familiar with the powerful ideas that shape our society.” But
even at Chicago, Flynn argues, his education did not maximize the modern
potential for applying conceptual thinking across domains.
Professors, he told me, are just too eager to share their favorite facts
gleaned from years of acceleratingly narrow study. He has taught for fifty
years, from Cornell to Canterbury, and is quick to include himself in that
criticism. When he taught intro to moral and political philosophy, he
couldn’t resist the urge to impart his favorite minutiae from Plato, Aristotle,
Hobbes, Marx, and Nietzsche.
Flynn introduced broad concepts in class, but he is sure that he often
buried them in a mountain of other information specific to that class alone—
a bad habit he worked to overcome. The study he conducted at the state
university convinced him that college departments rush to develop students
in a narrow specialty area, while failing to sharpen the tools of thinking that
can serve them in======================================================== retention
bonuses—just cash payments to junior officers if they agreed to serve a few
more years. It cost taxpayers $500 million, and was a massive waste.
Officers who had planned to stay anyway took it, and those who already
planned to leave did not. The Army learned a hard lesson: the problem was
not a financial one; it was a matching one.
In the industrial era, or the “company man” era, as the monograph
authors called it, “firms were highly specialized,” with employees generally
tackling the same suite of challenges repeatedly. Both the culture of the
time—pensions were pervasive and job switching might be viewed as
disloyal—and specialization were barriers to worker mobility outside of the

company. Plus, there was little incentive for companies to recruit from
outside when employees regularly faced kind learning environments, the
type where repetitive experience alone leads to improvement. By the 1980s,
corporate culture was changing. The knowledge economy created
“overwhelming demand for . . . employees with talents for
conceptualization and knowledge creation.” Broad conceptual skills now
helped in an array of jobs, and suddenly control over career trajectory
shifted from the employer, who looked inward at a ladder of opportunity, to
the employee, who peered out at a vast web of possibility. In the private
sector, an efficient talent market rapidly emerged as workers shuffled
around in pursuit of match quality. While the world changed, the Army
stuck with the industrial-era ladder.
The West Point professors explained that the Army, like many
bureaucratic organizations, missed out on match quality markets. “There is
no talent matching market mechanism,” they wrote. When a junior officer
changed direction and left the Army, it did not signal a loss of drive. It
signaled that a strong drive for personal development had changed the
officer’s goals entirely. “I’ve yet to meet a classmate who left the Army and
regretted it,” said Ashley Nicolas, the former intelligence officer. She went
on to become a math teacher and then a lawyer. She added that all were
grateful for the experience, even though it didn’t become a lifelong career.
While the private sector adjusted to the burgeoning need for high match
quality, the Army just threw money at people. It has, though, begun to
subtly change. That most hierarchical of entities has found success
embracing match flexibility. The Officer Career Satisfaction Program was
designed so that scholarship-ROTC and West Point graduates can take more
control of their own career progression. In return for three additional years
of active service, the program increased the number of officers who can
choose a branch (infantry, intelligence, engineering, dental, finance,
veterinary, communication technology, and many more), or a geographic
post. Where dangling money for junior officers failed miserably, facilitating
match quality succeeded. In the first four years of the program, four
thousand cadets agreed to extend their service commitments in exchange
for choice. 
*
It is just a small step. When Defense Secretary Ash Carter visited West
Point in 2016 for student meetings, he was flooded with concerns from very
gritty cadets about rigid career paths that did not allow them to adjust to

their own development. Carter had pledged to drastically reshape the
Army’s “industrial era” personnel management from the strict “up-or-out”
model to one that allows officers a shot to improve their own match quality
as they grow.
When they were high school graduates, with few skills and little
exposure to a world of career options, West Point cadets might easily have
answered “Not like me at all” to the Grit Scale statement “I often set a goal
but later choose to pursue a different one.” A few years later, with more
knowledge of their skills and preferences, choosing to pursue a different
goal was no longer the gritless route; it was the smart one .
Intuitively, grit research appeals to me. In the nonscientific, colloquial use
of the word, I tend to think I have a lot of it. After running track and playing
football, basketball, and baseball at a large public high school—and I’m
only five foot six—I walked on to a Division I track team in college as an
800-meter runner.
I was not close to the worst 800 runner on my college team freshman
year; I was the worst, by a landslide. I was allowed to keep practicing with
the team because as long as you are not chosen for travel, it doesn’t cost
anybody anything, not even the pair of shoes the recruits got. When the
traveling team went to South Carolina to train over spring break, I stayed on
the eerily quiet campus rather than going home, to train without distraction.
I stuck with it for two miserable years of vomit-inducing workouts and ego-
bruising races, while blue-chip recruits quit and were replaced by others.
There were plenty of days (and weeks, and an entire month or three) when I
felt like I should probably quit. But I was learning about the kind of training
that worked for me, and I was improving. In my senior season, I cracked the
university’s all-time top ten list indoors, was twice All-East, and part of a
relay that set the university record. The only other guy in my class who held
a university record was my gritty roommate, the other walk-on. Nearly the
entire recruited class from our year quit. Hilariously, I was awarded the
Gustave A. Jaeger Memorial Prize for the athlete who “achieved significant
athletic success in the face of unusual challenge and difficulty”—my
“unusual challenge and difficulty” just being that I epically stunk at first.
After the presentation, the head coach, with whom I’d had little direct

conversation as a walk-on, shared that he had felt sorry for me watching
workouts my freshman year.
There’s nothing particularly special about that story—it exists on every
team. But I think it is indicative of my approach to work. Nonetheless, I
scored at the 50th percentile on the Grit Scale compared to American adults
at large. I racked up points for assessing myself as a very hard worker who
is not discouraged by setbacks, but I missed a lot of points for confessing
that “my interests change from year to year,” and, like so many West Point
graduates, I sometimes “set a goal but later choose to pursue a different
one.” When I was seventeen and positive that I was going to go to the U.S.
Air Force Academy to become a pilot and then an astronaut, I probably
would have self-assessed at the very top of the Grit Scale. I got all the way
to Chicago-area congressman Sidney Yates agreeing to provide a
nomination.
But I never did any of that. Instead, at the last minute I changed my mind
and went elsewhere to study political science. I took a single poli-sci class,
and ended up majoring in Earth and environmental sciences and minoring
in astronomy, certain I would become a scientist. I worked in labs during
and after college and realized that I was not the type of person who wanted
to spend my entire life learning one or two things new to the world, but
rather the type who wanted constantly to learn things new to me and share
them. I transitioned from science to journalism; my first job was as a
midnight-shift street reporter in New York City. (Nothing happy that’s
going in the New York Daily News happens between midnight and 10 a.m.)
Growing self-knowledge kept changing my goals and interests until I
landed in a career the very lifeblood of which is investigating broad
interests. When I later worked at Sports Illustrated , determined students
would ask me whether it was better to study journalism or English to work
at SI . I told them I had no clue, but that a statistics or biology course never
hurt anyone.
I don’t think I have become less passionate or resilient over time, nor do I
think that all those former West Point cadets who left the Army lost the
drive that got them there in the first place. It makes sense to me that grit
would be powerfully predictive for cadets trying to get through their
rigorous orientation, or for a sample of schoolchildren or spelling bee
contestants. Very young people often have their goals set for them, or at
least have a limited menu to choose from, and pursuing them with passion

and resilience is the main challenge. The same goes for 800 runners. One of
the compelling aspects of sports goals is how straightforward and easily
measurable they are. On the final weekend of the 2018 Winter Olympics,
Sasha Cohen, a 2006 silver medalist figure skater, wrote an advice column
to retiring athletes. “Olympic athletes need to understand that the rules for
life are different from the rules for sports,” she wrote. “Yes, striving to
accomplish a single overarching goal every day means you have grit,
determination and resilience. But the ability to pull yourself together
mentally and physically in competition is different from the new challenges
that await you. So after you retire, travel, write a poem, try to start your
own business, stay out a little too late, devote time to something that
doesn’t have a clear end goal.” In the wider world of work, finding a goal
with high match quality in the first place is the greater challenge, and
persistence for the sake of persistence can get in the way.
A recent international Gallup survey of more than two hundred thousand
workers in 150 countries reported that 85 percent were either “not engaged”
with their work or “actively disengaged.” In that condition, according to
Seth Godin, quitting takes a lot more guts than continuing to be carried
along like debris on an ocean wave. The trouble, Godin noted, is that
humans are bedeviled by the “sunk cost fallacy.” Having invested time or
money in something, we are loath to leave it, because that would mean we
had wasted our time or money, even though it is already gone. Writer,
psychology PhD, and professional poker player Maria Konnikova explained
in her book The Confidence Game how the sunk cost mindset is so======================================================== concept of Kahneman and Tversky, ref1
instinctive decisions, ref1
instructors, ref1
Integrated Science Program at Northwestern University, ref1 , ref2
Intelligence Advanced Research Projects Activity (IARPA), ref1
interdisciplinary approaches, ref1 , ref2 , ref3n , ref4 , ref5
“interleaving” (mixed practice), ref1
inventors
failures and breakthroughs of, ref1
impacts of specialist vs. generalist, ref1 , ref2 , ref3
See also innovation and innovators
IQ scores and the Flynn effect, ref1 , ref2
Israel Defense Forces, ref1
Jackson, Kirabo, ref1
Japan, ref1 , ref2n
jazz musicians, ref1 , ref2
Jentleson, Katherine, ref1
Jeopardy! ref1
Jobs, Steve, ref1
Jordan, Hillary, ref1
Junger, Sebastian, ref1
Kaggle competitions, ref1
Kahan, Dan, ref1 , ref2
Kahneman, Daniel
on experience-expertise link, ref1 , ref2
high school curriculum project of, ref1
“inside view” concept of, ref1
on repetitive domains, ref1
Kaphar, Titus, ref1
Kasparov, Garry, ref1 , ref2 , ref3 , ref4
Kepler, Johannes
analogies used by, ref1 , ref2 , ref3 , ref4
and Copernican model of planets, ref1
documentation of discovery process, ref1
and Mars orbit, ref1
and planetary motion, ref1
“kind” learning environments
about, ref1
domains that benefit from, ref1
and early specialization, ref1 , ref2
efficiency of specialization in, ref1 , ref2
pattern recognition in, ref1 , ref2 , ref3
Klein, Gary, ref1 , ref2 , ref3
Knight, Phil, ref1
Konnikova, Maria, ref1
Kornell, Nate, ref1 , ref2
Kpelle people in Liberia, ref1
Kranz, Gene, ref1 , ref2

Lakhani, Karim, ref1 , ref2
language and abstract thinking, ref1
lateral thinking, ref1
about, ref1
of Darwin, ref1
and Dyson’s birds/frogs analogy, ref1
and Game Boy by Nintendo, ref1
and impact of specialist vs. generalist inventors, ref1 , ref2
and Ouderkirk’s multilayer optical film, ref1
of polymaths, ref1
and Unusual (or Alternative) Uses Task, ref1
with withered technology, ref1 , ref2 , ref3
of Yokoi at Nintendo, ref1 , ref2
late starts/specialization
in art world, ref1
in athletics, ref1 , ref2 , ref3 , ref4
and changing employment, ref1
as integral to success, ref1
in literary world, ref1
of Malamud, ref1
and “match quality” in vocations, ref1 , ref2
of Tillman Scholars, ref1 , ref2
learning environments, ref1 , ref2 , ref3 , ref4 . See also “kind” learning environments
learning fast and slow, ref1
and blocked practice, ref1
and “desirable difficulties,” ref1 , ref2 , ref3
and distributed practice/“spacing,” ref1 , ref2
in early childhood education programs, ref1 , ref2n
efficiency in, ref1
and generation effect, ref1
and hint-giving practices, ref1 , ref2 , ref3
“making connections” vs. “using procedures” question types, ref1 , ref2 , ref3
and mistakes, ref1 , ref2n
and mixed practice/“interleaving,” ref1
and open/closed skills, ref1
and short/long term performance, ref1 , ref2 , ref3 , ref4
and testing/self-testing, ref1 , ref2 , ref3
Ledecká, Ester, ref1
Lee, Stan, ref1
Lefai, Etienne, ref1
Lemke, Leslie, ref1
Lesmes, Tony, ref1 , ref2 , ref3
Levitt, Steven, ref1 , ref2n
Lewis, Sarah, ref1
Liberia, Kpelle people of, ref1
Limb, Charles, ref1
LinkedIn, ref1
local search strategies, ref1
Logic, The Theory of Inquiry (Dewey), ref1
Lomachenko, Vasyl, ref1

Lopes-Schliep, Priscilla, ref1 , ref2
Lovallo, Dan, ref1 , ref2
Lucas, William, ref1
Luria, Alexander, ref1 , ref2 , ref3
Lyell, Charles, ref1
Ma, Yo-Yo, ref1
Mackie, Glen, ref1
Maclean, Norman, ref1 , ref2
“making connections” question type, ref1 , ref2 , ref3
Malamud, Ofer, ref1
Mann Gulch fire, ref1
Marcus, Gary, ref1
Mars, orbit of, ref1
marshmallow test, ref1 , ref2n
Marvel Comics, ref1
match quality
about, ref1
in Army officer training, ref1 , ref2 , ref3n , ref4
of early and late specializers, ref1 , ref2
and grit/persistence, ref1 , ref2
and personal changes, ref1 , ref2
process for, ref1
sampling as means of maximizing, ref1
and Smithies, ref1
and value of experimentation, ref1
and Van Gogh, ref1
and winding career paths of dark horses, ref1
math-skills acquisition, ref1
McDonald, Allan, ref1 , ref2 , ref3 , ref4 , ref5 , ref6
medical field, ref1 , ref2 , ref3 , ref4 , ref5 , ref6
Melero, Eduardo, ref1
Mellers, Barbara, ref1 , ref2 , ref3
memories, ref1 , ref2
Metcalfe, Janet, ref1
Michelangelo, ref1 , ref2
military servicemen, ref1
Millennial generation, ref1n
Miller, Robert A., ref1 , ref2
Miranda, Lin-Manuel, ref1
Mirzakhani, Maryam, ref1 , ref2
Mischel, Walter, ref1 , ref2
mistakes, benefits of, ref1 , ref2n
mixed practice (“interleaving”), ref1
Miyazaki, Hayao, ref1
modern world, exposure to, ref1
Monet, Claude, ref1
Moore, Joanne, ref1n
Moravec’s paradox, ref1
motor skills learning, correcting, ref1n

mountain climbers, Himalayan, ref1
movie industry forecasts, ref1
Mozart, Wolfgang, ref1 , ref2 , ref3
Mudbound (Jordan), ref1
Mulloy, Larry, ref1 , ref2 , ref3 , ref4 , ref5
multilayer optical film, ref1
Murakami, Haruki, ref1
The Musical Mind (Sloboda), ref1
music and musicians, ref1
backgrounds of legendary artists, ref1
and creativity, ref1
and deliberate practice, ref1 , ref2
early specialization of, ref1 , ref2 , ref3
figlie del coro of Venice, ref1
and improvisation, ref1 , ref2
and inability to read music, ref1 , ref2 , ref3 , ref4 , ref5 , ref6
and instrument selection, ref1 , ref2
jazz, ref1 , ref2
and lessons, ref1
and multi-instrument approach, ref1 , ref2 , ref3 , ref4
quitting, ref1
and role of practice, ref1 , ref2
and sampling period, ref1 , ref2
self-taught, ref1 , ref2 , ref3
and Suzuki Method, ref1
traits of exceptional musicians, ref1
Mutual of America insurance company, ref1
Naifeh, Steven, ref1 , ref2 , ref3 , ref4
The Name of the Wind (Rothfuss), ref1
Napoleon, ref1
NASA
and Challenger disaster, ref1 , ref2 , ref3 , ref4 , ref5 , ref6
and Columbia disaster, ref1
communication in, ref1 , ref2
culture of, ref1 , ref2 , ref3 , ref4 , ref5 , ref6
and Gravity Probe B project, ref1
and InnoCentive solutions, ref1
Nash, Steve, ref1
National Assessment of Educational Progress, ref1
National Spelling Bee, ref1 , ref2 , ref3
naturalistic decision making (NDM), ref1
nature vs. nurture, ref1
Netflix’s recommendation algorithm, ref1
Nicolas, Ashley, ref1 , ref2
Nike, ref1
Nintendo, ref1 , ref2
Nobel laureates, ref1
Nolan, Christopher ref1
Northwestern University, ref1 , ref2

Novoselov, Konstantin, ref1 , ref2
officer training, military, ref1 , ref2 , ref3
Ogas, Ogi, ref1 , ref2 , ref3 , ref4
Ohsumi, Yoshinori, ref1
Okada, Satoru, ref1
O’Neal, Shaquille, ref1
open/closed skills, ref1
open-ended problem solving, ref1
open-mindedness, active, ref1 , ref2
ospedali , ref1 , ref2
Ouderkirk, Andy, ref1 , ref2 , ref3 , ref4
outsider advantage, ref1
and Appert’s food preservation solutions, ref1
and Bingham, ref1 , ref2
in Eli Lilly’s chemistry problems, ref1
and Exxon Valdez oil tanker disaster, ref1
and InnoCentive solutions, ref1 , ref2 , ref3 , ref4
and interdisciplinary opportunities, ref1 , ref2
in Kaggle competitions, ref1
and Viles’s genetics sleuthing, ref1
overpopulation debate, ref1
Palomeras, Neus, ref1
parents and household rules, ref1 , ref2
Pasteur, Louis, ref1
patents, ref1 , ref2 , ref3
Patil, Shefali, ref1 , ref2
pattern repetition and recognition
in chess, ref1 , ref2 , ref3 , ref4
and chunking, ref1
and cognitive entrenchment, ref1 , ref2
computer aided, ref1
in firefighting, ref1 , ref2 , ref3 , ref4
and forecasting, ref1
in golf, ref1
and kind learning environments, ref1 , ref2 , ref3
of military commanders, ref1
of savants, ref1
Pat Tillman Foundation, ref1 , ref2 , ref3 , ref4
Paul, Les, ref1
Peele, Jordan, ref1
Pegau, Scott, ref1
Pelegrina della Pietà, ref1
The Perfect Storm (Junger), ref1
persistence. See grit
personality traits, ref1 , ref2n , ref3
Pfirman, Stephanie, ref1n
Pinker, Steven, ref1n
poker players, ref1
Polgar, Laszlo, ref1 , ref2 , ref3

Polgar sisters, ref1 , ref2 , ref3 , ref4 , ref5
Pollock, Jackson, ref1
polymaths, ref1 , ref2
population of planet Earth, ref1
practice. See deliberate practice ; repetitive practice
predictions and forecasters, ref1
and active open-mindedness, ref1 , ref2
and breadth of experience, ref1 , ref2
and counterarguments, ref1
flu predictions, ref1
and hedgehog/foxes model, ref1 , ref2 , ref3 , ref4 , ref5
and IARPA tournament, ref1
and importance of curiosity, ref1 , ref2
improving accuracy in, ref1
learning from results of, ref1
and narrow experts as resource, ref1
and overpopulation debate of Ehrlich and Simon, ref1
superforecasters, ref1 , ref2
Tetlock’s research on, ref1
premodern people, ref1 , ref2 , ref3
PricewaterhouseCoopers, ref1
primates, ref1
Prince, ref1
producers, ref1
professors, ref1
R3 Initiative (Rigor, Responsibility, Reproducibility), ref1
Ramón y Cajal, Santiago, ref1
Raven’s Progressive Matrices, ref1 , ref2 , ref3
Redberg, Rita F., ref1
Reinhardt, Django, ref1 , ref2
repetitive practice
and chunking, ref1 , ref2
and “closed” skills, ref1
and comic book creators, ref1
domains that benefit from, ref1
in domains without clearly repetitive patterns, ref1
and mixed practice/“interleaving,” ref1
in music training, ref1
and specialized teams, ref1
and value of struggle, ref1
See also pattern repetition and recognition
research and development (R&D), ref1
Rhimes, Shonda, ref1n
Rhoades, Quentin, ref1 , ref2
Rhoten, Diana, ref1n
Richland, Lindsey, ref1 , ref2 , ref3 , ref4
Richter, Sviatoslav, ref1
Roberts, Brent W., ref1 , ref2n
Rose, Todd, ref1 , ref2

Rothfuss, Patrick, ref1
Rousseau, Jean-Jacques, ref1
routines of specialists, ref1
Rowling, J. K., ref1
rules in households, ref1
Sabin, Paul, ref1
sampling period, ref1 , ref2 , ref3 , ref4
savants, ref1 , ref2
Schultz, Theodore, ref1
Schwartz, Barry, ref1
“Science, the Endless Frontier” (Bush), ref1
science and scientists
atypical knowledge combinations in, ref1
and commercial applications, ref1
critical thinking skills of, ref1 , ref2
hyperspecialization in, ref1 , ref2n
and interdisciplinary approaches, ref1 , ref2n , ref3 , ref4
with outside avocations, ref1
parallel trenches of, ref1 , ref2
rush to======================================================== “necessary nor healthy” for children to
be directed toward one career before they can make that decision
themselves—a decision that, again, took her years of adulthood. “So what
does grit look like early in life?” she wrote. “A young child who decides
today that she wants to become a doctor but thinks tomorrow that she’d
rather build houses. A teenager who decides, no, she won’t go out for track
this year and instead will see what it’s like to write for the school
newspaper.” Specialization has benefits, she added, “but before
specialization comes sampling, the exploration of possibilities that, really,
you cannot know anything about until you try them . . . Don’t confuse the
healthy development of a work ethic with the premature commitment to a
singular passion.”
As one researcher suggested to me: “When you get fit, it will look like
grit.” That is, if you help someone find a good fit, they are more likely to
display the characteristics of grit—like sticking with something—even if
they didn’t before.
Needless to say, most people aren’t going to be Tillman Scholars,
executives, or William Shakespeare. And while many of the stories in
Range portray uncommon achievements, I hoped those would serve as
memorable portals of engagement into research that applies to a much
broader swath of humanity.
In fact, international research that studied thousands of workers—more
than three-quarters of whom did not have tertiary education—
produced findings that resonate with a major theme of the book: that
sometimes the actions that provide a head start will undermine long-term
development , whether that is choosing a career or a course of study, or
simply developing a skill or learning new material.
A 2017 study published by four economists in the U.S., Germany, and
China, analyzed education and employment data in eleven countries with

large vocational education or apprenticeship programs, comparing people
within each country who had similar backgrounds—including test scores,
family background, and years of education—but differed in whether they
received career-focused or broader, general education. Naturally, there was
considerable variation between countries and certainly between individuals,
but the general pattern was: people who got narrow, career-focused
education were more likely to be employed right out of school and earned
more right away, but over time both advantages evaporated; decades later,
they had spent less overall time in the labor market and had lower lifetime
earnings. The early specializers often won in the short term, and lost in the
long run. Workers who received general education, the economists
concluded, were better positioned to adapt to change in a wicked world
where work next year might not look like work last year.
The pattern was particularly pronounced in two countries with famously
extensive apprenticeship programs—Denmark and Germany—an important
finding given that, over the last decade, U.S. politicians on both sides of the
aisle have advocated for a move toward the German apprenticeship model.
In 2017, President Trump issued an executive order to expand
apprenticeship programs to prepare workers for “today’s rapidly changing
economy”. The economists, on the other hand, concluded that the more
rapidly a nation’s economy is changing, the greater the long-term advantage
of general education. Of the three countries with widespread apprenticeship
programs—Denmark, Germany, Switzerland—early specialization only
resulted in a lifetime earnings advantage in Switzerland, which has had
easily the slowest growing economy of those three nations in recent
decades. “This comparison is consistent with the idea that those with
general education are more adaptable to changed economic demands,” two
of the economists wrote. “Vocational education has been promoted largely
as a way of improving the transition from schooling to work, but it also
appears to reduce the adaptability of workers to technological and structural
change in the economy.”
Does that mean we should have no early vocational training or
apprenticeships at all? I certainly don’t think so, and one of the economists
who did this work pointed out that apprenticeships still work well in
specific areas, like the building trades, but also that those trades are a small
portion of unfilled jobs. In my opinion, we should preserve a variety of
pathways, to fit a variety of life circumstances. But I also think we need to

be aware of how easy it is to be fooled by head starts, assuming that they
represent terminally stable trajectories, whether the head start be for child
athletes, college students learning math, or workers entering the labor force.
“The advantages of vocational training in smoothing entry into the labor
market,” the economists wrote, “have to be set against disadvantages later
in life, disadvantages that are likely to be more severe as we move more
into being a knowledge economy.”
The question of how broad or specialized to be is important to just about
everyone at some point or another, but usually only discussed with
intuition. Like any complex question that involves human beings, there is
no one-size-fits-all answer. My hope was to make discussions about that
crucial topic more interesting and productive.
I’m a science writer, so in doing that, I wanted to rely heavily on
scientific research that bears on the question from various angles. In talking
with readers after the hardcover publication, though, I’ve increasingly felt
that the stories of late starts or zig-zagging career paths have a specific
importance. The “availability heuristic” is a well-known cognitive bias in
which we tend to rely on the first example that comes to mind when making
a decision or judging an idea. Tiger stories have supplied millions of brains
with availability-heuristic ammo when they think of specialization. I hope
some of the stories in this book—from Federer and Van Gogh to Frances
Hesselbein (now 104, still working)—might stick in readers’ minds and
surface when the Tiger stories do, adding much needed balance to how we
consider the topic.
When I was allotted this space for an afterword to the paperback edition,
I first thought I should stuff it with research that I had to cut from the
hardcover due to space constraints. (My editor talked me off that cliff.)
Instead, I’d like to share one more memorable story.
When asked, Titus Kaphar reflexively says that nobody in his family went
to college. His mother didn’t go to college. His father went to prison, but
not to college. Neither his grandmother nor his grandfather went to college.
Nobody went to college. Except—he corrects himself—a distant cousin
went to college. Kaphar himself certainly wasn’t going to college. He often
didn’t even go to high school, hence the 0.65 GPA.

And yet, in his twenties, he decided to enroll in a few junior college
classes. Allow him to explain: he wanted to date a soon-to-be teacher; she
was four years older, contemplating grad school, and not impressed that he
had no plans for his future.
“So I just went over to the junior college,” Kaphar told me, “kind of as a
joke, not really taking it seriously, because, you know, I’m not an academic.
I’m not a person who does really well in school.” He picked a few classes
more or less at random. He came back and told the object of his affection.
They had a quick laugh about it.
One class was art history. Why? “I probably read the word ‘art’ and
thought, ‘art should be easy,’” Kaphar told me. In an unexpected way, it
was. Kaphar realized that he could remember details of paintings—not just
remember what he had seen, but associate the painting with the style of a
particular artist or artistic movement. “I remember one day we were talking
about Van Gogh,” he recalled, “and I remember seeing the image and being
very aware of where the painting sat in the history of art, where it sat in the
timeline the professor was trying to lay out for us.” He began to contribute
to class discussions. His confidence grew, and he got a B in the class.
“That was a new experience for me,” he told me, “a B overall in the class
at something that was academic. It made me go, ‘Hold up, wait a minute.’ I
realized this was, in fact, something that I was enjoying, and I could feel
myself wanting to push harder. When it became difficult, that grit became
more apparent.” He took more classes, and tested strategies until he found
something that worked, like dictating essays into a recorder for his first
draft, and studying for history tests by focusing on connecting the images in
a book to the surrounding material. Suddenly, college didn’t seem like such
a crazy idea.
Kaphar proceeded to San José State University to study fine arts. In an
art history survey course, to his dismay, the professor skipped over the
section on black painters. So Kaphar decided to compile his own syllabus
on the topic. “I think to some degree, it was like being a reporter doing an
investigation,” he said. A friend’s grandmother was a sculptor, and gave him
books. The first was on the Harlem Renaissance. “That sort of opened the
floodgates,” he recalled. He realized that the traditional artistic canon used
in class was not some magical pantheon, but just another syllabus
assembled by humans. He collected more books, and introduced new names
into class discussion.

Eventually, he took an actual painting class, but the teacher told the class
that he didn’t believe in painting anymore, so he gave no instruction
whatsoever on technique. Instead, Kaphar started looking over other
students’ shoulders and self-teaching. “It was like, last semester I painted an
apple, and it looked like a cannonball,” he told me. “This semester that
actually looks like an apple. Now, it’s loose, a child could do the same
thing, but it looks like an apple. Awesome, let’s keep going.” He began to
study paintings and then try to reverse-engineer them. When he noticed that
shadows in Velázquez paintings didn’t======================================================== shoes of an opposing
attorney to predict how they will argue. Conceptual schemes are flexible,
able to arrange information and ideas for a wide variety of uses, and to

transfer knowledge between domains. Modern work demands knowledge
transfer: the ability to apply knowledge to new situations and different
domains. Our most fundamental thought processes have changed to
accommodate increasing complexity and the need to derive new patterns
rather than rely only on familiar ones. Our conceptual classification schemes
provide a scaffolding for connecting knowledge, making it accessible and
flexible.
Research on thousands of adults in six industrializing nations found that
exposure to modern work with self-directed problem solving and
nonrepetitive challenges was correlated with being “cognitively flexible.” As
Flynn makes sure to point out, this does not mean that brains now have more
inherent potential than a generation ago, but rather that utilitarian spectacles
have been swapped for spectacles through which the world is classified by
concepts. 
*
 Even recently, within some very traditional or orthodox religious
communities that have modernized but that still block women from engaging
in modern work, the Flynn effect has proceeded more slowly for women
than for men in the same community. Exposure to the modern world has
made us better adapted for complexity, and that has manifested as flexibility,
with profound implications for the breadth of our intellectual world.
In every cognitive direction, the minds of premodern citizens were
severely constrained by the concrete world before them. With cajoling, some
solved the following logic sequence: “Cotton grows well where it is hot and
dry. England is cold and damp. Can cotton grow there or not?” They had
direct experience growing cotton, so some of them could answer (tentatively
and when pushed) for a country they had never visited. The same exact
puzzle with different details stumped them: “In the Far North, where there is
snow, all bears are white. Novaya Zemlya is in the Far North and there is
always snow there. What colors are the bears there?” That time, no amount
of pushing could get the remote villagers to answer. They would respond
only with principles. “Your words can be answered only by someone who
was there,” one man said, even though he had never been to England but had
just answered the cotton question. But even a faint taste of modern work
began to change that. Given the white bear puzzle, Abdull, forty-five and
barely literate but chairman of a collective farm, would not give an answer
confidently, but he did exercise formal logic. “To go by your words,” he
said, “they should all be white.”

The transition completely transformed the villagers’ inner worlds. When
the scientists from Moscow asked the villagers what they would like to know
about them or the place they came from, the isolated farmers and herders
generally could not come up with a single question. “I haven’t seen what
people do in other cities,” one said, “so how can I ask?” Whereas those
engaged in collective farming were readily curious. “Well, you just spoke
about white bears,” said thirty-one-year-old Akhmetzhan, a collective
farmer. “I don’t understand where they come from.” He stopped for a
moment to ponder. “And then you mentioned America. Is it governed by us
or by some other power?” Nineteen-year-old Siddakh, who worked on a
collective farm and had studied in a school for two years, was brimming
with imaginative questions that probed self-improvement, from the personal
to the local and global: “Well, what could I do to make our kolkhozniks
[collective farmers] better people? How can we obtain bigger plants, or plant
ones which will grow to be like big trees? And then I’m interested in how
the world exists, where things come from, how the rich became rich and why
the poor are poor.”
Where the very thoughts of premodern villagers were circumscribed by
their direct experiences, modern minds are comparatively free. This is not to
say that one way of life is uniformly better than another. As Arab
historiographer Ibn Khaldun, considered a founder of sociology, pointed out
centuries ago, a city dweller traveling through the desert will be completely
dependent on a nomad to keep him alive. So long as they remain in the
desert, the nomad is a genius.
But it is certainly true that modern life requires range, making connections
across far-flung domains and ideas. Luria addressed this kind of
“categorical” thinking, which Flynn would later style as scientific spectacles.
“[It] is usually quite flexible,” Luria wrote. “Subjects readily shift from one
attribute to another and construct suitable categories. They classify objects
by substance (animals, flowers, tools), materials (wood, metal, glass), size
(large, small), and color (light, dark), or other property. The ability to move
freely, to shift from one category to another, is one of the chief
characteristics of ‘abstract thinking.’”
Flynn’s great disappointment is the degree to which society, and particularly
higher education, has responded to the broadening of the mind by pushing

specialization, rather than focusing early training on conceptual, transferable
knowledge.
Flynn conducted a study in which he compared the grade point averages
of seniors at one of America’s top state universities, from neuroscience to
English majors, to their performance on a test of critical thinking. The test
gauged students’ ability to apply fundamental abstract concepts from
economics, social and physical sciences, and logic to common, real-world
scenarios. Flynn was bemused to find that the correlation between the test of
broad conceptual thinking and GPA was about zero. In Flynn’s words, “the
traits that earn good grades at [the university] do not include critical ability
of any broad significance.” 
*
Each of twenty test questions gauged a form of conceptual thinking that
can be put to widespread use in the modern world. For test items that
required the kind of conceptual reasoning that can be gleaned with no formal
training—detecting circular logic, for example—the students did well. But in
terms of frameworks that can best put their conceptual reasoning skills to
use, they were horrible. Biology and English majors did poorly on
everything that was not directly related to their field. None of the majors,
including psychology, understood social science methods. Science students
learned the facts of their specific field without understanding how science
should work in order to draw true conclusions. Neuroscience majors did not
do particularly well on anything. Business majors performed very poorly
across the board, including in economics. Econ majors did the best overall.
Economics is a broad field by nature, and econ professors have been shown
to apply the reasoning principles they’ve learned to problems outside their
area. 
*
 Chemists, on the other hand, are extraordinarily bright, but in several
studies struggled to apply scientific reasoning to nonchemistry problems.
Students Flynn tested often mistook subtle value judgments for scientific
conclusions, and in a question that presented a tricky scenario and required
students not to mistake a correlation for evidence of causation, they
performed worse than random. Almost none of the students in any major
showed a consistent understanding of how to apply methods of evaluating
truth they had learned in their own discipline to other areas. In that way, the
students had something in common with Luria’s remote villagers—even the
science majors were typically unable to generalize research methods from
their own field to other fields. Flynn’s conclusion: “There is no sign that any

department attempts to develop [anything] other than narrow critical
competence.”
Flynn is now in his eighties. He has a full white beard, the wind-buffeted
cheeks of a lifelong runner, and piles of white curls that tuft and billow like
cumulus clouds around his head. His house on a hill in Dunedin looks out
over a gently rolling green farmscape.
When he recounts his own education at the University of Chicago, where
he was captain of the cross-country team, he raises his voice. “Even the best
universities aren’t developing critical intelligence,” he told me. “They aren’t
giving students the tools to analyze the modern world, except in their area of
specialization. Their education is too narrow.” He does not mean this in the
simple sense that every computer science major needs an art history class,
but rather that everyone needs habits of mind that allow them to dance across
disciplines.
Chicago has long prided itself on a core curriculum dedicated to
interdisciplinary critical thinking. The two-year core, according to the
university, “is intended as an introduction to the tools of inquiry used in
every discipline—science, mathematics, humanities, and social sciences.
The goal is not just to transfer knowledge, but to raise fundamental questions
and to become familiar with the powerful ideas that shape our society.” But
even at Chicago, Flynn argues, his education did not maximize the modern
potential for applying conceptual thinking across domains.
Professors, he told me, are just too eager to share their favorite facts
gleaned from years of acceleratingly narrow study. He has taught for fifty
years, from Cornell to Canterbury, and is quick to include himself in that
criticism. When he taught intro to moral and political philosophy, he
couldn’t resist the urge to impart his favorite minutiae from Plato, Aristotle,
Hobbes, Marx, and Nietzsche.
Flynn introduced broad concepts in class, but he is sure that he often
buried them in a mountain of other information specific to that class alone—
a bad habit he worked to overcome. The study he conducted at the state
university convinced him that college departments rush to develop students
in a narrow specialty area, while failing to sharpen the tools of thinking that
can serve them in======================================================== tactical prowess were
combined with human big-picture, strategic thinking?
In 1998, he helped organize the first “advanced chess” tournament, in
which each human player, including Kasparov himself, paired with a
computer. Years of pattern study were obviated. The machine partner could
handle tactics so the human could focus on strategy. It was like Tiger
Woods facing off in a golf video game against the best gamers. His years of
repetition would be neutralized, and the contest would shift to one of
strategy rather than tactical execution. In chess, it changed the pecking
order instantly. “Human creativity was even more paramount under these
conditions, not less,” according to Kasparov. Kasparov settled for a 3–3
draw with a player he had trounced four games to zero just a month earlier
in a traditional match. “My advantage in calculating tactics had been
nullified by the machine.” The primary benefit of years of experience with
specialized training was outsourced, and in a contest where humans focused
on strategy, he suddenly had peers.
A few years later, the first “freestyle chess” tournament was held. Teams
could be made up of multiple humans and computers. The lifetime-of-
specialized-practice advantage that had been diluted in advanced chess was
obliterated in freestyle. A duo of amateur players with three normal
computers not only destroyed Hydra, the best chess supercomputer, they
also crushed teams of grandmasters using computers. Kasparov concluded
that the humans on the winning team were the best at “coaching” multiple
computers on what to examine, and then synthesizing that information for
an overall strategy. Human/Computer combo teams—known as
“centaurs”—were playing the highest level of chess ever seen. If Deep

Blue’s victory over Kasparov signaled the transfer of chess power from
humans to computers, the victory of centaurs over Hydra symbolized
something more interesting still: humans empowered to do what they do
best without the prerequisite of years of specialized pattern recognition.
In 2014, an Abu Dhabi–based chess site put up $20,000 in prize money
for freestyle players to compete in a tournament that also included games in
which chess programs played without human intervention. The winning
team comprised four people and several computers. The captain and
primary decision maker was Anson Williams, a British engineer with no
official chess rating. His teammate, Nelson Hernandez, told me, “What
people don’t understand is that freestyle involves an integrated set of skills
that in some cases have nothing to do with playing chess.” In traditional
chess, Williams was probably at the level of a decent amateur. But he was
well versed in computers and adept at integrating streaming information for
strategy decisions. As a teenager, he had been outstanding at the video game
Command & Conquer, known as a “real time strategy” game because
players move simultaneously. In freestyle chess, he had to consider advice
from teammates and various chess programs and then very quickly direct
the computers to examine particular possibilities in more depth. He was like
an executive with a team of mega-grandmaster tactical advisers, deciding
whose advice to probe more deeply and ultimately whose to heed. He
played each game cautiously, expecting a draw, but trying to set up
situations that could lull an opponent into a mistake.
In the end, Kasparov did figure out a way to beat the computer: by
outsourcing tactics, the part of human expertise that is most easily replaced,
the part that he and the Polgar prodigies spent years honing.
In 2007, National Geographic TV gave Susan Polgar a test. They sat her at
a sidewalk table in the middle of a leafy block of Manhattan’s Greenwich
Village, in front of a cleared chessboard. New Yorkers in jeans and fall
jackets went about their jaywalking business as a white truck bearing a
large diagram of a chessboard with twenty-eight pieces in midgame play
took a left turn onto Thompson Street, past the deli, and past Susan Polgar.
She glanced at the diagram as the truck drove by, and then perfectly re-
created it on the board in front of her. The show was reprising a series of

famous chess experiments that pulled back the curtain on kind-learning-
environment skills.
The first took place in the 1940s, when Dutch chess master and
psychologist Adriaan de Groot flashed midgame chessboards in front of
players of different ability levels, and then asked them to re-create the
boards as well as they could. A grandmaster repeatedly re-created the entire
board after seeing it for only three seconds. A master-level player managed
that half as often as the grandmaster. A lesser, city champion player and an
average club player were never able to re-create the board accurately. Just
like Susan Polgar, grandmasters seemed to have photographic memories.
After Susan succeeded in her first test, National Geographic TV turned
the truck around to show the other side, which had a diagram with pieces
placed at random. When Susan saw that side, even though there were fewer
pieces, she could barely re-create anything at all.
That test reenacted an experiment from 1973, in which two Carnegie
Mellon University psychologists, William G. Chase and soon-to-be Nobel
laureate Herbert A. Simon, repeated the De Groot exercise, but added a
wrinkle. This time, the chess players were also given boards with the pieces
in an arrangement that would never actually occur in a game. Suddenly, the
experts performed just like the lesser players. The grandmasters never had
photographic memories after all. Through repetitive study of game patterns,
they had learned to do what Chase and Simon called “chunking.” Rather
than struggling to remember the location of every individual pawn, bishop,
and rook, the brains of elite players grouped pieces into a smaller number of
meaningful chunks based on familiar patterns. Those patterns allow expert
players to immediately assess the situation based on experience, which is
why Garry Kasparov told me that grandmasters usually know their move
within seconds. For Susan Polgar, when the van drove by the first time, the
diagram was not twenty-eight items, but five different meaningful chunks
that indicated how the game was progressing.
Chunking helps explain instances of apparently miraculous, domain-
specific memory, from musicians playing long pieces by heart to
quarterbacks recognizing patterns of players in a split second and making a
decision to throw. The reason that elite athletes seem to have superhuman
reflexes is that they recognize patterns of ball or body movements that tell
them what’s coming before it happens. When tested outside of their sport
context, their superhuman reactions disappear.

We all rely on chunking every day in skills in which we are expert. Take
ten seconds and try to memorize as many of these twenty words as you can:
Because groups twenty patterns
meaningful are words easier into chunk remember
really sentence familiar can to you much in a.
Okay, now try again:
Twenty words are really much easier to
remember in a meaningful sentence because
you can chunk familiar patterns into groups.
Those are the same twenty pieces of information, but over the course of
your life, you’ve learned patterns of words that allow you to instantly make
sense of the second arrangement, and to remember it much more easily.
Your restaurant server doesn’t just happen to have a miraculous memory;
like musicians and quarterbacks, they’ve learned to group recurring
information into chunks.
Studying an enormous number of repetitive patterns is so important in
chess that early specialization in technical practice is critical. Psychologists
Fernand Gobet (an international master) and Guillermo Campitelli (coach to
future grandmasters) found that the chances of a competitive chess player
reaching international master status (a level down from grandmaster)
dropped from one in four to one in fifty-five if rigorous training had not
begun by age twelve. Chunking can seem like magic, but it comes from
extensive, repetitive practice. Laszlo Polgar was right to believe in it. His
daughters don’t even constitute the most extreme evidence.
For more than fifty years, psychiatrist Darold Treffert studied savants,
individuals with an insatiable drive to practice in one domain, and ability in
that area that far outstrips their abilities in other areas. “Islands of genius,”
Treffert calls it. 
*
 Treffert documented the almost unbelievable feats of
savants like pianist Leslie Lemke, who can play thousands of songs from
memory. Because Lemke and other savants have seemingly limitless
retrieval capacity, Treffert initially attributed their abilities to perfect
memories; they are human tape recorders. Except, when they are tested
after hearing a piece of music for the first time, musical savants reproduce
“tonal” music—the genre of nearly all pop and most classical music—more
easily than “atonal” music, in which successive notes do not follow familiar
harmonic structures. If savants were human tape recorders playing notes

back, it would make no difference whether they were asked to re-create
music that follows popular rules of composition or not. But in practice, it
makes an enormous difference. In one study of a savant pianist, the
researcher, who had heard the man play hundreds of songs flawlessly, was
dumbstruck when the savant could not re-create an atonal piece even after a
practice session with it. “What I heard seemed so unlikely that I felt obliged
to check that the keyboard had not somehow slipped into transposing
mode,” the researcher recorded. “But he really had made a mistake, and the
errors continued.” Patterns and familiar structures were critical to the
savant’s extraordinary recall ability. Similarly, when artistic savants are
briefly shown pictures and asked to reproduce them, they do much better
with images of real-life objects than with more abstract depictions.
It took Treffert======================================================== named Seymour, a
distinguished curriculum expert who had seen the process with other teams,
how this one compared.
Seymour thought for a while. Moments earlier, he had estimated it would
take about two more years. Faced with Kahneman’s question about other
teams, he said he had never even thought to compare this instance to
separate projects, but that about 40 percent of the teams he’d seen never
finished at all, and not a single one he could think of took less than seven
years.
Kahneman’s group was not willing to spend six more years on a
curriculum project that might fail. They spent a few minutes debating the
new opinion, and decided to forge ahead trusting the about-two-years
wisdom of the group. Eight years later, they finished, by which point
Kahneman was not even on the team or living in the country, and the
agency that asked for the curriculum was no longer interested.
Our natural inclination to take the inside view can be defeated by
following analogies to the “outside view.” The outside view probes for deep

structural similarities to the current problem in different ones. The outside
view is deeply counterintuitive because it requires a decision maker to
ignore unique surface features of the current project, on which they are the
expert, and instead look outside for structurally similar analogies. It
requires a mindset switch from narrow to broad.
For a unique 2012 experiment, University of Sydney business strategy
professor Dan Lovallo—who had conducted inside-view research with
Kahneman—and a pair of economists theorized that starting out by making
loads of diverse analogies, Kepler style, would naturally lead to the outside
view and improve decisions. They recruited investors from large private
equity firms who consider a huge number of potential projects in a variety
of domains. The researchers thought the investors’ work might naturally
lend itself to the outside view.
The private equity investors were told to assess a real project they were
currently working on with a detailed description of the steps to success, and
to predict the project’s return on investment. They were then asked to write
down a batch of other investment projects they knew of with broad
conceptual similarity to theirs—for instance, other examples of a business
owner looking to sell, or a start-up with a technologically risky product.
They were instructed to estimate the return for each of those examples too.
In the end, the investors estimated that the return on their own project
would be about 50 percent higher than the outside projects they had
identified as conceptually similar. When given the chance at the end to
rethink and revise, they slashed their own initial estimate. “They were sort
of shocked,” Lovallo told me, “and the senior people were the most
shocked.” The investors initially judged their own projects, where they
knew all the details, completely differently from similar projects to which
they were outsiders.
This is a widespread phenomenon. If you’re asked to predict whether a
particular horse will win a race or a particular politician will win an
election, the more internal details you learn about any particular scenario—
physical qualities of the specific horse, the background and strategy of the
particular politician—the more likely you are to say that the scenario you
are investigating will occur.
Psychologists have shown repeatedly that the more internal details an
individual can be made to consider, the more extreme their judgment
becomes. For the venture capitalists, they knew more details about their

own project, and judged that it would be an extreme success, until they
were forced to consider other projects with broad conceptual similarities. In
another example, students rated a university a lot better if they were told
about a few specific science departments that were ranked in the top ten
nationally than if they were simply told that every science department at the
university was ranked among the top ten. In one famous study, participants
judged an individual as more likely to die from “heart disease, cancer, or
other natural causes” than from “natural causes.” Focusing narrowly on
many fine details specific to a problem at hand feels like the exact right
thing to do, when it is often exactly wrong.
Bent Flyvbjerg, chair of Major Programme Management at Oxford
University’s business school, has shown that around 90 percent of major
infrastructure projects worldwide go over budget (by an average of 28
percent) in part because managers focus on the details of their project and
become overly optimistic. Project managers can become like Kahneman’s
curriculum-building team, which decided that thanks to its roster of experts
it would certainly not encounter the same delays as did other groups.
Flyvbjerg studied a project to build a tram system in Scotland, in which an
outside consulting team actually went through an analogy process akin to
what the private equity investors were instructed to do. They ignored
specifics of the project at hand and focused on others with structural
similarities. The consulting team saw that the project group had made a
rigorous analysis using all of the details of the work to be done. And yet,
using analogies to separate projects, the consulting team concluded that the
cost projection of £320 million (more than $400 million) was probably a
massive underestimate. When the tram opened three years late, it was
headed toward £1 billion. After that, other UK infrastructure projects began
implementing outside-view approaches, essentially forcing managers to
make analogies to many outside projects of the past.
Following their private-equity-investor experiment, the outside-view
researchers turned to the movie business, a notoriously uncertain realm with
high risk, high reward, and a huge store of data on actual outcomes. They
wondered if forcing analogical thinking on moviegoers could lead to
accurate forecasts of film success. They started by giving hundreds of
movie fans basic film information—lead actor names, the promotional
poster, and a synopsis—for an upcoming release. At the time, those
included Wedding Crashers, Fantastic Four, Deuce Bigalow: European

Gigolo, and others. The moviegoers were also given a list of forty older
movies, and asked to score how well each one probably served as an
analogy to each upcoming release. The researchers used those similarity
scores (and a little basic film information, like whether it was a sequel) to
predict the eventual revenue of the upcoming releases. They pitted those
predictions against a mathematical model stuffed with information about
seventeen hundred past movies and each upcoming film, including genre,
budget, star actors, release year, and whether it was a holiday release. Even
without all that detailed information, the revenue predictions that used
moviegoer analogy scores were vastly better. The moviegoer-analogies
forecast performed better on fifteen of nineteen upcoming releases. Using
the moviegoers’ analogies gave revenue projections that were less than 4
percent off for War of the Worlds, Bewitched, and Red Eye, and 1.7 percent
off for Deuce Bigalow: European Gigolo .
Netflix came to a similar conclusion for improving its recommendation
algorithm. Decoding movies’ traits to figure out what you like was very
complex and less accurate than simply analogizing you to many other
customers with similar viewing histories. Instead of predicting what you
might like, they examine who you are like, and the complexity is captured
therein.
Interestingly, if the researchers used only the single film that the movie
fans ranked as most analogous to the new release, predictive power
collapsed. What seemed like the single best analogy did not do well on its
own. Using a full “reference class” of analogies—the pillar of the outside
view—was immensely more accurate.
Think back to chapter 1 , to the types of intuitive experts that Gary Klein
studied in kind learning environments, like chess masters and firefighters.
Rather than beginning by generating options, they leap to a decision based
on pattern recognition of surface features. They may then evaluate it, if they
have time, but often stick with it. This time will probably be like the last
time, so extensive narrow experience works. Generating new ideas or
facing novel problems with high uncertainty is nothing like that. Evaluating
an array of options before letting intuition reign is a trick for the wicked
world.
In another experiment, Lovallo and his collaborator Ferdinand Dubin
asked 150 business students to generate strategies to help the fictitious
Mickey Company, which was struggling with its computer mouse business

in Australia and China. After business students learned about the company’s
challenges, they were told to write down all the strategies they could think
of to try to improve Mickey’s position.
Lovallo and Dubin gave some students one or more analogies in their
instructions. (For example: “The profile of Nike Inc. and McDonald’s Corp.
may be helpful to supplement your recommendations but should not limit
them.”) Other students got none. The students prompted with one analogy
came up with more strategies than those given no analogies, and students
given multiple analogies came up with more strategies than those reminded
only of one. And the more distant the analogy, the better it was for idea
generation. Students who were pointed to Nike and McDonald’s generated
more strategic options than their peers who were reminded of computer
companies Apple and Dell. Just being reminded to analogize widely made
the business students more creative. Unfortunately, students also said that if
they were to use analogy companies at all, they believed the best way to
generate strategic options would be to focus on a single example in the
same field. Like the venture capitalists, their intuition was to use too few
analogies, and to rely on those that were the most superficially======================================================== On Wednesday, he
performed so beautifully that the audience demanded an encore of all seven
movements. In 1998, alongside Sir Edmund Hillary, who with Tenzing
Norgay was the first to summit Mount Everest, Smith was awarded
Smithsonian’s Bicentennial Medal for outstanding cultural contributions.
Pianist Dave Brubeck earned the medal as well. His song “Take Five”
was chosen by NPR listeners as the quintessential jazz tune of all time.
Brubeck’s mother tried to teach him piano, but he refused to follow
instructions. He was born cross-eyed, and his childhood reluctance was
related to his inability to see the musical notation. His mother gave up, but
he listened when she taught others and tried to imitate. Brubeck still could
not read music when he dropped out of veterinary premed at the College of
the Pacific and walked across the lawn to the music department, but he was
a masterful faker. He put off studying piano for instruments that would
more easily allow him to improvise his way through exercises. Senior year,
he could hide no longer. “I got a wonderful piano teacher,” he recalled,
“who figured out I couldn’t read in about five minutes.” The dean informed
Brubeck that he could not graduate and furthermore was a disgrace to the
conservatory. Another teacher who had noticed his creativity stuck up for
him, and the dean cut a deal. Brubeck was allowed to graduate on the
condition that he promise never to embarrass the institution by teaching.

Twenty years later, the college apparently felt it had sufficiently escaped
embarrassment, and awarded him an honorary doctorate.
Perhaps the greatest improv master of all could not read, period—words
or music. Django Reinhardt was born in Belgium in 1910, in a Romani
caravan. His early childhood talents were chicken stealing and trout tickling
—feeling along a riverbank for fish and rubbing their bellies until they
relaxed and could be tossed ashore. Django grew up outside Paris in an area
called la Zone, where the city’s cesspool cleaners unloaded waste each
night. His mother, Négros, was too busy supporting the family making
bracelets out of spent artillery shell casings she gathered from a World War
I battlefield to lord over anyone’s music practice. Django went to school if
he felt like it, but he mostly didn’t. He crashed movie theaters and shot
billiards, and was surrounded by music. Wherever Romani gathered, there
were banjos, harps, pianos, and especially violins.
The violin’s portability made it the classic Romani instrument, and
Django started there, but he didn’t love it. He learned in the call-and-
response style. An adult would play a section of music and he would try to
copy it. When he was twelve, an acquaintance gave him a hybrid banjo-
guitar. He had found his thing, and became obsessed. He experimented with
different objects as picks when his fingers needed a break: spoons, sewing
thimbles, coins, a piece of whalebone. He teamed up with a banjo-playing
hunchback named Lagardère, and they wandered the Paris streets, busking
and improvising duets.
In his mid teens, Django was at a restaurant in Paris where the city’s
accordionists had gathered. He and his banjo-guitar were asked to the stage
to play for the other musicians. Django launched into a polka that was
known as a skill-proving piece for accordionists because it was so hard to
play. When he finished the traditional form, rather than stopping he
careened into a series of lightning improvisations, bending and twisting the
song into creations none of the veteran musicians had ever heard. Django
was playing “with a drawn knife,” as the lingo went. He was looking for a
fight by warping a sacred dancehall tune, but he was so original that he got
away with it. His creativity was unbound. “I wonder if, in his younger
days,” one of his music partners said, “he even knew that printed music
existed.” Django would soon need all the versatility he had learned.
He was eighteen when a candle in his wagon ignited a batch of celluloid
flowers that his wife, Bella, had fashioned for a funeral. The wagon

exploded into an inferno. Django was burned over half his body and ended
up bedridden for a year and a half. For the rest of his life the pinkie and ring
finger of his left hand, his fret hand, were dangling flesh, useless on the
strings. Django was used to improvising. Like Pelegrina of the figlie del
coro when she lost her teeth, he pivoted. He taught himself how to play
chords with a thumb and two fingers. His left hand had to sprint up and
down the neck of his guitar, the index and middle finger flitting waterbug-
like over the strings. He re-emerged with a new way of handling the
instrument, and his creativity erupted.
With a French violinist, Django fused dancehall musette with jazz and
invented a new form of improvisational music that defied easy
characterization, so it was just called “Gypsy jazz.” Some of his
spontaneous compositions became “standards,” pieces that enter the canon
from which other musicians improvise. He revolutionized the now-familiar
virtuosic guitar solo that pervaded the next generation’s music, from Jimi
Hendrix, who kept an album of Django’s recordings and named one of his
groups Band of Gypsys, to Prince ( self-taught, played more than a half-
dozen different genres of instruments on his debut album). Long before
Hendrix melted “The Star-Spangled Banner” into his own wondrous
creation, Django did it with the French national anthem, “La Marseillaise.”
Even though he never learned to read music (or words—a fellow
musician had to teach him to sign his autograph for fans), Django composed
a symphony, playing on his guitar what he wanted each instrument in the
ensemble to do while another musician struggled to transcribe it.
He died of a brain hemorrhage at forty-three, but music he made nearly a
century ago continues to show up in pop culture, including Hollywood
blockbusters like The Matrix and The Aviator, and in the hit BioShock video
games. The author of The Making of Jazz anointed the man who could
neither read music nor study it with the traditional fingerings “without
question, the single most important guitarist in the history of jazz.”
Cecchini has bushy eyebrows and a beard that parts and closes quickly like
ruffled shrubbery when he talks excitedly. Like now: he’s talking Django,
and he’s a huge fan. He used to have a black poodle named Django. He
opens a sepia-toned YouTube clip and whispers conspiratorially, “Watch
this.”

There is Django, bow tie, pencil mustache, and slicked-back hair. The
two useless fingers on his left hand are tucked into a claw. Suddenly, the
hand shoots all the way up the guitar neck, and then all the way back down,
firing a rapid succession of notes. “That’s amazing!” Cecchini says. “The
synchronization between the left and right hand is phenomenal.”
The strict deliberate practice school describes useful training as focused
consciously on error correction. But the most comprehensive examination
of development in improvisational forms, by Duke University professor
Paul Berliner, described the childhoods of professionals as “one of
osmosis,” not formal instruction. “Most explored the band room’s diverse
options as a prelude to selecting an instrument of specialization,” he wrote.
“It was not uncommon for youngsters to develop skills on a variety of
instruments.” Berliner added that aspiring improvisational musicians
“whose educational background has fostered a fundamental dependence on
[formal] teachers must adopt new approaches to learning.” A number of
musicians recounted
Brubeck-like scenarios to Berliner, the time a teacher found out that they
could not read music but had become adept enough at imitation and
improvisation that “they had simply pretended to follow the notation.”
Berliner relayed the advice of professional musicians to a young
improvisational learner as “not to think about playing—just play. ”
While I was sitting with Cecchini, he reeled off an impressive
improvisation. I asked him to repeat it so I could record it. “I couldn’t play
that again if you put a gun to my head,” he said. Charles Limb, a musician,
hearing specialist, and auditory surgeon at the University of California, San
Francisco, designed an iron-free keyboard so that jazz musicians could
improvise while inside an MRI scanner. Limb saw that brain areas
associated with focused attention, inhibition, and self-censoring turned
down when the musicians were creating. “It’s almost as if the brain turned
off its own ability to criticize itself,” he told National Geographic. While
improvising, musicians do pretty much the opposite of consciously
identifying errors and stopping to correct them.
Improv masters learn like babies: dive in and imitate and improvise first,
learn the formal rules later. “At the beginning, your mom didn’t give you a
book and say, ‘This is a noun, this is a pronoun, this is a dangling
participle,’” Cecchini told me. “You acquired the sound first. And then you
acquire the grammar later.”

Django Reinhardt was once in a taxi with Les Paul, inventor of the solid-
body electric guitar. Paul was a self-taught musician, and the only person in
both the Rock and Roll and National Inventors halls of fame. Reinhardt
tapped Paul on the shoulder and asked if he could read music. “I said no, I
didn’t,” Paul recounted, “and he laughed till he was crying and said, ‘Well, I
can’t read either. I don’t even know what a C is; I just play them.’”
Cecchini told me that he was regularly stunned when he would ask an
exceptional jazz performer onstage to play a certain note, and find the
musician could not understand him. “It’s an old joke among jazz
musicians,” Cecchini said. “You ask, ‘Can you read music?’ And the guy
says, ‘Not enough to hurt my playing.’” There is truth in the joke. Cecchini
has taught musicians who played professionally for the Chicago Symphony,
which in 2015 was rated as the top orchestra in the country and fifth in the
world by a panel of critics.======================================================== norms.
Engineers grew concerned about a technical problem they did not fully
understand, but they could not make a quantitative case. When they went to
the Department of Defense to request high-resolution photographs of a part
of the shuttle they thought was damaged, not only did NASA managers
block outside assistance, but they apologized to DoD for contact outside
“proper channels.” NASA administrators promised the violation of protocol
would not happen again. The Columbia Accident Investigation Board
concluded that NASA’s culture “emphasized chain of command, procedure,
following the rules, and going by the book. While rules and procedures
were essential for coordination, they had an unintended negative effect.”
Once again, “allegiance to hierarchy and procedure” had ended in disaster.
Again, lower ranking engineers had concerns they could not quantify; they
stayed silent because “the requirement for data was stringent and
inhibiting.”
The management and culture aspects of the Challenger and Columbia
disasters were so eerily similar that the investigation board decreed that
NASA was not functioning as “a learning organization.” In the absence of
cultural cross-pressures, NASA had failed to learn, just like the subjects in
Patil’s work who were placed in strongly congruent cultures.
There were, though, individuals in NASA who learned vital culture
lessons, and when the time came, put them to use.
In the spring of 2003, just two months after NASA lost the space shuttle
Columbia, it had to decide whether to scrap a high-profile project that had

been forty years and three-quarters of a billion dollars in the making.
Gravity Probe B was a technological marvel designed for a direct test of
Einstein’s general theory of relativity. It would be launched into space to
measure how Earth’s mass and rotation warped the fabric of space-time,
like a bowling ball twirling in a vat of honey. GP-B had the distinction of
being the longest-running project in the history of NASA. That was not a
compliment.
It was conceived one year after the founding of NASA itself. The launch
was delayed numerous times for technical problems, and the project was
nearly cancelled on three separate occasions. There were staff members at
NASA who no longer thought its mission was possible, and funding had to
be rescued repeatedly by a Stanford physicist with a knack for lobbying
Congress.
The technological challenges were immense. The probe required the
roundest objects ever manufactured—quartz gyroscope rotors the size of
ping-pong balls and so perfectly spherical that if you blew them up to the
size of Earth, the highest mountain peak would be eight feet tall. The
gyroscopes had to be cooled to −450°F by liquid helium, and the probe
required surgically delicate thrusters for precise maneuvering. The
technology took twenty years in development before it was ready for a test
flight.
Congressional eyes were on NASA. The agency could not afford to
launch the probe and have a high-profile failure right after Columbia . But
if the Gravity Probe B launch had to be delayed once more, it could be the
last time. “There was a huge amount of pressure to get this thing flown,”
Rex Geveden, the GP-B program manager, told me. Unfortunately,
engineers preparing for the prelaunch flight readiness review found a
problem.
The power supply to an electronics box was interfering with a critical
scientific instrument. Thankfully, the box only had to work at the beginning
of the mission, to get the gyroscopes spinning. It could then be turned off,
so it was not a catastrophic issue. But it was unexpected. If there were other
flaws that prevented the box from spinning up the gyroscopes to start the
experiment, the mission would be a total waste.
The giant Thermos-like container holding the gyroscopes had already
been filled with liquid helium, cooled, and sealed for launch. If the box
needed inspection, parts that had taken three months to install would have

to come off the probe; a launch delay would cost $ 10–$20 million. Some
engineers felt there was more risk in removing and potentially damaging
parts than in leaving it all alone. Stanford University was the prime
contractor, and the Stanford team leader “was confident that we could
succeed,” he said, “so I pushed hard that we should go ahead and fly.”
NASA’s chief engineer and head scientist for Gravity Probe B also both
pushed to launch. Plus, the probe had been moved to Vandenberg Air Force
Base in California for launch, and a delay would increase the chance of GP-
B sitting there when an earthquake struck. So: race, or don’t race?
The decision was in Geveden’s hands. “My God, I can’t even express
how stressful it was,” he told me. Even before the latest snafu, he had a
hunch held lightly—he was uneasy about how the electronics box had been
managed. But as long as the box was attached to the probe, there would be
no more information forthcoming.
Geveden joined NASA in 1990, and was a keen observer of the culture.
“When I was coming through NASA,” he said, “I had the intuition that
there’s a real conformance culture.” Early in his tenure, he attended a team-
building class offered by the agency. On the very first day the instructor
asked the class, rhetorically, for the single most important principle in
decision making. His answer: to get consensus. “And I said, ‘I don’t think
the people who launched the space shuttle Challenger agree with that
point,’” Geveden told me. “Consensus is nice to have, but we shouldn’t be
optimizing happiness, we should be optimizing our decisions. I just had a
feeling all along that there was something wrong with the culture. We didn’t
have a healthy tension in the system.” NASA still had its hallowed process,
and Geveden saw everywhere a collective culture that nudged conflict into
darkened corners. “You almost couldn’t go into a meeting without someone
saying, ‘Let’s take that offline,’” he recalled, just as Morton Thiokol had
done for the infamous offline caucus.
Geveden, in his own way, was in favor of balancing the typical, formal
process culture with a dose of informal individualism, as Kranz and von
Braun once had. “The chain of communication has to be informal,” he told
me, “completely different from the chain of command.” He wanted a
culture where everyone had the responsibility to protest if something didn’t
feel right. He decided to go prospecting for doubts.
He deeply respected Stanford’s electronics manager. The manager had
worked with the same kind of power supply before, and viewed it as fragile

technology. After a formal meeting in which NASA’s head engineer and its
head scientist on the project both advocated for leaving the box in place,
Geveden held informal individual meetings. In one of those, he learned
from a member of the NASA team that a manager from Lockheed Martin,
which had built the box, was concerned. Like Challenger ’s O-rings, the
known problem with the box was surmountable, but it was unexpected.
There were unknown unknowns.
Against the recommendation of the chief engineer and the Stanford team
leader, Geveden decided to scrub the launch and pull the box. Once it came
off, engineers quickly discovered three other design problems that had not
been clear in schematics, including a case of having used the flat-out wrong
parts. The surprises prompted Lockheed to go back over every single circuit
in the box. They found twenty separate issues.
As if Gravity Probe B was required by the space gods to scale every
imaginable obstacle, a month after the box was pulled there was an
earthquake near the launch site. The launch vehicle was slightly damaged,
but fortunately the probe was intact. Four months later, in April 2004, GP-B
finally took off. It was the first direct test to support Einstein’s idea that
Earth drags the fabric of space-time around with it as it spins. The
technology left a greater legacy. Components designed for Gravity Probe B
improved digital cameras and satellites; the centimeter-accurate GPS was
applied to automatic aircraft landing systems and precision farming.
The following year, a new NASA administrator was appointed by the
president. The new administrator demanded the kind of individualism and
opinionated debate that could serve as a cross-pressure for NASA’s robust
process accountability. He made Geveden the associate administrator,
essentially the COO of NASA, and the highest position in the agency that is
not politically appointed .
In 2017, Geveden took his lessons to a new role as CEO of BWX
Technologies, a company whose wide purview includes nuclear propulsion
technology that could power a manned Mars mission. Some of BWX
Technologies’ decision makers are retired military leaders whose dearly
held tool is firm hierarchy. So when Geveden became CEO, he wrote a
short memo on his expectations for teamwork. “I told them I expect
disagreement with my decisions at the time we’re trying to make decisions,

and that’s a sign of organizational health,” he told me. “After the decisions
are made, we want compliance and support, but we have permission to fight
a little bit about those things in a professional way.” He emphasized that
there is a difference between the chain of command and the chain of
communication, and that the difference represents a healthy cross-pressure.
“I warned them, I’m going to communicate with all levels of the
organization down to the shop floor, and you can’t feel suspicious or
paranoid about that,” he said. “I told them I will not intercept your decisions
that belong in your chain of command, but I will give and receive
information anywhere in the organization, at any time. I just can’t get
enough understanding of the organization from listening to the voices at the
top.”
His description reminded me of Girl Scouts CEO Frances Hesselbein’s
“circular management.” Instead of a ladder, the organizational structure was
concentric circles, with Hesselbein in the middle. Information could flow in
many directions,======================================================== pivot after
another, applying the lessons as he went.
Ogas uses the shorthand “standardization covenant” for the cultural
notion that it is rational to trade a winding path of self-exploration for a
rigid goal with a head start because it ensures stability. “The people we
study who are fulfilled do pursue a long-term goal, but they only formulate
it after a period of discovery,” he told me. “Obviously, there’s nothing
wrong with getting a law or medical degree or PhD. But it’s actually riskier
to make that commitment before you know how it fits you. And don’t
consider the path fixed. People realize things about themselves halfway
through medical school.” Charles Darwin, for example.

At his father’s behest he planned to be a doctor, but he found medical
lectures “intolerably dull,” and partway through his education he walked
out of an operation at the grind of the surgical saw. “Nor did I ever attend
again,” Darwin wrote, “for hardly any inducement would have been strong
enough to make me do so.” Darwin was a Bible literalist at the time, and
figured he would become a clergyman. He bounced around classes,
including a botany course with a professor who subsequently recommended
him for an unpaid position aboard the HMS Beagle . After convincing his
father (with his uncle’s help) that he would not become a deadbeat if he
took this one detour, Darwin began perhaps the most impactful post-college
gap year in history. His father’s wishes eventually “died a natural death.”
Decades later, Darwin reflected on the process of self-discovery. “It seems
ludicrous that I once intended to be a clergyman,” he wrote. His father, a
doctor for more than sixty years, detested the sight of blood. “If his father
had given him any choice,” Darwin wrote, “nothing should have induced
him to follow it.”
Michael Crichton started with medicine too, after learning how few
writers make a living. With medicine, “I would never have to wonder if the
work was worthwhile,” he wrote. Except, a few years in he became
disenchanted with medical practice. He graduated from Harvard Medical
School, but decided to become a writer. His medical education was not
remotely wasted. He used it to craft some of the most popular stories in the
world—the novel Jurassic Park, and the TV series ER, with its record-
setting 124 Emmy nominations.
Career goals that once felt safe and certain can appear ludicrous, to use
Darwin’s adjective, when examined in the light of more self-knowledge.
Our work preferences and our life preferences do not stay the same, because
we do not stay the same.
Psychologist Dan Gilbert called it the “end of history illusion.” From
teenagers to senior citizens, we recognize that our desires and motivations
sure changed a lot in the past (see: your old hairstyle), but believe they will
not change much in the future. In Gilbert’s terms, we are works in progress
claiming to be finished.
Gilbert and colleagues measured the preferences, values, and
personalities of more than nineteen thousand adults aged eighteen to sixty-

eight. Some were asked to predict how much they would change over the
next decade, others to reflect about how much they had changed in the
previous one. Predictors expected that they would change very little in the
next decade, while reflectors reported having changed a lot in the previous
one. Qualities that feel immutable changed immensely. Core values—
pleasure, security, success, and honesty—transformed. Preferences for
vacations, music, hobbies, and even friends were transfigured. Hilariously,
predictors were willing to pay an average of $129 a ticket for a show ten
years away by their current favorite band, while reflectors would only pay
$80 to see a show today by their favorite band from ten years ago. The
precise person you are now is fleeting, just like all the other people you’ve
been. That feels like the most unexpected result, but it is also the most well
documented.
It is definitely true that a shy child is more likely to foreshadow a shy
adult, but it is far from a perfect correlation. And if one particular
personality trait does not change, others will. The only certainty is change,
both on average as a generation ages, and within each individual. University
of Illinois psychologist Brent W. Roberts specializes in studying personality
development. He and another psychologist aggregated the results of ninety-
two studies and revealed that some personality traits change over time in
fairly predictable ways. Adults tend to become more agreeable, more
conscientious, more emotionally stable, and less neurotic with age, but less
open to experience. In middle age, adults grow more consistent and
cautious and less curious, open-minded, and inventive. 
*
 The changes have
well-known impacts, like the fact that adults generally become less likely to
commit violent crimes with age, and more able to create stable
relationships. The most momentous personality changes occur between age
eighteen and one’s late twenties, so specializing early is a task of predicting
match quality for a person who does not yet exist. It could work, but it
makes for worse odds. Plus, while personality change slows, it does not
stop at any age. Sometimes it can actually happen instantly.
Thanks to YouTube, the “marshmallow test” could be the most famous
scientific experiment in the world. It was actually a series of experiments
starting in the 1960s. The original premise was simple: An experimenter
places a marshmallow (or a cookie, or a pretzel) in front of a nursery school

child; before leaving, the experimenter tells the child that if she can wait
until the experimenter returns, she’ll get that marshmallow plus a second
one. If the child can’t wait, she can eat the marshmallow. The children were
not told how long the wait would be (it was fifteen to twenty minutes,
depending on age), so they just had to hold out if they wanted the maximum
reward.
Psychologist Walter Mischel and his research team followed up with the
children years later, and found that the longer a child had been able to wait,
the more likely she was to be successful socially, academically, and
financially, and the less likely she was to abuse drugs.
The marshmallow test was already a celebrity as scientific experiments
go, but it became the Beyoncé of studies when media outlets and parents
eager to foretell their child’s destiny started posting DIY marshmallow tests
online. The videos are by turns adorable and intriguing. Nearly all kids wait
at least a little. Some stare at the marshmallow, touch it, sniff it, delicately
tap their tongue to it and pull back as if it were hot. Maybe they even put it
in their mouth, pull it out, and simulate a big bite. Some tear a barely
noticeable piece off for a micro test taste. Before the end of the video, the
kids who start by touching it will have eaten the marshmallow. The kids
who successfully hold out wield all manner of distraction, from looking
away to shoving the plate away, covering their eyes, turning and screaming,
singing, talking to themselves, counting, generally thrashing around in the
chair, or (boys) hitting themselves in the face. One little boy who spent his
time looking in every direction except at the marshmallow is so ravenous
when the experimenter returns with his second treat that he mashes them
both into his mouth immediately.
The crystal ball allure of the marshmallow test is undeniable, and also
misconstrued. Mischel’s collaborator Yuichi Shoda has repeatedly made a
point of saying that plenty of preschoolers who ate the marshmallow turned
out just fine. 
*
 Shoda maintained that the most exciting aspect of the studies
was demonstrating how easily children could learn to change a specific
behavior with simple mental strategies, like thinking about the
marshmallow as a cloud rather than food. Shoda’s post-marshmallow-test
work has been one part of a bridge in psychology between extreme
arguments in the debate about the roles of nature and nurture in personality.
One extreme suggests that personality traits are almost entirely a function of
one’s nature, and the other that personality is entirely a function of the

environment. Shoda argued that both sides of the so-called person-situation
debate were right. And wrong. At a given point in life, an individual’s
nature influences how they respond to a particular situation, but their nature
can appear surprisingly different in some other situation. With Mischel, he
began to study “if-then signatures.” If David is at a giant party, then he
seems introverted, but if David is with his team at work, then he seems
extroverted. (True.) So is David introverted or extroverted? Well, both, and
consistently so.
Ogas and Rose call this the “context principle.” In 2007, Mischel wrote,
“The gist of such findings is that the child who is aggressive at home may
be less aggressive than most when in school; the man exceptionally hostile
when rejected in love may be unusually tolerant about criticism of his work;
the one who melts with anxiety in the doctor’s office may be a calm
mountain climber; the risk-taking entrepreneur may take few social risks.”
Rose framed it more colloquially: “If you are conscientious and neurotic
while driving today, it’s a pretty safe bet you will be conscientious and
neurotic while driving tomorrow. At the same time . . . you may not be
conscientious and neurotic when you are playing Beatles cover songs with
your band in the context of the local pub.” Perhaps that is one reason Daniel
Kahneman and his colleagues in the military (chapter 1 ) failed to predict
who would be a leader in battle based on who had been a leader in an
obstacle course exercise. When I was a college runner, I had teammates
whose drive and determination seemed almost boundless on the track, and
nearly absent in the classroom, and vice versa. Instead of asking whether
someone is gritty, we should ask when they are. “If you get someone into a
context that suits them,” Ogas said,======================================================== have won the Nobel Prize are more likely still.
Compared to other scientists, Nobel laureates are at least twenty-two times
more likely to partake as an amateur actor, dancer, magician, or other type
of performer. Nationally recognized scientists are much more likely than
other scientists to be musicians, sculptors, painters, printmakers,
woodworkers, mechanics, electronics tinkerers, glassblowers, poets, or
writers, of both fiction and nonfiction. And, again, Nobel laureates are far
more likely still. The most successful experts also belong to the wider
world. “To him who observes them from afar,” said Spanish Nobel laureate
Santiago Ramón y Cajal, the father of modern neuroscience, “it appears as
though they are scattering and dissipating their energies, while in reality
they are channeling and strengthening them.” The main conclusion of work
that took years of studying scientists and engineers, all of whom were
regarded by peers as true technical experts, was that those who did not
make a creative contribution to their field lacked aesthetic interests outside
their narrow area. As psychologist and prominent creativity researcher Dean
Keith Simonton observed, “rather than obsessively focus[ing] on a narrow
topic,” creative achievers tend to have broad interests. “This breadth often
supports insights that cannot be attributed to domain-specific expertise
alone.”
Those findings are reminiscent of a speech Steve Jobs gave, in which he
famously recounted the importance of a calligraphy class to his design
aesthetics. “When we were designing the first Macintosh computer, it all
came back to me,” he said. “If I had never dropped in on that single course
in college, the Mac would have never had multiple typefaces or
proportionally spaced fonts.” Or electrical engineer Claude Shannon, who

launched the Information Age thanks to a philosophy course he took to
fulfill a requirement at the University of Michigan. In it, he was exposed to
the work of self-taught nineteenth-century English logician George Boole,
who assigned a value of 1 to true statements and 0 to false statements and
showed that logic problems could be solved like math equations. It resulted
in absolutely nothing of practical importance until seventy years after Boole
passed away, when Shannon did a summer internship at AT& T’s Bell Labs
research facility. There he recognized that he could combine telephone call-
routing technology with Boole’s logic system to encode and transmit any
type of information electronically. It was the fundamental insight on which
computers rely. “It just happened that no one else was familiar with both
those fields at the same time,” Shannon said.
In 1979, Christopher Connolly cofounded a psychology consultancy in
the United Kingdom to help high achievers (initially athletes, but then
others) perform at their best. Over the years, Connolly became curious
about why some professionals floundered outside a narrow expertise, while
others were remarkably adept at expanding their careers—moving from
playing in a world-class orchestra, for example, to running one. Thirty years
after he started, Connolly returned to school to do a PhD investigating that
very question, under Fernand Gobet, the psychologist and chess
international master. Connolly’s primary finding was that early in their
careers, those who later made successful transitions had broader training
and kept multiple “career streams” open even as they pursued a primary
specialty. They “traveled on an eight-lane highway,” he wrote, rather than
down a single-lane one-way street. They had range. The successful adapters
were excellent at taking knowledge from one pursuit and applying it
creatively to another, and at avoiding cognitive entrenchment. They
employed what Hogarth called a “circuit breaker.” They drew on outside
experiences and analogies to interrupt their inclination toward a previous
solution that may no longer work. Their skill was in avoiding the same old
patterns. In the wicked world, with ill-defined challenges and few rigid
rules, range can be a life hack .
Pretending the world is like golf and chess is comforting. It makes for a
tidy kind-world message, and some very compelling books. The rest of this
one will begin where those end—in a place where the popular sport is
Martian tennis, with a view into how the modern world became so wicked
in the first place.

OceanofPDF.com

CHAPTER 2
How the Wicked World Was Made
THE TOWN OF DUNEDIN sits at the base of a hilly peninsula that juts off of
New Zealand’s South Island into the South Pacific. The peninsula is famous
for yellow-eyed penguins, and Dunedin boasts, demurely, the world’s
steepest residential street. It also features the University of Otago, the oldest
university in New Zealand, and home to James Flynn, a professor of political
studies who changed how psychologists think about thinking.
He started in 1981, intrigued by a thirty-year-old paper that reported IQ
test scores of American soldiers in World Wars I and II. The World War II
soldiers had performed better, by a lot. A World War I soldier who scored
smack in the middle of his peers—the 50th percentile—would have made
only the 22nd percentile compared to soldiers in World War II. Flynn
wondered if perhaps civilians had experienced a similar improvement. “I
thought, if IQ gains had occurred anywhere,” he told me, “maybe they had
occurred everywhere.” If he was right, psychologists had been missing
something big right before their eyes.
Flynn wrote to researchers in other countries asking for data, and on a dull
November Saturday in 1984, he found a letter in his university mailbox. It
was from a Dutch researcher, and it contained years of raw data from IQ
tests given to young men in the Netherlands. The data were from a test
known as Raven’s Progressive Matrices, designed to gauge the test taker’s
ability to make sense of complexity. Each question of the test shows a set of
abstract designs with one design missing. The test taker must try to fill in the
missing design to complete a pattern. Raven’s was conceived to be the
epitome of a “culturally reduced” test; performance should be unaffected by
material learned in life, inside or outside of school. Should Martians alight

on Earth, Raven’s should be the test capable of determining how bright they
are. And yet Flynn could immediately see that young Dutchmen had made
enormous gains from one generation to the next.
Flynn found more clues in test reference manuals. IQ tests are all
standardized so that the average score is always 100 points. (They are graded
based on a curve, with 100 in the middle.) Flynn noticed that the tests had to
be restandardized from time to time to keep the average at 100, because test
takers were giving more correct answers than they had in the past. In the
twelve months after he received the Dutch letter, Flynn collected data from
fourteen countries. Every single one showed huge gains for both children
and adults. “Our advantage over our ancestors,” as he put it, is “from the
cradle to the grave.”
Flynn had asked the right question. Score gains had occurred everywhere.
Other academics had stumbled upon pieces of the same data earlier, but none
had investigated whether it was part of a global pattern, even those who were
having to tweak the test scoring system to keep the average at 100. “As an
outsider,” Flynn told me, “things strike me as surprising that I think people
trained in psychometrics just accepted.”
The Flynn effect—the increase in correct IQ test answers with each new
generation in the twentieth century—has now been documented in more than
thirty countries. The gains are startling: three points every ten years. To put
that in perspective, if an adult who scored average today were compared to
adults a century ago, she would be in the 98th percentile.
When Flynn published his revelation in 1987, it hit the community of
researchers who study cognitive ability like a firebomb. The American
Psychological Association convened an entire meeting on the issue, and
psychologists invested in the immutable nature of IQ test scores offered an
array of explanations to usher the effect away, from more education and
better nutrition—which presumably contributed—to test-taking experience,
but none fit the unusual pattern of score improvements. On tests that gauged
material picked up in school or with independent reading or study—general
knowledge, arithmetic, vocabulary—scores hardly budged. Meanwhile,
performance on more abstract tasks that are never formally taught, like the
Raven’s matrices, or “similarities” tests, which require a description of how
two things are alike, skyrocketed.

A young person today asked to give similarities between “dusk” and
“dawn” might immediately realize that both connote times of day. But they
would be far more likely than their grandmothers to produce a higher-level
similarity: both separate day from night. A child today who scores average
on similarities would be in the 94th percentile of her grandparents’
generation. When a group of Estonian researchers used national test scores
to compare word understandings of schoolkids in the 1930s to those in 2006,
they saw that improvement came very specifically on the most abstract
words. The more abstract the word, the bigger the improvement. The kids
barely bested their grandparents on words for directly observable objects or
phenomena (“hen,” “eating,” “illness”), but they improved massively on
imperceptible concepts (“law,” “pledge,” “citizen”).
The gains around the world on Raven’s Progressive Matrices—where
change was least expected—were the biggest of all. “The huge Raven’s
gains show that today’s children are far better at solving problems on the
spot without a previously learned method for doing so,” Flynn concluded.
They are more able to extract rules and patterns where none are given. Even
in countries that have recently had a decrease in verbal and math IQ test
scores, Raven’s scores went======================================================== in cartoons when a robot’s belly opens
up and a boxing glove fires out. He stuck a gripping tool on the outer end
that closed when he squeezed handles to extend the arm. Now he could
lazily retrieve distant objects .
The company president saw the new hire goofing around with his
contraption and called him into his office. “I thought I would be scolded,”
Yokoi recalled. Instead, the desperate executive told Yokoi to turn his
device into a game. Yokoi added a group of colored balls that could be
grabbed, and the “Ultra Hand” went to market immediately. It was
Nintendo’s first toy, and it sold 1.2 million units. The company paid off a
chunk of its debt. That was the end of Yokoi’s maintenance career. The
president assigned him to start Nintendo’s first research and development
department. The facility that briefly made instant rice was converted into a
toy factory.
More toy success followed, but it was an abject failure that first year that
profoundly influenced Yokoi. He helped create Drive Game, a tabletop unit
where a player used a steering wheel to guide a plastic car along a
racetrack, which scrolled beneath the car via electric motor. It was the first
Nintendo toy that required electricity, and a complete flop. The internal
mechanism was advanced for the time and ended up so complex and fragile
that it was expensive and hard to produce, and units were riddled with

defects. But the debacle was the seed of a creative philosophy Yokoi would
hone for the next thirty years.
Yokoi was well aware of his engineering limitations. As one aficionado
of game history put it, “He studied electronics at a time where the
technology was evolving faster than the snow melts in sunlight.” Yokoi had
no desire (or capability) to compete with electronics companies that were
racing one another to invent some entirely new sliver of dazzling
technology. Nor could Nintendo compete with Japan’s titans of traditional
toys—Bandai, Epoch, and Takara—on their familiar turf. With that, and
Drive Game, in mind, Yokoi embarked on an approach he called “lateral
thinking with withered technology.” Lateral thinking is a term coined in the
1960s for the reimagining of information in new contexts, including the
drawing together of seemingly disparate concepts or domains that can give
old ideas new uses. By “withered technology,” Yokoi meant tech that was
old enough to be extremely well understood and easily available, so it
didn’t require a specialist’s knowledge. The heart of his philosophy was
putting cheap, simple technology to use in ways no one else considered. If
he could not think more deeply about new technologies, he decided, he
would think more broadly about old ones. He intentionally retreated from
the cutting edge, and set to monozukuri .
He connected a transistor to a cheap, store-bought galvanometer, and
noticed he could measure the current flowing through his coworkers. Yokoi
imagined a toy that would make it fun for boys and girls to hold hands,
risqué at the time in Japan. 
*
 The Love Tester was nothing more than two
conductive handles and a gauge. Players grasped a handle and joined hands,
thereby completing the circuit. The gauge reported electrical current as if it
were a measure of the love between participants. The sweatier their palms,
the better a couple’s conductance. It was a hit among teenagers, and a party
prop for adults. Yokoi was encouraged. He committed to using technology
that had already become cheap, even obsolete, in new ways.
By the early 1970s, radio-controlled toy cars were popular, but good RC
technology could cost a month’s salary, so it was a hobby reserved for
adults. As he often did, Yokoi pondered a way to democratize RC toys. So
he took the tech backward. Expense came from the need for multiple radio
control channels. Cars started with two channels, one to control the engine
output and one the steering wheel. The more functions a toy had, the more
channels it required. Yokoi stripped the technology down to the absolute

bare minimum, a single-channel RC car that could only turn left. Product
name: Lefty RX. It was less than a tenth the cost of typical RC toys, and
just fine for counterclockwise races. Even when it did have to navigate
obstacles, kids easily learned how to left-turn their way out of trouble.
One day in 1977, while riding the bullet train back from a business trip in
Tokyo, Yokoi awoke from a nap to see a salaryman playing with a
calculator to relieve the boredom of his commute. The trend at the time was
to make toys as impressively big as possible. What if, Yokoi wondered,
there was a game small enough that an adult could play it discreetly while
commuting? He sat on the idea for a while, until one day when he was
drafted to be the company president’s chauffeur. The normal driver had the
flu, and thanks to Yokoi’s interest in foreign vehicles, he was the only one
of Nintendo’s hundred employees who had driven a car with the steering
wheel on the left, like the president’s Cadillac. He floated his miniature
game idea from the front seat. “He was nodding along,” Yokoi recalled,
“but he didn’t seem all that interested.”
A week later, Yokoi received a surprise visit from executives at Sharp, a
calculator manufacturer. At the meeting Yokoi had driven him to, the
Nintendo president sat next to the head of Sharp, and relayed his
chauffeur’s idea. For several years, Sharp had been engaged in calculator
wars with Casio. In the early 1970s a calculator cost a few hundred dollars,
but as components got cheaper and companies raced for market share, cost
plummeted and the market saturated. Sharp was eager to find a new use for
its LCD screens.
When Sharp executives heard Yokoi’s idea for a video game the size of a
business card holder, and that could be held in the lap and played with
thumbs, they were intrigued, and skeptical. Was it worth mobilizing a new
partnership just to reuse technology that had become dirt cheap? They
weren’t convinced it was even possible to make a display smooth enough
for the game Yokoi proposed, which involved a juggler whose arms move
left and right, trying not to drop balls as they speed up. Nonetheless, the
Sharp engineers made Yokoi an LCD screen in the appropriate size. Then he
hit a severe problem. The electronics in the tiny game were packed in such
a thin space that the liquid crystal display element touched a plate in the
screen, which created a visual distortion of light and dark bands, known as
Newton’s rings. Yokoi needed a sliver of space between the LCD and the
plate. He took an idea from the credit card industry. With a slight tweak of

the old hanafuda printing machines, he delicately embossed the screen with
hundreds of dots to keep the plate and the display element narrowly
separated. As a final flourish, with just a few hours of work, a colleague
helped him program a clock into the display. LCD screens were already in
wristwatches, and they figured it would give adults an excuse to buy their
“Game & Watch.”
In 1980, Nintendo released its first three Game & Watch models, with
high hopes for one hundred thousand sales. Six hundred thousand copies
sold in the first year. Nintendo could not keep up with international
demand. The Donkey Kong Game & Watch was released in 1982 and alone
sold eight million units. Game & Watch remained in production for eleven
years and sold 43.4 million units. It also happened to include another Yokoi
invention that would be used laterally: the directional pad, or “D-pad,”
which allowed a player to move their character in any direction using just a
thumb. After the success of the Game & Watch, Nintendo put the D-pad in
controllers on its new Nintendo Entertainment System. That home console
brought arcade games into millions of homes around the world, and
launched a new era of gaming. The combination of successes—the Game &
Watch and the NES—also led to Yokoi’s lateral-thinking magnum opus, a
handheld console that played any game a developer could put on a
cartridge: the Game Boy.
From a technological standpoint, even in 1989, the Game Boy was
laughable. Yokoi’s team cut every corner. The Game Boy’s processor had
been cutting edge—in the 1970s. By the mid-1980s, home consoles were in
fierce competition over graphics quality. The Game Boy was an eyesore. It
featured a total of four grayscale shades, displayed on a tiny screen that was
tinted a greenish hue somewhere between mucus and old alfalfa. Graphics
in fast lateral motion smeared across the screen. To top it off, the Game Boy
had to compete with handheld consoles from Sega and Atari that were
technologically superior in every way. And it destroyed them .
What its withered technology lacked, the Game Boy made up in user
experience. It was cheap. It could fit in a large pocket. It was all but
indestructible. If a drop cracked the screen—and it had to be a horrific drop
—it kept on ticking. If it were left in a backpack that went in the washing
machine, once it dried out it was ready to roll a few days later. Unlike its
power-guzzling color competitors, it played for days (or weeks) on AA
batteries. Old hardware was extremely familiar to developers inside and

outside Nintendo, and with their creativity and speed unencumbered by
learning new technology, they pumped out games as if they were early
ancestors of iPhone app designers— Tetris, Super Mario Land , The Final
Fantasy Legend, and a slew of sports games released in the first year were
all smash hits. With simple technology, Yokoi’s team sidestepped the
hardware arms race and drew the game programming community onto its
team.
The Game Boy became the Sony Walkman of video gaming, forgoing
top-of-the-line tech for portability and affordability. It sold 118.7 million
units, far and away the bestselling console of the twentieth century. Not bad
for the little company that was allowed to sell hanafuda .
Even though he was revered by then, Yokoi had to push and shove
internally for his “lateral thinking with withered======================================================== pivot after
another, applying the lessons as he went.
Ogas uses the shorthand “standardization covenant” for the cultural
notion that it is rational to trade a winding path of self-exploration for a
rigid goal with a head start because it ensures stability. “The people we
study who are fulfilled do pursue a long-term goal, but they only formulate
it after a period of discovery,” he told me. “Obviously, there’s nothing
wrong with getting a law or medical degree or PhD. But it’s actually riskier
to make that commitment before you know how it fits you. And don’t
consider the path fixed. People realize things about themselves halfway
through medical school.” Charles Darwin, for example.

At his father’s behest he planned to be a doctor, but he found medical
lectures “intolerably dull,” and partway through his education he walked
out of an operation at the grind of the surgical saw. “Nor did I ever attend
again,” Darwin wrote, “for hardly any inducement would have been strong
enough to make me do so.” Darwin was a Bible literalist at the time, and
figured he would become a clergyman. He bounced around classes,
including a botany course with a professor who subsequently recommended
him for an unpaid position aboard the HMS Beagle . After convincing his
father (with his uncle’s help) that he would not become a deadbeat if he
took this one detour, Darwin began perhaps the most impactful post-college
gap year in history. His father’s wishes eventually “died a natural death.”
Decades later, Darwin reflected on the process of self-discovery. “It seems
ludicrous that I once intended to be a clergyman,” he wrote. His father, a
doctor for more than sixty years, detested the sight of blood. “If his father
had given him any choice,” Darwin wrote, “nothing should have induced
him to follow it.”
Michael Crichton started with medicine too, after learning how few
writers make a living. With medicine, “I would never have to wonder if the
work was worthwhile,” he wrote. Except, a few years in he became
disenchanted with medical practice. He graduated from Harvard Medical
School, but decided to become a writer. His medical education was not
remotely wasted. He used it to craft some of the most popular stories in the
world—the novel Jurassic Park, and the TV series ER, with its record-
setting 124 Emmy nominations.
Career goals that once felt safe and certain can appear ludicrous, to use
Darwin’s adjective, when examined in the light of more self-knowledge.
Our work preferences and our life preferences do not stay the same, because
we do not stay the same.
Psychologist Dan Gilbert called it the “end of history illusion.” From
teenagers to senior citizens, we recognize that our desires and motivations
sure changed a lot in the past (see: your old hairstyle), but believe they will
not change much in the future. In Gilbert’s terms, we are works in progress
claiming to be finished.
Gilbert and colleagues measured the preferences, values, and
personalities of more than nineteen thousand adults aged eighteen to sixty-

eight. Some were asked to predict how much they would change over the
next decade, others to reflect about how much they had changed in the
previous one. Predictors expected that they would change very little in the
next decade, while reflectors reported having changed a lot in the previous
one. Qualities that feel immutable changed immensely. Core values—
pleasure, security, success, and honesty—transformed. Preferences for
vacations, music, hobbies, and even friends were transfigured. Hilariously,
predictors were willing to pay an average of $129 a ticket for a show ten
years away by their current favorite band, while reflectors would only pay
$80 to see a show today by their favorite band from ten years ago. The
precise person you are now is fleeting, just like all the other people you’ve
been. That feels like the most unexpected result, but it is also the most well
documented.
It is definitely true that a shy child is more likely to foreshadow a shy
adult, but it is far from a perfect correlation. And if one particular
personality trait does not change, others will. The only certainty is change,
both on average as a generation ages, and within each individual. University
of Illinois psychologist Brent W. Roberts specializes in studying personality
development. He and another psychologist aggregated the results of ninety-
two studies and revealed that some personality traits change over time in
fairly predictable ways. Adults tend to become more agreeable, more
conscientious, more emotionally stable, and less neurotic with age, but less
open to experience. In middle age, adults grow more consistent and
cautious and less curious, open-minded, and inventive. 
*
 The changes have
well-known impacts, like the fact that adults generally become less likely to
commit violent crimes with age, and more able to create stable
relationships. The most momentous personality changes occur between age
eighteen and one’s late twenties, so specializing early is a task of predicting
match quality for a person who does not yet exist. It could work, but it
makes for worse odds. Plus, while personality change slows, it does not
stop at any age. Sometimes it can actually happen instantly.
Thanks to YouTube, the “marshmallow test” could be the most famous
scientific experiment in the world. It was actually a series of experiments
starting in the 1960s. The original premise was simple: An experimenter
places a marshmallow (or a cookie, or a pretzel) in front of a nursery school

child; before leaving, the experimenter tells the child that if she can wait
until the experimenter returns, she’ll get that marshmallow plus a second
one. If the child can’t wait, she can eat the marshmallow. The children were
not told how long the wait would be (it was fifteen to twenty minutes,
depending on age), so they just had to hold out if they wanted the maximum
reward.
Psychologist Walter Mischel and his research team followed up with the
children years later, and found that the longer a child had been able to wait,
the more likely she was to be successful socially, academically, and
financially, and the less likely she was to abuse drugs.
The marshmallow test was already a celebrity as scientific experiments
go, but it became the Beyoncé of studies when media outlets and parents
eager to foretell their child’s destiny started posting DIY marshmallow tests
online. The videos are by turns adorable and intriguing. Nearly all kids wait
at least a little. Some stare at the marshmallow, touch it, sniff it, delicately
tap their tongue to it and pull back as if it were hot. Maybe they even put it
in their mouth, pull it out, and simulate a big bite. Some tear a barely
noticeable piece off for a micro test taste. Before the end of the video, the
kids who start by touching it will have eaten the marshmallow. The kids
who successfully hold out wield all manner of distraction, from looking
away to shoving the plate away, covering their eyes, turning and screaming,
singing, talking to themselves, counting, generally thrashing around in the
chair, or (boys) hitting themselves in the face. One little boy who spent his
time looking in every direction except at the marshmallow is so ravenous
when the experimenter returns with his second treat that he mashes them
both into his mouth immediately.
The crystal ball allure of the marshmallow test is undeniable, and also
misconstrued. Mischel’s collaborator Yuichi Shoda has repeatedly made a
point of saying that plenty of preschoolers who ate the marshmallow turned
out just fine. 
*
 Shoda maintained that the most exciting aspect of the studies
was demonstrating how easily children could learn to change a specific
behavior with simple mental strategies, like thinking about the
marshmallow as a cloud rather than food. Shoda’s post-marshmallow-test
work has been one part of a bridge in psychology between extreme
arguments in the debate about the roles of nature and nurture in personality.
One extreme suggests that personality traits are almost entirely a function of
one’s nature, and the other that personality is entirely a function of the

environment. Shoda argued that both sides of the so-called person-situation
debate were right. And wrong. At a given point in life, an individual’s
nature influences how they respond to a particular situation, but their nature
can appear surprisingly different in some other situation. With Mischel, he
began to study “if-then signatures.” If David is at a giant party, then he
seems introverted, but if David is with his team at work, then he seems
extroverted. (True.) So is David introverted or extroverted? Well, both, and
consistently so.
Ogas and Rose call this the “context principle.” In 2007, Mischel wrote,
“The gist of such findings is that the child who is aggressive at home may
be less aggressive than most when in school; the man exceptionally hostile
when rejected in love may be unusually tolerant about criticism of his work;
the one who melts with anxiety in the doctor’s office may be a calm
mountain climber; the risk-taking entrepreneur may take few social risks.”
Rose framed it more colloquially: “If you are conscientious and neurotic
while driving today, it’s a pretty safe bet you will be conscientious and
neurotic while driving tomorrow. At the same time . . . you may not be
conscientious and neurotic when you are playing Beatles cover songs with
your band in the context of the local pub.” Perhaps that is one reason Daniel
Kahneman and his colleagues in the military (chapter 1 ) failed to predict
who would be a leader in battle based on who had been a leader in an
obstacle course exercise. When I was a college runner, I had teammates
whose drive and determination seemed almost boundless on the track, and
nearly absent in the classroom, and vice versa. Instead of asking whether
someone is gritty, we should ask when they are. “If you get someone into a
context that suits them,” Ogas said,======================================================== particular violinist: “She is the first of her sex to
challenge the success of our great artists.” Even listeners not obviously
disposed to support the arts were moved. Francesco Coli described “angelic
Sirens,” who exceeded “even the most ethereal of birds” and “threw open
for listeners the doors of Paradise.” Especially surprising praise, perhaps,
considering that Coli was the official book censor for the Venetian
Inquisition.
The best figlie became Europe-wide celebrities, like Anna Maria della
Pietà. A German baron flatly declared her “the premier violinist in Europe.”
The president of the parliament of Burgundy said she was “unsurpassed”
even in Paris. An expense report that Vivaldi recorded in 1712 shows that
he spent twenty ducats on a violin for sixteen-year-old Anna Maria, an
engagement-ring-like sum for Vivaldi, who made that much in four months.
Among the hundreds of concertos Vivaldi wrote for the figlie del coro are
twenty-eight that survived in the “Anna Maria notebook.” Bound in leather
and dyed Venetian scarlet, it bears Anna Maria’s name in gold leaf
calligraphy. The concertos, written specifically to showcase her prowess,
are filled with high-speed passages that require different notes to be played
on multiple strings at the same time. In 1716, Anna Maria and the figlie
were ordered by the Senate to intensify their musical work in an effort to
bring God’s favor to the Venetian armies as they battled the Ottoman

Empire on the island of Corfu. (In that siege, the Venetian violin, and a
well-timed storm, proved mightier than the Turkish cannon.)
Anna Maria was middle-aged in the 1740s, when Jean-Jacques Rousseau
came to visit. The rebel philosopher who would fuel the French Revolution
was also a composer. “I had brought with me from Paris the national
prejudice against Italian music,” Rousseau wrote. And yet he declared that
the music played by the figlie del coro “has not its like, either in Italy, or the
rest of the world.” Rousseau had one problem, though, that “drove me to
despair.” He could not see the women. They performed behind a thin crepe
hung in front of wrought-iron latticework grilles in elevated church
balconies. They could be heard, but only their silhouettes seen, tilting and
swaying with the tides of the music, like shadow pictures in a vaudeville
stage set. The grilles “concealed from me the angels of beauty,” Rousseau
wrote. “I could talk of nothing else.”
He talked about it so much that he happened to talk about it with one of
the figlie ’s important patrons. “If you are so desirous to see those little
girls,” the man told Rousseau, “it will be an easy matter to satisfy your
wishes.”
Rousseau was so desirous. He pestered the man incessantly until he took
him to meet the musicians. And there, Rousseau, whose fearless writing
would be banned and burned before it fertilized the soil of democracy, grew
anxious. “When we entered the salon which confined these longed-for
beauties,” he wrote, “I felt an amorous trembling, which I had never before
experienced.”
The patron introduced the women, the siren prodigies whose fame had
spread like a grassfire through Europe—and Rousseau was stunned.
There was Sophia—“horrid,” Rousseau wrote. Cattina—“she had but one
eye.” Bettina—“the smallpox had entirely disfigured her.” “Scarcely one of
them,” according to Rousseau, “was without some striking defect.”
A poem had recently been written about one of the best singers: “Missing
are the fingers of her left hand / Also absent is her left foot.” An
accomplished instrumentalist was the “poor limping lady.” Other guests left
even less considerate records.
Like Rousseau, English visitor Lady Anna Miller was entranced by the
music and pleaded to see the women perform with no barrier hiding them.

“My request was granted,” Miller wrote, “but when I entered I was seized
with so violent a fit of laughter, that I am surprised they had not driven me
out again. . . . My eyes were struck with the sight of a dozen or fourteen
beldams ugly and old . . . these with several young girls.” Miller changed
her mind about watching them play, “so much had the sight of the
performers disgusted me.”
The girls and women who delighted delicate ears had not lived delicate
lives. Many of their mothers worked in Venice’s vibrant sex industry and
contracted syphilis before they had babies and dropped them off at the
Ospedale della Pietà. The name literally means “Hospital of Pity,” but
figuratively it was the House of Mercy, where the girls grew up and learned
music. It was the largest of four ospedali, charitable institutions in Venice
founded to ameliorate particular social ills. In the Pietà’s case, the ill was
that fatherless babies (mostly girls) frequently ended up in the canals.
Most of them would never know their mothers. They were dropped off in
the scaffetta, a drawer built into the outer wall of the Pietà. Like the size
tester for carry-on luggage at the airport, if a baby was small enough to fit,
the Pietà would raise her.
The great Anna Maria was a representative example. Someone, probably
her mother, who was probably a prostitute, took baby Anna Maria to the
doorstep of the Pietà on the waterfront of Venice’s St. Mark’s Basin, along a
bustling promenade. A bell attached to the scaffetta alerted staff of each
new arrival. Babies were frequently delivered with a piece of fabric, a coin,
ring, or some trinket left in the scaffetta as a form of identification should
anyone ever return to claim them. One mother left half of a brilliantly
illustrated weather chart, hoping one day to return with the other half. As
with many of the objects, and many of the girls, it remained forever in the
Pietà. Like Anna Maria, most of the foundlings would never know a blood
relative, and so they were named for their home: Anna Maria della Pietà—
Anna Maria of the Pieta. An eighteenth-century roster lists Anna Maria’s de
facto sisters: Adelaide della Pietà, Agata della Pietà, Ambrosina della Pietà,
and on and on, all the way through Violeta, Virginia, and Vittoria della
Pietà.
The ospedali were public-private partnerships, each overseen by a
volunteer board of upper-class Venetians. The institutions were officially
secular, but they were adjoined to churches, and life inside ran according to
quasi-monastic rules. Residents were separated according to age and

gender. Daily Mass was required before breakfast, and regular confession
was expected. Everyone, even children, worked constantly to keep the
institution running. One day a year, girls were allowed a trip to the
countryside, chaperoned, of course. It was a rigid existence, but there were
benefits.
The children were taught to read, write, and do arithmetic, as well as
vocational skills. Some became pharmacists for the residents, others
laundered silk or sewed ship sails that could be sold. The ospedali were
fully functioning, self-contained communities. Everyone was compensated
for their work, and the Pietà had its own interest-paying bank meant to help
wards learn to manage their own money. Boys learned a trade or joined the
navy and left as teenagers. For girls, marriage was the primary route to
emancipation. Dowries were kept ready, but many wards stayed forever.
As the ospedali accrued instruments, music was added to the education
of dozens of girls so that they could play during religious ceremonies in the
adjacent churches. After a plague in 1630 wiped out one-third of the
population, Venetians found themselves in an especially “penitential mood,”
as one historian put it. The musicians suddenly became more important.
The ospedali governors noticed that a lot more people were attending
church, and that the institutional endowments swelled with donations
proportional to the quality of the girls’ music. By the eighteenth century, the
governors were openly promoting the musicians for fundraising. Each
Saturday and Sunday, concerts began before sunset. The church was so
packed that the Eucharist had to be moved. Visitors were still welcome for
free, of course, but if a guest wanted to sit, ospedali staff were happy to rent
out chairs. Once the indoor space was full, listeners crowded outside
windows, or paused their gondolas in the basin outside. Foundlings became
an economic engine not just sustaining the social welfare system in Venice,
but drawing tourists from abroad. Entertainment and penitence mixed in
amusing ways. Audience members were not allowed to applaud in church,
so after the final note they coughed and hemmed and scraped their feet and
blew their noses in admiration.
The ospedali commissioned composers for original works. Over one six-
year period, Vivaldi wrote 140 concertos exclusively for the Pietà
musicians. A teaching system evolved, where the older figlie taught the
younger, and the younger the beginners. They held multiple jobs—Anna
Maria was a teacher and copyist—and yet they produced star after virtuoso

star. After Anna Maria, her soloist successor, Chiara della Pietà, was hailed
as the greatest violinist in all of Europe .
It all raises the question: Just what magical training mechanism was
deployed to transform the orphan foundlings of the Venetian sex industry,
who but for the grace of charity would have died in the city’s canals, into
the world’s original international rock stars?
The Pietà’s music program was not unique for its rigor. According to a list
of Pietà directives, formal lessons were Tuesdays, Thursdays, and
Saturdays, and figlie were free to practice on their own. Early in the rise of
the figlie del coro, work and chores took most of their time, so they were
only allowed an hour a day of music study.
The most surprising feature was how many instruments they learned.
Shortly after he received his music doctorate from Oxford, eighteenth-
century English composer and historian Charles Burney set out to write a
definitive history of modern music, which involved several ospedali visits.
Burney, who became famous as both a travel writer and======================================================== challenge. Even the most sacred tools. Even the tools
so taken for granted they become invisible. It is, of course, easier said than
done. Especially when the tool is the very core of an organization’s culture.
As Captain Tony Lesmes described it, his team at Bagram Air Base in
northeast Afghanistan only went to work when someone got really unlucky.
Lesmes commanded a team of Air Force pararescue jumpers, PJs for short,
a division of Special Operations designed for harrowing rescue missions,
like parachuting into enemy territory at night to save downed pilots. Cross a
soldier, a paramedic, a rescue diver, a firefighter, a mountain rescue
specialist, and a parachutist, and you get a PJ. Their emblem depicts an
angel with arms wrapped around the world, and the words “That others may
live.”
There was no typical day for the PJs at Bagram. One day they were
rappelling down a mountain to rescue a soldier who fell into an unmarked
well. Another day they were rushing to treat Marines injured in a firefight.
PJs could accompany units out on missions, but mostly they stayed on
twenty-four-hour alert, waiting for a “9-line,” a form (with nine lines) that
provided basic information about an active emergency. Like one that came
in on an autumn day in 2009. It was a category alpha, traumatic injuries.
Within minutes, the team would be airborne.

Intel was sparse. A roadside bomb had exploded in the middle of an
Army convoy of armored vehicles. The site was approximately a half hour
away by helicopter. There were serious injuries, but it was unclear how
many or how serious, and whether the bomb was part of a search and rescue
trap, where enemies lie in ambush awaiting the rescue team.
The PJs were used to working with cloudy information, but this was
ambiguous even for them. Lesmes knew they would have to bring heavy
equipment, like the Jaws of Life and a diamond-tipped saw, because “you
can’t just cut through an armored vehicle like a car door,” he told me.
Weight was an obstacle, especially at altitude in the mountains. If the
choppers were too heavy, they wouldn’t grab enough air to stay aloft. Fuel
limitations were a challenge. Space was a bigger one. Each PJ came with
gear, and each of the two helicopters only had interior space on the order of
a large van. They didn’t know how many soldiers were injured badly
enough to need evacuation, and how much space they would need for them.
Lesmes was certain of just one thing: he wanted to make sure they saved
enough room for potential patients so that they would only have to visit the
explosion site once. It would take extra time to treat and load severely
wounded soldiers. The more time on site, the more likely the operation
would draw enemy attention. The rescue team could end up needing a
rescue team.
He was twenty-seven, and the previous year had led a stateside hurricane
rescue team. Afghanistan was his first extended deployment, and he was
directing a team with older members who had had numerous overseas
deployments. As usual, Lesmes brought two team members to the
operations center to get information and help him make sense of the
situation. “Sometimes other guys are able to get really good questions out
that I wouldn’t normally think of,” he told me. “And you want to share as
much information as possible, and there isn’t a lot of time.” But there was
little additional intel. “In Hollywood, a drone flies over the site and you get
all the information,” Lesmes told me. “But that’s Hollywood.”
He walked out to the helicopters, where PJs were donning their full battle
rattle, as he put it. The situation didn’t fit the usual decision trees; he laid
out the challenges, and asked the men: How do we solve this?
Just move equipment around to cram more stuff into the helicopters, one
team member suggested. Another said they could leave a few PJs with the
Army convoy if they needed extra helicopter room for patients. One

recommended they evacuate the most serious patients, and if a second trip
was needed, move the convoy from the explosion site and meet them
somewhere less conspicuous. But the bomb had exploded in the middle of a
procession of vehicles, in rugged terrain. Lesmes didn’t even know how
mobile the convoy would be.
“We weren’t coming up with any real solution that would give us an
advantage. I wanted a speed advantage, and the ability to leverage the
weight and space for wounded soldiers,” he told me. “The distance and the
timeline and the constraints and the unknown of the enemy all started to add
up. I just started feeling like we didn’t have the setup to be successful in a
worst-case scenario. There wasn’t that pattern recognition, it was outside of
the normal pattern.” In others words, he didn’t have the definitive intel he
would have liked. Based on the information he had, Lesmes guessed there
would be more than three serious injuries but fewer than fifteen. An idea
started to form, one that could preserve more space for potential patients.
He could put aside a tool he had never dropped in this situation: himself.
Lesmes had never not accompanied his team on a mass-casualty category
alpha. He was the site conductor. His role was to keep a broad view of the
situation while PJs were “heads down” working furiously to save patients,
or their limbs. He helped secure the site; communicated with his guys, the
base, and helicopter pilots who were circling waiting to pick up patients and
go; he radioed planes for backup if a firefight erupted; he coordinated with
officers in the area, frequently from other military branches. Emotional
chaos was an explosion site certainty. Soldiers watching their shell-shocked
teammates suck on fentanyl lollipops, in danger of bleeding out, are
desperate to help, but they must be moved. The site had to be managed.
This time, as long as there were not many more injuries than Lesmes
guessed, he knew his senior enlisted team member could manage leadership
on the ground while administering medical aid. Lesmes could help ready
the field hospital for returning patients, and coordinate helicopter pickups
from the operations center, adjusting as he listened via radio to his guys on
the ground. It was a trade-off, but every option was.
Lesmes went to the team with his “hypothesis,” as he called it—his
hunch held lightly. “I wanted them to disprove it,” he told me. He told them
he planned to stay at the base to save room for equipment and patients. The
helicopter blades were spinning up, moments ticking away in the so-called
golden hour, the critical window for saving a severely injured soldier. He

told them to talk quickly, and he would consider everything they had to say.
A few were quiet. Several objected. Togetherness was their most basic tool,
the one they didn’t know could be dropped until someone said to drop it.
One of the men said flatly that it was the commanding officer’s job to come
along, and he should do his job. Another got angry. A third reflexively
suggested that Lesmes was afraid. He told Lesmes that when it was his
time, it was his time, so they should just do what they always did. Lesmes
was afraid, but not for his life. “If something bad happens, and the officer is
not there,” he told me, “think about explaining that to ten families.”
I was sitting with him at the World War II Memorial in Washington, D.C.,
when he said that. He had been stoic, and then he started crying. “The
whole construct is built on that training and that familiarity and that
cohesion,” he said. “I totally understand why some guys were upset. It was
breaking our standard operating procedure. I mean, my judgment was
questioned. But if I go, we might have to go to the rescue site twice.” The
objections he got were emotional and philosophical, not tactical. They had
changed his mind about a plan before, but not this time. He would stay, and
it was time for them to go. The helicopters strained into the air as Lesmes
returned to the operations center. “I struggled immensely,” he said. “I could
see what was going on, and if something bad were to happen, I could
literally watch the rescue helicopter go down.”
The rescue mission, thankfully, was an unqualified success. PJs treated
injuries at the explosion site, and seven wounded soldiers had to be loaded
into the helicopters. They were packed in like sardines. Several required
amputations at the field hospital, but all survived.
When it was over, the senior enlisted man acknowledged it was the right
call. Another PJ did not address it for months, and then only to say that he
was taken aback that Lesmes had that much trust in them. The soldier who
had gotten angry initially remained angry, for a while. Another Bagram PJ I
spoke with said, “If I was in that position, I definitely would have said,
‘Yeah, we’re all going.’ It must have been really hard.”
“I don’t know, man,” Lesmes told me. “Sometimes, I still struggle with
that decision. Something could’ve gone wrong and then it would be a bad
decision. Maybe it was luck. None of the options at the time looked very
optimal.”
As we finished talking, I mentioned Weick’s work about wilderness
firefighters clinging to their tools. Under pressure, Weick explained,

experienced pros regress to what they know best. I suggested to Lesmes that
maybe his PJs were just reacting emotionally, with a reflex for the familiar.
There must be times when even the sacrosanct tool of togetherness should
be dropped, right? “Yeah, mmm-hmm.” He nodded in agreement. It was, of
course, easy for me to say. He paused for a moment. “Yeah,” he said, “but
everything is built on that.”
The Challenger managers made mistakes of conformity. They stuck to the
usual tools in the face of an unusual challenge. Captain Lesmes dropped a
sacred tool, and it worked. Once emotions cooled, several members of his
team acknowledged it was the right call. Others never did. Going back over
it brought Lesmes to tears. It isn’t exactly the fairy-tale ending to a good
decision. Had NASA canceled the launch, Allan======================================================== of a naval clash
claiming more than ten lives in the East China Sea? Forecasters could
update predictions as often as they wanted, but the scoring system rewarded
accuracy over time, so a great prediction at the last minute before a
question’s end date was of limited value.
The team run by Tetlock and Mellers was called the Good Judgment
Project. Rather than recruit decorated experts, in the first year of the
tournament they made an open call for volunteers. After a simple screening,
they invited thirty-two hundred to start forecasting. From those, they
identified a small group of the foxiest forecasters—just bright people with
wide-ranging interests and reading habits but no particular relevant
background—and weighted team forecasts toward them. They destroyed the
competition.
In year two, the Good Judgment Project randomly arranged the top
“superforecasters” into online teams of twelve, so that they could share
information and ideas. They beat the other university-run teams so badly
that IARPA dropped those lesser competitors from the tournament. The
volunteers drawn from the general public beat experienced intelligence
analysts with access to classified data “by margins that remain classified,”

according to Tetlock. (He has, though, referenced a Washington Post report
indicating that the Good Judgment Project performed about 30 percent
better than a collection of intelligence community analysts.)
Not only were the best forecasters foxy as individuals, they had qualities
that made them particularly effective collaborators—partners in sharing
information and discussing predictions. Every team member still had to
make individual predictions, but the team was scored by collective
performance. On average, forecasters on the small superteams became 50
percent more accurate in their individual predictions. Superteams beat the
wisdom of much larger crowds—in which the predictions of a large group
of people are averaged—and they also beat prediction markets, where
forecasters “trade” the outcomes of future events like stocks, and the market
price represents the crowd prediction.
It might seem like the complexity of predicting geopolitical and
economic events would necessitate a group of narrow specialists, each
bringing to the team extreme depth in one area. But it was actually the
opposite. As with comic book creators and inventors patenting new
technologies, in the face of uncertainty, individual breadth was critical. The
foxiest forecasters were impressive alone, but together they exemplified the
most lofty ideal of teams: they became more than the sum of their parts. A
lot more.
A few of the qualities that make the best Good Judgment Project forecasters
valuable teammates are obvious from talking to them. They are bright, but
so were the hedgehog experts Tetlock started with. They toss around
numbers easily, estimating this country’s poverty rate or that state’s
proportion of farmland. And they have range.
Scott Eastman told me that he “never completely fit in one world.” He
grew up in Oregon and competed in math and science contests, but in
college he studied English literature and fine arts. He has been a bicycle
mechanic, a housepainter, founder of a housepainting company, manager of
a multimillion-dollar trust, a photographer, a photography teacher, a lecturer
at a Romanian university—in subjects ranging from cultural anthropology
to civil rights—and, most unusually, chief adviser to the mayor of Avrig, a
small town in the middle of Romania. In that role, he did everything from

helping integrate new technologies into the local economy to dealing with
the press and participating in negotiations with Chinese business leaders.
Eastman narrates his life like a book of fables; each experience comes
with a lesson. “I think that housepainting was probably one of the greatest
helps,” he told me. It afforded him the chance to interact with a diverse
palette of colleagues and clients, from refugees seeking asylum to Silicon
Valley billionaires whom he would chat with if he had a long project
working on their homes. He described it as fertile ground for collecting
perspectives. But housepainting is probably not a singular education for
geopolitical prediction. Eastman, like his teammates, is constantly
collecting perspectives anywhere he can, always adding to his intellectual
range, so any ground is fertile for him.
Eastman was uncannily accurate at predicting developments in Syria, and
surprised to learn that Russia was his weak spot. He studied Russian and
has a friend who was a former ambassador to Russia. “I should have every
leg up there, but I saw over a large series of questions, it was one of my
weakest areas,” he told me. He learned that specializing in a topic
frequently did not bear fruit in the forecasts. “So if I know somebody [on
the team] is a subject area expert, I am very, very happy to have access to
them, in terms of asking questions and seeing what they dig up. But I’m not
going to just say, ‘Okay, the biochemist said a certain drug is likely to come
to market, so he must be right.’ Often if you’re too much of an insider, it’s
hard to get good perspective.” Eastman described the core trait of the best
forecasters to me as: “genuinely curious about, well, really everything.”
Ellen Cousins researches fraud for trial lawyers. Her research naturally
roams from medicine to business. She has wide-ranging interests on the
side, from collecting historical artifacts to embroidery, laser etching, and
lock picking. She conducts pro bono research on military veterans who
should (and sometimes do) get upgraded to the Medal of Honor. She felt
exactly the same as Eastman. Narrow experts are an invaluable resource,
she told me, “but you have to understand that they may have blinders on. So
what I try to do is take facts from them, not opinions.” Like polymath
inventors, Eastman and Cousins take ravenously from specialists and
integrate.
Superforecasters’ online interactions are exercises in extremely polite
antagonism, disagreeing without being disagreeable. Even on a rare
occasion when someone does say, “ ‘You’re full of beans, that doesn’t make

sense to me, explain this,’” Cousins told me, “they don’t mind that.”
Agreement is not what they are after; they are after aggregating
perspectives, lots of them. In an impressively unsightly image, Tetlock
described the very best forecasters as foxes with dragonfly eyes. Dragonfly
eyes are composed of tens of thousands of lenses, each with a different
perspective, which are then synthesized in the dragonfly’s brain.
One forecast discussion I saw was a team trying to predict the highest
single-day close for the exchange rate between the U.S. dollar and
Ukrainian hryvnia during an extremely volatile stretch in 2014. Would it be
less than 10, between 10 and 13, or more than 13? The discussion started
with a team member offering percentage predictions for each of the three
possibilities, and sharing an Economist article. Another team member
chimed in with a Bloomberg link and online historical data, and offered
three different probability predictions, with “between 10 and 13” favored. A
third teammate was convinced by the second’s argument. A fourth shared
information about the dire state of Ukrainian finances. A fifth addressed the
broader issue of how exchange rates change, or don’t, in relation to world
events. The teammate who started the conversation then posted again; he
was persuaded by the previous arguments and altered his predictions, but
still thought they were overrating the possibility of “more than 13.” They
continued to share information, challenge one another, and update their
forecasts. Two days later, a team member with specific expertise in finance
saw that the hryvnia was strengthening amid events he thought would
surely weaken it. He chimed in to inform his teammates that this was
exactly the opposite of what he expected, and that they should take it as a
sign of something wrong in his understanding. In contrast to politicians, the
most adept predictors flip-flop like crazy. The team finally homed in on
“between 10 and 13” as the heavy favorite, and they were correct.
In separate work, from 2000 to 2010 German psychologist Gerd
Gigerenzer compiled annual dollar-euro exchange rate predictions made by
twenty-two of the most prestigious international banks—Barclays,
Citigroup, JPMorgan Chase, Bank of America Merrill Lynch, and others.
Each year, every bank predicted the end-of-year exchange rate.
Gigerenzer’s simple conclusion about those projections, from some of the
world’s most prominent specialists: “Forecasts of dollar-euro exchange
rates are worthless.” In six of the ten years, the true exchange rate fell
outside the entire range of all twenty-two bank forecasts. Where a

superforecaster quickly highlighted a change in exchange rate direction that
confused him, and adjusted, major bank forecasts missed every single
change of direction in the decade Gigerenzer analyzed.
A hallmark of interactions on the best teams is what psychologist Jonathan
Baron termed “active open-mindedness.” The best forecasters view their
own ideas as hypotheses in need of testing. Their aim is not to convince
their teammates of their own expertise, but to encourage their teammates to
help them falsify their own notions. In the sweep of humanity, that is not
normal. Asked a difficult question—for example, “Would providing more
money for public schools significantly improve the quality of teaching and
learning?”—people naturally come up with a deluge of “myside” ideas.
Armed with a web browser, they don’t start searching for why they are
probably wrong. It is not that we are unable to come up with contrary ideas,
it is just that our strong instinct is not to.
Researchers in Canada and the United States began a 2017 study by
asking a politically diverse and well-educated group of adults to read
arguments confirming their beliefs about controversial issues. When
participants======================================================== most successful young learners
were those who had been through a range of musical instruments”); and J. A. Sloboda and M. J. A.
Howe, “Biographical Precursors of Musical Excellence,” Psychology of Music 19 (1991): 3–21 (“The
exceptional children practiced much less than the average children on their first chosen instrument
but much more than the average children on their third instrument”).
“a mismatch between the instruments”: S. A. O’Neill, “Developing a Young Musician’s Growth
Mindset,” in Music and the Mind, ed. I. Deliège and J. W. Davidson (Oxford: Oxford University
Press, 2011).
“It seems very clear”: Sloboda and Howe, “Biographical Precursors of Musical Excellence.”
A study that followed up: A. Ivaldi, “Routes to Adolescent Musical Expertise,” in Music and the
Mind, ed. Deliège and Davidson.
“Despite the ever-increasing number”: P. Gorner, “Cecchini’s Guitar Truly Classical,” Chicago
Tribune, July 13, 1968. (Studs Terkel interviewed Cecchini the day before the performance. That
fantastic conversation about music can be found here: http://jackcecchini.com/Interviews.html ).
“There was no connection”: T. Teachout, Duke: A Life of Duke Ellington (New York: Gotham
Books, 2013).
America’s preeminent composer: Kerman and Tomlinson, Listen , 394.

“John played anything”: L. Flanagan, Moonlight in Vermont: The Official Biography of Johnny
Smith (Anaheim Hills, CA: Centerstream, 2015).
“I got a wonderful piano teacher”: F. M. Hall, It’s About Time: The Dave Brubeck Story.
(Fayetteville: University of Arkansas Press, 1996).
“with a drawn knife”; “I wonder if”: M. Dregni, Django: The Life and Music of a Gypsy Legend
(Oxford: Oxford University Press, 2004 [Kindle ebook]). Two other sources provided particularly
important details about Django’s life: C. Delaunay, Django Reinhardt (New York: Da Capo, 1961)
(on the back cover, James Lincoln Collier, author of The Making of Jazz, identifies Django as
“without question, the single most important guitarist”); and a special Django issue of Guitar Player
magazine (November 1976) devoted to legendary musicians recounting their time with him.
creativity erupted: The 5-CD set “Django Reinhardt—Musette to Maestro 1928–1937: The Early
Work of a Guitar Genius” (JSP Records, 2010) includes recordings of a young Reinhardt both before
and after his injury.
Jimi Hendrix, who kept an album of Django’s: Jacob McMurray, senior curator at Seattle’s
Museum of Pop Culture, kindly confirmed this with the museum’s permanent collection.
sepia-toned YouTube clip: “Django Reinhardt Clip Performing Live (1945),” YouTube,
www.youtube.com/watch?v=aZ308aOOX04 . (The date on the You-Tube video is incorrect. The clip
is from the 1938 short film “Jazz ‘Hot.’”)
“one of osmosis” (and other Berliner quotes): P. F. Berliner, Thinking in Jazz (Chicago: University of
Chicago Press, 1994).
“as if the brain turned off”: C. Kalb, “Who Is a Genius?,” National Geographic, May 2017.
“Well, I can’t read either”: Guitar Player, November 1976.
“a concept that went against conservatory training”: Dregni, Django .
“I can’t improvise at all”: A. Midgette, “Concerto on the Fly: Can Classical Musicians Learn to
Improvise,” Washington Post, June 15, 2012, online ed.
“My complete self-taught technique” and detail about hitting siblings with violins: S. Suzuki,
Nurtured by Love, trans. W. Suzuki (Alfred Music, 1993 [Kindle ebook]).
household rules: J. S. Dacey, “Discriminating Characteristics of the Families of Highly Creative
Adolescents,” Journal of Creative Behavior 23, no. 4 (1989): 263–71. (Grant referenced the study in:
“How to Raise a Creative Child. Step One: Back Off,” New York Times , Jan. 30, 2016.)
CHAPTER 4:LEARNING, FAST AND SLOW
“Okay? You’re going to an Eagles game”: The classroom scene is from video, transcript, and
analysis from the Trends in International Mathematics and Science Study (TIMSS). The particular
video is “M-US2 Writing Variable Expressions.”
“three dollars for a hot dog”: The teacher briefly misspoke and said “two.” It is corrected for
clarity.
“using procedures”; “making connections”: J. Hiebert et al., “Teaching Mathematics in Seven
Countries,” National Center for Education Statistics, 2003, chap. 5.
bansho : E.R.A. Kuehnert et al. “Bansho: Visually Sequencing Mathematical Ideas,” Teaching
Children Mathematics 24, no. 6 (2018): 362–69.
“Students do not view mathematics as a system ”: L. E. Richland et al., “Teaching the Conceptual
Structure of Mathematics,” Educational Psychology 47, no. 3 (2012): 189–203.
tested sixth graders in the South Bronx: N. Kornell and J. Metcalfe, “The Effects of Memory
Retrieval, Errors and Feedback on Learning,” in Applying Science of Learning in Education , V.A.

Benassi et al., ed. (Society for the Teaching of Psychology, 2014); J. Metcalfe and N. Kornell,
“Principles of Cognitive Science in Education,” Psychonomic Bulletin and Review 14, no. 2 (2007):
225–29.
“hypercorrection effect”: T. S. Eich et al., “The Hypercorrection Effect in Younger and Older
Adults,” Neuropsychology, Development and Cognition. Section B, Aging, Neuropsychology and
Cognition 20, no. 5 (2013): 511–21; J. Metcalfe et al., “Neural Correlates of People’s
Hypercorrection of Their False Beliefs,” Journal of Cognitive Neuroscience 24, no. 7 (2012): 1571–
83.
Oberon and Macduff: N. Kornell and H. S. Terrace, “The Generation Effect in Monkeys,”
Psychological Science 18, no. 8 (2007): 682–85.
“Like life”: N. Kornell et al., “Retrieval Attempts Enhance Learning, but Retrieval Success (Versus
Failure) Does Not Matter,” Journal of Experimental Psychology: Learning, Memory, and Cognition
41, no. 1 (2015): 283–94.
Spanish vocabulary learners: H. P. Bahrick and E. Phelps, “Retention of Spanish Vocabulary over 8
Years,” Journal of Experimental Psychology: Learning, Memory, and Cognition 13, no. 2 (1987):
344–49.
Iowa State researchers read: L. L. Jacoby and W. H. Bartz, “Rehearsal and Transfer to LTM,”
Journal of Verbal Learning and Verbal Behavior 11 (1972): 561–65.
“produce misleadingly high levels”: N. J. Cepeda et al., “Spacing Effects in Learning,”
Psychological Science 19, no. 11 (2008): 1095–1102.
In 2007, the U.S. Department of Education: H. Pashler et al., “Organizing Instruction and Study to
Improve Student Learning,” National Center for Education Research, 2007.
an extraordinarily unique study: S. E. Carrell and J. E. West, “Does Professor Quality Matter?,”
Journal of Political Economy 118, no. 3 (2010): 409–32.
A similar study was conducted at Italy’s Bocconi University: M. Braga et al., “Evaluating
Students’ Evaluations of Professors,” Economics of Education Review 41 (2014): 71–88.
“desirable difficulties”: R. A. Bjork, “Institutional Impediments to Effective Training,” in Learning,
Remembering, Believing: Enhancing Human Performance, ed. D. Druckman and R. A. Bjork
(Washington, DC: National Academies Press, 1994), 295–306.
“Above all, the most basic message”: C. M. Clark and R. A. Bjork, “When and Why Introducing
Difficulties and Errors Can Enhance Instruction,” in Applying the Science of Learning in Education,
ed. V. A. Benassi et al. (Society for the Teaching of Psychology, 2014 [ebook]).
said in national surveys: C. Rampell, “Actually, Public Education is Getting Better, Not Worse,”
Washington Post , September 18, 2014.
School has not gotten worse; “jobs that pay well”: G. Duncan and R. J. Murnane, Restoring
Opportunity (Cambridge, MA: Harvard Education Press, 2014 [Kindle ebook]).
In a study using college math problems: D. Rohrer and K. Taylor, “The Shuffling of Mathematics
Problems Improves Learning,” Instructional Science 35 (2007): 481–98. See also: D. Rohrer et al.,
“A Randomized Controlled Trial of Interleaved Mathematics Practice,” Journal of Educational
Psychology (May 16, 2019): advance online publication.
butterfly species identification to psychological-disorder diagnosis: M. S. Birnbaum et al., “Why
Interleaving Enhances Inductive Learning,” Memory and Cognition 41 (2013): 392–402.
naval air defense simulations: C. L. Holladay and M.A. Quiñones, “Practice Variability and
Transfer of Training,” Journal of Applied Psychology 88, no. 6 (2003): 1094–1103.
In one of Kornell and Bjork’s interleaving studies, 80 percent of students: N. Kornell and R. A.
Bjork, “Learning Concepts and Categories: Is Spacing the ‘Enemy of Induction’?,” Psychological

Science 19, no. 6 (2008): 585–92.
a particular left-hand jump across fifteen keys: M. Bangert et al., “When Less of the Same Is
More: Benefits of Variability of Practice in Pianists,” Proceedings of the International Symposium on
Performance Science (2013): 117–22.
O’Neal should practice from a foot in front: Bjork makes this suggestion in Daniel Coyle’s The
Talent Code (New York: Bantam, 2009).
hallmark of expert problem solving: See, for example: M.T.H. Chi et al., “Categorization and
Representation of Physics Problems by Experts and Novices,” Cognitive Science 5, no. 2 (1981):
121–52; and J. F. Voss et al., “Individual Differences in the Solving of Social Science Problems,” in
Individual Differences in Cognition, vol. 1, ed. R. F. Dillon and R. R. Schmeck (New York:
Academic Press, 1983).
reviewed sixty-seven early childhood education programs: D. Bailey et al., “Persistence and
Fadeout in Impacts of Child and Adolescent Interventions,” Journal of Research on Educational
Effectiveness 10, no. 1 (2017): 7–39.
The motor-skill equivalent: S. G. Paris, “Reinterpreting the Development of Reading Skills,”
Reading Research Quarterly 40, no. 2 (2005): 184–202.
CHAPTER 5:THINKING OUTSIDE EXPERIENCE
Giordano Bruno: A. A. Martinez, “Giordano Bruno and the Heresy of Many Worlds,” Annals of
Science 73, no. 4 (2016): 345–74.
Johannes Kepler inherited: Sources that give excellent background on the worldviews that Kepler
inherited, and his transformative analogies, are: D. Gentner et al., “Analogical Reasoning and
Conceptual Change: A Case Study of Johannes Kepler,” Journal of the Learning Sciences 6, no. 1
(1997): 3–40; D. Gentner, “Analogy in======================================================== decades to realize he had been wrong, and that savants
have more in common with prodigies like the Polgar sisters than he thought.
They do not merely regurgitate. Their brilliance, just like the Polgar
brilliance, relies on repetitive structures, which is precisely what made the
Polgars’ skill so easy to automate.
With the advances made by the AlphaZero chess program (owned by an AI
arm of Google’s parent company), perhaps even the top centaurs would be
vanquished in a freestyle tournament. Unlike previous chess programs,
which used brute processing force to calculate an enormous number of
possible moves and rate them according to criteria set by programmers,
AlphaZero actually taught itself to play. It needed only the rules, and then to
play itself a gargantuan number of times, keeping track of what tends to
work and what doesn’t, and using that to improve. In short order, it beat the
best chess programs. It did the same with the game of Go, which has many
more possible positions. But the centaur lesson remains: the more a task
shifts to an open world of big-picture strategy, the more humans have to
add.
AlphaZero programmers touted their impressive feat by declaring that
their creation had gone from “tabula rasa” (blank slate) to master on its
own. But starting with a game is anything but a blank slate. The program is
still operating in a constrained, rule-bound world. Even in video games that
are less bound by tactical patterns, computers have faced a greater
challenge.

The latest video game challenge for artificial intelligence is StarCraft, a
franchise of real-time strategy games in which fictional species go to war
for supremacy in some distant reach of the Milky Way. It requires much
more complex decision making than chess. There are battles to manage,
infrastructure to plan, spying to do, geography to explore, and resources to
collect, all of which inform one another. Computers struggled to win at
StarCraft , Julian Togelius, an NYU professor who studies gaming AI, told
me in 2017. Even when they did beat humans in individual games, human
players adjusted with “long-term adaptive strategy” and started winning.
“There are so many layers of thinking,” he said. “We humans sort of suck at
all of them individually, but we have some kind of very approximate idea
about each of them and can combine them and be somewhat adaptive. That
seems to be what the trick is.”
In 2019, in a limited version of StarCraft , AI beat a pro for the first time.
(The pro adapted and earned a win after a string of losses.) But the game’s
strategic complexity provides a lesson: the bigger the picture, the more
unique the potential human contribution. Our greatest strength is the exact
opposite of narrow specialization. It is the ability to integrate broadly.
According to Gary Marcus, a psychology and neural science professor who
sold his machine learning company to Uber, “In narrow enough worlds,
humans may not have much to contribute much longer. In more open-ended
games, I think they certainly will. Not just games, in open ended real-world
problems we’re still crushing the machines.”
The progress of AI in the closed and orderly world of chess, with instant
feedback and bottomless data, has been exponential. In the rule-bound but
messier world of driving, AI has made tremendous progress, but challenges
remain. In a truly open-world problem devoid of rigid rules and reams of
perfect historical data, AI has been disastrous. IBM’s Watson destroyed at
Jeopardy! and was subsequently pitched as a revolution in cancer care,
where it flopped so spectacularly that several AI experts told me they
worried its reputation would taint AI research in health-related fields. As
one oncologist put it, “The difference between winning at Jeopardy! and
curing all cancer is that we know the answer to Jeopardy! questions.” With
cancer, we’re still working on posing the right questions in the first place.
In 2009, a report in the esteemed journal Nature announced that Google
Flu Trends could use search query patterns to predict the winter spread of
flu more rapidly than and just as accurately as the Centers for Disease

Control and Prevention. But Google Flu Trends soon got shakier, and in the
winter of 2013 it predicted more than double the prevalence of flu that
actually occurred in the United States. Today, Google Flu Trends is no
longer publishing estimates, and just has a holding page saying that “it is
still early days” for this kind of forecasting. Tellingly, Marcus gave me this
analogy for the current limits of expert machines: “AI systems are like
savants.” They need stable structures and narrow worlds.
When we know the rules and answers, and they don’t change over time—
chess, golf, playing classical music—an argument can be made for savant-
like hyperspecialized practice from day one. But those are poor models of
most things humans want to learn.
When narrow specialization is combined with an unkind domain, the
human tendency to rely on experience of familiar patterns can backfire
horribly—like the expert firefighters who suddenly make poor choices
when faced with a fire in an unfamiliar structure. Chris Argyris, who helped
create the Yale School of Management, noted the danger of treating the
wicked world as if it is kind. He studied high-powered consultants from top
business schools for fifteen years, and saw that they did really well on
business school problems that were well defined and quickly assessed. But
they employed what Argyris called single-loop learning, the kind that favors
the first familiar solution that comes to mind. Whenever those solutions
went wrong, the consultant usually got defensive. Argyris found their
“brittle personalities” particularly surprising given that “the essence of their
job is to teach others how to do things differently.”
Psychologist Barry Schwartz demonstrated a similar, learned inflexibility
among experienced practitioners when he gave college students a logic
puzzle that involved hitting switches to turn light bulbs on and off in
sequence, and that they could play over and over. It could be solved in
seventy different ways, with a tiny money reward for each success. The
students were not given any rules, and so had to proceed by trial and error. 
*
If a student found a solution, they repeated it over and over to get more
money, even if they had no idea why it worked. Later on, new students were
added, and all were now asked to discover the general rule of all solutions.
Incredibly, every student who was brand-new to the puzzle discovered the
rule for all seventy solutions, while only one of the students who had been
getting rewarded for a single solution did. The subtitle of Schwartz’s paper:

“How Not to Teach People to Discover Rules”—that is, by providing
rewards for repetitive short-term success with a narrow range of solutions.
All this is bad news for some of the business world’s favorite successful-
learning analogies—the Polgars, Tiger, and to some degree analogies based
in any sport or game. Compared to golf, a sport like tennis is much more
dynamic, with players adjusting to opponents every second, to surfaces, and
sometimes to their own teammates. (Federer was a 2008 Olympic gold
medalist in doubles.) But tennis is still very much on the kind end of the
spectrum compared to, say, a hospital emergency room, where doctors and
nurses do not automatically find out what happens to a patient after their
encounter. They have to find ways to learn beyond practice, and to
assimilate lessons that might even contradict their direct experience.
The world is not golf, and most of it isn’t even tennis. As Robin Hogarth
put it, much of the world is “Martian tennis.” You can see the players on a
court with balls and rackets, but nobody has shared the rules. It is up to you
to derive them, and they are subject to change without notice .
We have been using the wrong stories. Tiger’s story and the Polgar story
give the false impression that human skill is always developed in an
extremely kind learning environment. If that were the case, specialization
that is both narrow and technical and that begins as soon as possible would
usually work. But it doesn’t even work in most sports.
If the amount of early, specialized practice in a narrow area were the key
to innovative performance, savants would dominate every domain they
touched, and child prodigies would always go on to adult eminence. As
psychologist Ellen Winner, one of the foremost authorities on gifted
children, noted, no savant has ever been known to become a “Big-C
creator,” who changed their field.
There are domains beyond chess in which massive amounts of narrow
practice make for grandmaster-like intuition. Like golfers, surgeons
improve with repetition of the same procedure. Accountants and bridge and
poker players develop accurate intuition through repetitive experience.
Kahneman pointed to those domains’ “robust statistical regularities.” But
when the rules are altered just slightly, it makes experts appear to have
traded flexibility for narrow skill. In research in the game of bridge where
the order of play was altered, experts had a more difficult time adapting to

new rules than did nonexperts. When experienced accountants were asked
in a study to use a new tax law for deductions that replaced a previous one,
they did worse than novices. Erik Dane, a Rice University professor who
studies organizational behavior, calls this phenomenon “cognitive
entrenchment.” His suggestions for avoiding it are about the polar opposite
of the strict version of the ten-thousand-hours school of thought: vary
challenges within a domain drastically, and, as a fellow researcher put it,
insist on “having one foot outside your world.”
Scientists and members of the general public are about equally likely to
have artistic hobbies, but scientists inducted into the highest national
academies are much more likely to have avocations outside of their
vocation. And those who======================================================== domains, microbiology and immunology. He has studied AIDS and
anthrax, and has illuminated important aspects of how fungal diseases
work. His “h-index,” a measure of a scientist’s productivity and how often
they are cited, recently surpassed Albert Einstein’s. 
*
 So his peers took it
seriously when he arrived at the Johns Hopkins Bloomberg School of
Public Health in 2015, as chair of molecular microbiology and
immunology, and warned that scientific research is in crisis.
In a lecture to his new colleagues, Casadevall declared that the pace of
progress had slowed, while the rate of retractions in scientific literature had
accelerated, proportionally outpacing the publication of new studies. “If this

continues unabated,” he said, “the entire literature will be retracted in a few
years.” It was science gallows humor, but grounded in data. Part of the
problem, he argued, is that young scientists are rushed to specialize before
they learn how to think; they end up unable to produce good work
themselves and unequipped to spot bad (or fraudulent) work by their
colleagues.
The reason Casadevall came to Hopkins, from a comfy post at New York
City’s Albert Einstein College of Medicine, is that the new gig offered him
the chance to create a prototype of what he thinks graduate science
education, and eventually all education, should be.
Counter to the prevailing trend, Casadevall—with Gundula Bosch, a
professor of both biology and education—is despecializing training, even
for students who plan to become the most specialized of specialists. The
program, known as the R3 Initiative (Rigor, Responsibility,
Reproducibility), starts with interdisciplinary classes that include
philosophy, history, logic, ethics, statistics, communication, and leadership.
A course titled “How Do We Know What Is True?” examines types of
evidence through history and across disciplines. In “Anatomy of Scientific
Error,” students are detectives, hunting for signs of misconduct or poor
methods in real research, while also learning how errors and serendipity
have led to momentous discoveries.
When Casadevall described his vision of broad education on a
professional panel in 2016, a copanelist and editor of the New England
Journal of Medicine (an extremely prestigious and retraction-prone journal)
countered that it would be absurd to add more training time to the already
jam-packed curricula for doctors and scientists. “I would say keep the same
time, and deemphasize all the other didactic material,” Casadevall said. “Do
we really need to go through courses with very specialized knowledge that
often provides a huge amount of stuff that is very detailed, very specialized,
very arcane, and will be totally forgotten in a couple of weeks? Especially
now, when all the information is on your phone. You have people walking
around with all the knowledge of humanity on their phone, but they have no
idea how to integrate it. We don’t train people in thinking or reasoning.”
Doctors and scientists frequently are not even trained in the basic
underlying logic of their own tools. In 2013, a group of doctors and
scientists gave physicians and medical students affiliated with Harvard and
Boston University a type of problem that appears constantly in medicine:

If a test to detect a disease whose prevalence is 1/ 1000 has a false positive rate of 5%, what is
the chance that a person found to have a positive result actually has the disease, assuming you
know nothing about the person’s symptoms or signs?
Assuming the test detects every true case, the correct answer is that there
is about a 2 percent chance (1.96 to be exact) that the patient actually has
the disease. Only a quarter of the physicians and physicians-in-training got
it right. The most common answer was 95 percent. It should be a very
simple problem for professionals who rely on diagnostic tests for a living:
in a sample of 10,000 people, 10 have the disease and get a true positive
result; 5 percent, or 500, will get a false positive; out of 510 people who test
positive, only 10, or 1.96 percent, are actually sick. The problem is not
intuitive, but nor is it difficult. Every medical student and physician has the
numerical ability to solve it. So, as James Flynn observed when he tested
bright college students in basic reasoning, they must not be primed to use
the broader reasoning tools of their trade, even though they are capable.
“I would argue, at least in medicine and basic science where we fill
people up with facts from courses, that what is needed is just some
background, and then the tools for thinking,” Casadevall told me. Currently,
“everything is configuring in the wrong way.”
He compared the current system to medieval guilds. “The guild system in
Europe arose in the Middle Ages as artisans and merchants sought to
maintain and protect specialized skills and trades,” he wrote with a
colleague. “Although such guilds often produced highly trained and
specialized individuals who perfected their trade through prolonged
apprenticeships, they also encouraged conservatism and stifled innovation.”
Both training and professional incentives are aligning to accelerate
specialization, creating intellectual archipelagos.
There is a growth industry of conferences that invite only scientists who
work on a single specific microorganism. Meanwhile, a complete
understanding of the body’s response to a paper cut was hampered because
hyperspecialists in hematology and immunology focus on pieces of the
puzzle in isolation, even though the immune response is an integrated
system.
“You can do your entire career on one cell type and it’s more likely you
keep your job by getting grants,” Casadevall told me. “There is not even
pressure to integrate. In fact, if you write a grant proposal about how the B
cell is integrating with the macrophage [a basic interaction of the immune

system], 
*
 there may be no one to review it. If it goes to the macrophage
people, they say, ‘Well, I don’t know anything about it. Why B cells?’ The
system maintains you in a trench. You basically have all these parallel
trenches, and it’s very rare that anybody stands up and actually looks at the
next trench to see what they are doing, and often it’s related.” 
*
Substitute a few specific terms, and the system of parallel trenches he
described could fit many industries. While I was researching this book, an
official with the U.S. Securities and Exchange Commission learned I was
writing about specialization and contacted me to make sure I knew that
specialization had played a critical role in the 2008 global financial crisis.
“Insurance regulators regulated insurance, bank regulators regulated banks,
securities regulators regulated securities, and consumer regulators regulated
consumers,” the official told me. “But the provision of credit goes across all
those markets. So we specialized products, we specialized regulation, and
the question is, ‘Who looks across those markets?’ The specialized
approach to regulation missed systemic issues.”
In 2015, Casadevall showed that biomedical research funding rose
exponentially over a recent thirty-five-year period, while discovery slowed
down. Life expectancy in countries at the biomedical cutting edge, like the
United Kingdom and the United States, recently declined after decades of
improvement. The flu annually kills hundreds of thousands of people
worldwide while humanity fights it with a cumbersomely produced vaccine
from the 1940s. Casadevall’s mother is ninety-three, and on five
medications that were available when he was a medical resident in the
1980s. “Two of them are older than I am,” he said, and two others are
barely younger. “I cannot believe we can’t do better.” He paused for a
moment, tilted his head, and leaned forward. “If you write an
interdisciplinary grant proposal, it goes to people who are really, really
specialized in A or B, and maybe if you’re lucky they have the capacity to
see the connections at the interface of A and B,” he told me. “Everyone
acknowledges that great progress is made at the interface, but who is there
to defend the interface?”
The interface between specialties, and between creators with disparate
backgrounds, has been studied, and it is worth defending .

When Northwestern and Stanford researchers analyzed the networks that
give rise to creative triumph, they found what they deemed a “universal”
setup. Whether they looked at research groups in economics or ecology, or
the teams that write, compose, and produce Broadway musicals, thriving
ecosystems had porous boundaries between teams.
In professional networks that acted as fertile soil for successful groups,
individuals moved easily among teams, crossing organizational and
disciplinary boundaries and finding new collaborators. Networks that
spawned unsuccessful teams, conversely, were broken into small, isolated
clusters in which the same people collaborated over and over. Efficient and
comfortable, perhaps, but apparently not a creative engine. “The entire
network looks different when you compare a successful team with an
unsuccessful team,” according to Luís A. Nunes Amaral, a Northwestern
physicist who studies networks. Amaral’s remark does not compare
individual teams, but rather the larger ecosystems that foster the formation
of successful teams.
The commercial fate of Broadway during any particular era, be it
unusually prosperous or exceptionally flop-ridden, had less to do with
specific famous names and more to do with whether collaborators mixed
and matched vibrantly. The 1920s featured dozens of shows with Cole
Porter, Irving Berlin, George Gershwin, Rodgers and Hammerstein (albeit
not yet in collaboration), and also an unusually high overall flop rate of 90
percent for new shows. It was an era of stagnant teams, rife with repeat
collaborations and scant boundary crossing.
New collaborations allow creators “to take ideas that are conventions in
one area and bring them into a new area, where they’re======================================================== in

cancer, but rather in cancer related to a single organ, and the trend advances
each year. Surgeon and writer Atul Gawande pointed out that when doctors
joke about left ear surgeons, “we have to check to be sure they don’t exist.”
In the ten-thousand-hours-themed bestseller Bounce , British journalist
Matthew Syed suggested that the British government was failing for a lack
of following the Tiger Woods path of unwavering specialization. Moving
high-ranking government officials between departments, he wrote, “is no
less absurd than rotating Tiger Woods from golf to baseball to football to
hockey.”
Except that Great Britain’s massive success at recent Summer Olympics,
after decades of middling performances, was bolstered by programs set up
specifically to recruit adults to try new sports and to create a pipeline for late
developers—“slow bakers,” as one of the officials behind the program
described them to me. Apparently the idea of an athlete, even one who wants
to become elite, following a Roger path and trying different sports is not so
absurd. Elite athletes at the peak of their abilities do spend more time on
focused, deliberate practice than their near-elite peers. But when scientists
examine the entire developmental path of athletes, from early childhood, it
looks like this:

Eventual elites typically devote less time early on to deliberate practice in
the activity in which they will eventually become experts. Instead, they
undergo what researchers call a “sampling period.” They play a variety of
sports, usually in an unstructured or lightly structured environment; they
gain a range of physical proficiencies from which they can draw; they learn
about their own abilities and proclivities; and only later do they focus in and
ramp up technical practice in one area. The title of one study of athletes in
individual sports proclaimed “Late Specialization” as “the Key to Success”;
another, “Making It to the Top in Team Sports: Start Later, Intensify, and Be
Determined.”

When I began to write about these studies, I was met with thoughtful
criticism, but also denial. “Maybe in some other sport,” fans often said, “but
that’s not true of our sport.” The community of the world’s most popular
sport, soccer, was the loudest. And then, as if on cue, in late 2014 a team of
German scientists published a study showing that members of their national
team, which had just won the World Cup, were typically late specializers
who didn’t play more organized soccer than amateur-league players until age
twenty-two or later. They spent more of their childhood and adolescence
playing nonorganized soccer and other sports. Another soccer study
published two years later matched players for skill at age eleven and tracked
them for two years. Those who participated in more sports and nonorganized
soccer, “but not more organized soccer practice/ training,” improved more
by age thirteen. Findings like these have now been echoed in a huge array of
sports, from hockey to volleyball.
The professed necessity of hyperspecialization forms the core of a vast,
successful, and sometimes well-meaning marketing machine, in sports and
beyond. In reality, the Roger path to sports stardom is far more prevalent
than the Tiger path, but those athletes’ stories are much more quietly told, if
they are told at all. Some of their names you know, but their backgrounds
you probably don’t.
I started writing this introduction right after the 2018 Super Bowl, in
which a quarterback who had been drafted into professional baseball before
football (Tom Brady), faced off against one who participated in football,
basketball, baseball, and karate and had chosen between college basketball
and football (Nick Foles). Later that very same month, Czech athlete Ester
Ledecká became the first woman ever to win gold in two different sports
(skiing and snowboarding) at the same Winter Olympics. When she was
younger, Ledecká participated in multiple sports (she still plays beach
volleyball and windsurfs), focused on school, and never rushed to be number
one in teenage competition categories. The Washington Post article the day
after her second gold proclaimed, “In an era of sports specialization,
Ledecká has been an evangelist for maintaining variety.” Just after her feat,
Ukrainian boxer Vasyl Lomachenko set a record for the fewest fights needed
to win world titles in three different weight classes. Lomachenko, who took
four years off boxing as a kid to learn traditional Ukrainian dance, reflected,
“I was doing so many different sports as a young boy—gymnastics,

basketball, football, tennis—and I think, ultimately, everything came
together with all those different kinds of sports to enhance my footwork.”
Prominent sports scientist Ross Tucker summed up research in the field
simply: “We know that early sampling is key, as is diversity.”
In 2014, I included some of the findings about late specialization in sports in
the afterword of my first book, The Sports Gene . The following year, I got
an invitation to talk about that research from an unlikely audience—not
athletes or coaches, but military veterans. In preparation, I perused scientific
journals for work on specialization and career-swerving outside of the sports
world. I was struck by what I found. One study showed that early career
specializers jumped out to an earnings lead after college, but that later
specializers made up for the head start by finding work that better fit their
skills and personalities. I found a raft of studies that showed how
technological inventors increased their creative impact by accumulating
experience in different domains, compared to peers who drilled more deeply
into one; they actually benefited by proactively sacrificing a modicum of
depth for breadth as their careers progressed. There was a nearly identical
finding in a study of artistic creators.
I also began to realize that some of the people whose work I deeply
admired from afar—from Duke Ellington (who shunned music lessons to
focus on drawing and baseball as a kid) to Maryam Mirzakhani (who
dreamed of becoming a novelist and instead became the first woman to win
math’s most famous prize, the Fields Medal)—seemed to have more Roger
than Tiger in their development stories. I delved further and encountered
remarkable individuals who succeeded not in spite of their range of
experiences and interests, but because of it: a CEO who took her first job
around the time her peers were getting ready to retire; an artist who cycled
through five careers before he discovered his vocation and changed the
world; an inventor who stuck to a self-made antispecialization philosophy
and turned a small company founded in the nineteenth century into one of
the most widely resonant names in the world today.
I had only dipped my toe into research on specialization in the wider
world of work, so in my talk to the small group of military veterans I mostly
stuck to sports. I touched on the other findings only briefly, but the audience
seized on it. All were late specializers or career changers, and as they filed

up one after another to introduce themselves after the talk, I could tell that
all were at least moderately concerned, and some were borderline ashamed
of it.
They had been brought together by the Pat Tillman Foundation, which, in
the spirit of the late NFL player who left a professional football career to
become an Army Ranger, provides scholarships to veterans, active-duty
military, and military spouses who are undergoing career changes or going
back to school. They were all scholarship recipients, former paratroopers and
translators who were becoming teachers, scientists, engineers, and
entrepreneurs. They brimmed with enthusiasm, but rippled with an
undercurrent of fear. Their LinkedIn profiles didn’t show the linear
progression toward a particular career they had been told employers wanted.
They were anxious starting grad school alongside younger (sometimes much
younger) students, or changing lanes later than their peers, all because they
had been busy accumulating inimitable life and leadership experiences.
Somehow, a unique advantage had morphed in their heads into a liability.
A few days after I spoke to the Tillman Foundation group, a former Navy
SEAL who came up after the talk emailed me: “We are all transitioning from
one career to another. Several of us got together after you had left and
discussed how relieved we were to have heard you speak.” I was slightly
bemused to find that a former Navy SEAL with an undergraduate degree in
history and geophysics pursuing graduate degrees in business and public
administration from Dartmouth and Harvard could feel behind. But like the
others in the room, he had been told, implicitly or explicitly, that changing
directions was dangerous.
The talk was greeted with so much enthusiasm that the foundation invited
me to give a keynote speech at the annual conference in 2016, and then to
small group gatherings in different cities. Before each occasion, I read more
studies and spoke with more researchers and found more evidence that it
takes time—and often forgoing a head start—to develop personal and
professional range, but it is worth it.
I dove into work showing that highly credentialed experts can become so
narrow-minded that they actually get worse with experience, even while
becoming more confident—a dangerous combination. And I was stunned
when cognitive psychologists I spoke with led me to an enormous and too
often ignored body of work demonstrating that learning itself is best done
slowly to accumulate lasting knowledge, even when that means performing

poorly on tests of immediate progress. That is, the most effective learning
looks inefficient; it looks like falling behind.
Starting something new in middle age might look that way too. Mark
Zuckerberg famously noted that “young people are just smarter.” And yet a
tech founder who is fifty years old is nearly twice as likely to start a
blockbuster======================================================== pivot after
another, applying the lessons as he went.
Ogas uses the shorthand “standardization covenant” for the cultural
notion that it is rational to trade a winding path of self-exploration for a
rigid goal with a head start because it ensures stability. “The people we
study who are fulfilled do pursue a long-term goal, but they only formulate
it after a period of discovery,” he told me. “Obviously, there’s nothing
wrong with getting a law or medical degree or PhD. But it’s actually riskier
to make that commitment before you know how it fits you. And don’t
consider the path fixed. People realize things about themselves halfway
through medical school.” Charles Darwin, for example.

At his father’s behest he planned to be a doctor, but he found medical
lectures “intolerably dull,” and partway through his education he walked
out of an operation at the grind of the surgical saw. “Nor did I ever attend
again,” Darwin wrote, “for hardly any inducement would have been strong
enough to make me do so.” Darwin was a Bible literalist at the time, and
figured he would become a clergyman. He bounced around classes,
including a botany course with a professor who subsequently recommended
him for an unpaid position aboard the HMS Beagle . After convincing his
father (with his uncle’s help) that he would not become a deadbeat if he
took this one detour, Darwin began perhaps the most impactful post-college
gap year in history. His father’s wishes eventually “died a natural death.”
Decades later, Darwin reflected on the process of self-discovery. “It seems
ludicrous that I once intended to be a clergyman,” he wrote. His father, a
doctor for more than sixty years, detested the sight of blood. “If his father
had given him any choice,” Darwin wrote, “nothing should have induced
him to follow it.”
Michael Crichton started with medicine too, after learning how few
writers make a living. With medicine, “I would never have to wonder if the
work was worthwhile,” he wrote. Except, a few years in he became
disenchanted with medical practice. He graduated from Harvard Medical
School, but decided to become a writer. His medical education was not
remotely wasted. He used it to craft some of the most popular stories in the
world—the novel Jurassic Park, and the TV series ER, with its record-
setting 124 Emmy nominations.
Career goals that once felt safe and certain can appear ludicrous, to use
Darwin’s adjective, when examined in the light of more self-knowledge.
Our work preferences and our life preferences do not stay the same, because
we do not stay the same.
Psychologist Dan Gilbert called it the “end of history illusion.” From
teenagers to senior citizens, we recognize that our desires and motivations
sure changed a lot in the past (see: your old hairstyle), but believe they will
not change much in the future. In Gilbert’s terms, we are works in progress
claiming to be finished.
Gilbert and colleagues measured the preferences, values, and
personalities of more than nineteen thousand adults aged eighteen to sixty-

eight. Some were asked to predict how much they would change over the
next decade, others to reflect about how much they had changed in the
previous one. Predictors expected that they would change very little in the
next decade, while reflectors reported having changed a lot in the previous
one. Qualities that feel immutable changed immensely. Core values—
pleasure, security, success, and honesty—transformed. Preferences for
vacations, music, hobbies, and even friends were transfigured. Hilariously,
predictors were willing to pay an average of $129 a ticket for a show ten
years away by their current favorite band, while reflectors would only pay
$80 to see a show today by their favorite band from ten years ago. The
precise person you are now is fleeting, just like all the other people you’ve
been. That feels like the most unexpected result, but it is also the most well
documented.
It is definitely true that a shy child is more likely to foreshadow a shy
adult, but it is far from a perfect correlation. And if one particular
personality trait does not change, others will. The only certainty is change,
both on average as a generation ages, and within each individual. University
of Illinois psychologist Brent W. Roberts specializes in studying personality
development. He and another psychologist aggregated the results of ninety-
two studies and revealed that some personality traits change over time in
fairly predictable ways. Adults tend to become more agreeable, more
conscientious, more emotionally stable, and less neurotic with age, but less
open to experience. In middle age, adults grow more consistent and
cautious and less curious, open-minded, and inventive. 
*
 The changes have
well-known impacts, like the fact that adults generally become less likely to
commit violent crimes with age, and more able to create stable
relationships. The most momentous personality changes occur between age
eighteen and one’s late twenties, so specializing early is a task of predicting
match quality for a person who does not yet exist. It could work, but it
makes for worse odds. Plus, while personality change slows, it does not
stop at any age. Sometimes it can actually happen instantly.
Thanks to YouTube, the “marshmallow test” could be the most famous
scientific experiment in the world. It was actually a series of experiments
starting in the 1960s. The original premise was simple: An experimenter
places a marshmallow (or a cookie, or a pretzel) in front of a nursery school

child; before leaving, the experimenter tells the child that if she can wait
until the experimenter returns, she’ll get that marshmallow plus a second
one. If the child can’t wait, she can eat the marshmallow. The children were
not told how long the wait would be (it was fifteen to twenty minutes,
depending on age), so they just had to hold out if they wanted the maximum
reward.
Psychologist Walter Mischel and his research team followed up with the
children years later, and found that the longer a child had been able to wait,
the more likely she was to be successful socially, academically, and
financially, and the less likely she was to abuse drugs.
The marshmallow test was already a celebrity as scientific experiments
go, but it became the Beyoncé of studies when media outlets and parents
eager to foretell their child’s destiny started posting DIY marshmallow tests
online. The videos are by turns adorable and intriguing. Nearly all kids wait
at least a little. Some stare at the marshmallow, touch it, sniff it, delicately
tap their tongue to it and pull back as if it were hot. Maybe they even put it
in their mouth, pull it out, and simulate a big bite. Some tear a barely
noticeable piece off for a micro test taste. Before the end of the video, the
kids who start by touching it will have eaten the marshmallow. The kids
who successfully hold out wield all manner of distraction, from looking
away to shoving the plate away, covering their eyes, turning and screaming,
singing, talking to themselves, counting, generally thrashing around in the
chair, or (boys) hitting themselves in the face. One little boy who spent his
time looking in every direction except at the marshmallow is so ravenous
when the experimenter returns with his second treat that he mashes them
both into his mouth immediately.
The crystal ball allure of the marshmallow test is undeniable, and also
misconstrued. Mischel’s collaborator Yuichi Shoda has repeatedly made a
point of saying that plenty of preschoolers who ate the marshmallow turned
out just fine. 
*
 Shoda maintained that the most exciting aspect of the studies
was demonstrating how easily children could learn to change a specific
behavior with simple mental strategies, like thinking about the
marshmallow as a cloud rather than food. Shoda’s post-marshmallow-test
work has been one part of a bridge in psychology between extreme
arguments in the debate about the roles of nature and nurture in personality.
One extreme suggests that personality traits are almost entirely a function of
one’s nature, and the other that personality is entirely a function of the

environment. Shoda argued that both sides of the so-called person-situation
debate were right. And wrong. At a given point in life, an individual’s
nature influences how they respond to a particular situation, but their nature
can appear surprisingly different in some other situation. With Mischel, he
began to study “if-then signatures.” If David is at a giant party, then he
seems introverted, but if David is with his team at work, then he seems
extroverted. (True.) So is David introverted or extroverted? Well, both, and
consistently so.
Ogas and Rose call this the “context principle.” In 2007, Mischel wrote,
“The gist of such findings is that the child who is aggressive at home may
be less aggressive than most when in school; the man exceptionally hostile
when rejected in love may be unusually tolerant about criticism of his work;
the one who melts with anxiety in the doctor’s office may be a calm
mountain climber; the risk-taking entrepreneur may take few social risks.”
Rose framed it more colloquially: “If you are conscientious and neurotic
while driving today, it’s a pretty safe bet you will be conscientious and
neurotic while driving tomorrow. At the same time . . . you may not be
conscientious and neurotic when you are playing Beatles cover songs with
your band in the context of the local pub.” Perhaps that is one reason Daniel
Kahneman and his colleagues in the military (chapter 1 ) failed to predict
who would be a leader in battle based on who had been a leader in an
obstacle course exercise. When I was a college runner, I had teammates
whose drive and determination seemed almost boundless on the track, and
nearly absent in the classroom, and vice versa. Instead of asking whether
someone is gritty, we should ask when they are. “If you get someone into a
context that suits them,” Ogas said,======================================================== company as one who is thirty, and the thirty-year-old has a better
shot than a twenty-year-old. Researchers at Northwestern, MIT, and the U.S.
Census Bureau studied new tech companies and showed that among the
fastest-growing start-ups, the average age of a founder was forty-five when
the company was launched.
Zuckerberg was twenty-two when he said that. It was in his interest to
broadcast that message, just as it is in the interest of people who run youth
sports leagues to claim that year-round devotion to one activity is necessary
for success, never mind evidence to the contrary. But the drive to specialize
goes beyond that. It infects not just individuals, but entire systems, as each
specialized group sees a smaller and smaller part of a large puzzle.
One revelation in the aftermath of the 2008 global financial crisis was the
degree of segregation within big banks. Legions of specialized groups
optimizing risk for their own tiny pieces of the big picture created a
catastrophic whole. To make matters worse, responses to the crisis betrayed
a dizzying degree of specialization-induced perversity. A federal program
launched in 2009 incentivized banks to lower monthly mortgage payments
for homeowners who were struggling but still able to make partial payments.
A nice idea, but here’s how it worked out in practice: a bank arm that
specialized in mortgage lending started the homeowner on lower payments;
an arm of the same bank that specialized in foreclosures then noticed that the
homeowner was suddenly paying less, declared them in default, and seized
the home. “No one imagined silos like that inside banks,” a government
adviser said later. Overspecialization can lead to collective tragedy even
when every individual separately takes the most reasonable course of action.
Highly specialized health care professionals have developed their own
versions of the “if all you have is a hammer, everything looks like a nail”
problem. Interventional cardiologists have gotten so used to treating chest
pain with stents—metal tubes that pry open blood vessels—that they do so
reflexively even in cases where voluminous research has proven that they
are inappropriate or dangerous. A recent study found that cardiac patients
were actually less likely to die if they were admitted during a national

cardiology meeting, when thousands of cardiologists were away; the
researchers suggested it could be because common treatments of dubious
effect were less likely to be performed.
An internationally renowned scientist (whom you will meet toward the
end of this book) told me that increasing specialization has created a “system
of parallel trenches” in the quest for innovation. Everyone is digging deeper
into their own trench and rarely standing up to look in the next trench over,
even though the solution to their problem happens to reside there. The
scientist is taking it upon himself to attempt to despecialize the training of
future researchers; he hopes that eventually it will spread to training in every
field. He profited immensely from cultivating range in his own life, even as
he was pushed to specialize. And now he is broadening his purview again,
designing a training program in an attempt to give others a chance to deviate
from the Tiger path. “This may be the most important thing I will ever do in
my life,” he told me.
I hope this book helps you understand why.
When the Tillman Scholars spoke of feeling unmoored, and worried they
were making a mistake, I understood better than I let on. I was working on a
scientific research vessel in the Pacific Ocean after college when I decided
for sure that I wanted to be a writer, not a scientist. I never expected that my
path from science into writing would go through work as the overnight crime
reporter at a New York City tabloid, nor that I would shortly thereafter be a
senior writer at Sports Illustrated, a job that, to my own surprise, I would
soon leave. I began worrying that I was a job-commitment-phobic drifter
who must be doing this whole career thing wrong. Learning about the
advantages of breadth and delayed specialization has changed the way I see
myself and the world. The research pertains to every stage of life, from the
development of children in math, music, and sports, to students fresh out of
college trying to find their way, to midcareer professionals in need of a
change and would-be retirees looking for a new vocation after moving on
from their previous one.
The challenge we all face is how to maintain the benefits of breadth,
diverse experience, interdisciplinary thinking, and delayed concentration in a
world that increasingly incentivizes, even demands, hyperspecialization.
While it is undoubtedly true that there are areas that require individuals with

Tiger’s precocity and clarity of purpose, as complexity increases—as
technology spins the world into vaster webs of interconnected systems in
which each individual only sees a small part—we also need more Rogers:
people who start broad and embrace diverse experiences and perspectives
while they progress. People with range.
OceanofPDF.com

CHAPTER 1
The Cult of the Head Start
ONE YEAR AND FOUR DAYS after World War II in Europe ended in
unconditional surrender, Laszlo Polgar was born in a small town in
Hungary—the seed of a new family. He had no grandmothers, no
grandfathers, and no cousins; all had been wiped out in the Holocaust,
along with his father’s first wife and five children. Laszlo grew up
determined to have a family, and a special one.
He prepped for fatherhood in college by poring over biographies of
legendary thinkers, from Socrates to Einstein. He decided that traditional
education was broken, and that he could make his own children into
geniuses, if he just gave them the right head start. By doing so, he would
prove something far greater: that any child can be molded for eminence in
any discipline. He just needed a wife who would go along with the plan.
Laszlo’s mother had a friend, and the friend had a daughter, Klara. In
1965, Klara traveled to Budapest, where she met Laszlo in person. Laszlo
didn’t play hard to get; he spent the first visit telling Klara that he planned
to have six children and that he would nurture them to brilliance. Klara
returned home to her parents with a lukewarm review: she had “met a very
interesting person,” but could not imagine marrying him .
They continued to exchange letters. They were both teachers and agreed
that the school system was frustratingly one-size-fits-all, made for
producing “the gray average mass,” as Laszlo put it. A year and a half of
letters later, Klara realized she had a very special pen pal. Laszlo finally
wrote a love letter, and proposed at the end. They married, moved to

Budapest, and got to work. Susan was born in early 1969, and the
experiment was on.
For his first genius, Laszlo picked chess. In 1972, the year before Susan
started training, American Bobby Fischer defeated Russian Boris Spassky
in the “Match of the Century.” It was considered a Cold War proxy in both
hemispheres, and chess was suddenly pop culture. Plus, according to Klara,
the game had a distinct benefit: “Chess is very objective and easy to
measure.” Win, lose, or draw, and a point system measures skill against the
rest of the chess world. His daughter, Laszlo decided, would become a
chess champion.
Laszlo was patient, and meticulous. He started Susan with “pawn wars.”
Pawns only, and the first person to advance to the back row wins. Soon,
Susan was studying endgames and opening traps. She enjoyed the game and
caught on quickly. After eight months of study, Laszlo took her to a smoky
chess club in Budapest and challenged grown men to play his four-year-old
daughter, whose legs dangled from her chair. Susan won her first game, and
the man she beat stormed off. She entered the Budapest girls’ championship
and won the under-eleven title. At age four she had not lost a game.
By six, Susan could read and write and was years ahead of her grade
peers in math. Laszlo and Klara decided they would educate her at home
and keep the day open for chess. The Hungarian police threatened to throw
Laszlo in jail if he did not send his daughter to the compulsory school
system. It took him months of lobbying the Ministry of Education to gain
permission. Susan’s new little sister, Sofia, would be homeschooled too, as
would Judit, who was coming soon, and whom Laszlo and Klara almost
named Zseni, Hungarian for “genius.” All three became part of the grand
experiment.
On a normal day, the girls were at the gym by 7 a.m. playing table tennis
with trainers, and then back home at 10:00 for breakfast, before a long day
of chess. When Laszlo reached the limit of his expertise, he hired coaches
for his three geniuses in training. He spent his extra time cutting two
hundred thousand records of game sequences from chess journals—many
offering a preview of potential opponents—and filing them in a custom card
catalog, the “cartotech.” Before computer chess programs, it gave the
Polgars the largest chess database in the world to study outside of—maybe
—the Soviet Union’s secret archives.

When she was seventeen, Susan became the first woman to qualify for
the men’s world championship, although the world chess federation did not
allow her to participate. (A rule that would soon be changed, thanks to her
accomplishments.) Two years later, in 1988, when Sofia was fourteen and
Judit twelve, the girls comprised three of the four Hungarian team members
for the women’s Chess Olympiad. They won, and beat the Soviet Union,
which had won eleven of the twelve Olympiads since the event began. The
Polgar sisters became “national treasures,” as Susan put it. The following
year, communism fell, and the girls could compete all over the world. In
January 1991, at the age of twenty-one, Susan became the first woman to
achieve grandmaster status through tournament play against men. In
December, Judit, at fifteen years and five months,